{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "27a53169-955e-4004-b704-079816d9bfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "import lightgbm as lgb\n",
    "\n",
    "#データ読み込み\n",
    "train = pd.read_csv(\"data_EDA/train.csv\")\n",
    "test = pd.read_csv(\"data_EDA/test.csv\")\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "#https://www.kaggle.com/code/masaonda/titanic-how-to-use-tabnet/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ce0b25ec-0d25-454d-a627-0a54d26264d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[['DiabetesPedigreeFunction',\n",
    "                 'BMI',\n",
    "                 'Glucose',\n",
    "                 'Age',\n",
    "                 'Pregnancies',\n",
    "                 'Pregnancies_bin',\n",
    "                 'BloodPressure_na' ,\n",
    "                 'BloodPressure', \n",
    "                 'SkinThickness',\n",
    "                 'Insulin',\n",
    "                 'Insulin_na',\n",
    "                 ]]\n",
    "id_train = train[['index']]\n",
    "y_train = train[['Outcome']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a142ff65-b5f5-4dd2-8ab3-ab5d886c16df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['BloodPressure_na'] = X_train['BloodPressure_na'].astype(str)\n",
    "X_train['Insulin_na'] = X_train['Insulin_na'].astype(str)\n",
    "X_train['Pregnancies_bin'] = X_train['Pregnancies_bin'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ce3b36fb-feb4-4a7c-8ebb-30c3c3409869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BP_oh_0</th>\n",
       "      <th>BP_oh_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      BP_oh_0  BP_oh_1\n",
       "0           1        0\n",
       "1           1        0\n",
       "2           1        0\n",
       "3           1        0\n",
       "4           1        0\n",
       "...       ...      ...\n",
       "2995        1        0\n",
       "2996        1        0\n",
       "2997        1        0\n",
       "2998        1        0\n",
       "2999        1        0\n",
       "\n",
       "[3000 rows x 2 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BloodPressure_oh = pd.get_dummies(X_train['BloodPressure_na'],prefix='BP_oh')\n",
    "Insulin_oh = pd.get_dummies(X_train['Insulin_na'],prefix='In_oh')\n",
    "Pregnancies_bin_oh =pd.get_dummies(X_train['Pregnancies_bin'],prefix='Pre_oh')\n",
    "BloodPressure_oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "98a58c24-b2df-4be4-ad9f-fc4d69753612",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_columns = ['DiabetesPedigreeFunction',\n",
    "                    'BMI',\n",
    "                    'Glucose',\n",
    "                    'Age',\n",
    "                    'Pregnancies',\n",
    "                    'BloodPressure', \n",
    "                    'SkinThickness',\n",
    "                    'Insulin',\n",
    "                 ]\n",
    "std = StandardScaler()\n",
    "std.fit(X_train[scaling_columns])\n",
    "# 標準化したカラムのみ元のDataFrameに戻す\n",
    "scaled_X_train = pd.DataFrame(std.transform(X_train[scaling_columns]), columns=scaling_columns, index=X_train.index)\n",
    "X_train.update(scaled_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2c0ec2a1-0e60-4a90-a259-0bcd820a12e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 123\n",
    "params = {\n",
    "    \n",
    "    'random_state' : random_state,\n",
    "    \n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0ac63ff2-a8ae-448d-bbe8-c7b11d288201",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, x_va, y_tr, y_va = train_test_split(X_train,\n",
    "                                          y_train,\n",
    "                                          test_size=0.2,\n",
    "                                          shuffle=True,\n",
    "                                          stratify=y_train,\n",
    "                                          random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e672ed52-7fd3-407f-b3cb-f8c35f423aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr=np.squeeze(y_tr.values)\n",
    "y_va=np.squeeze(y_va.values)\n",
    "\n",
    "x_tr=x_tr.values\n",
    "x_va=x_va.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "71ff60ed-56b5-4a99-8860-512d86f68081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.71333 | val_0_auc: 0.42985 | val_1_auc: 0.48085 |  0:00:00s\n",
      "epoch 1  | loss: 0.59412 | val_0_auc: 0.46189 | val_1_auc: 0.46627 |  0:00:00s\n",
      "epoch 2  | loss: 0.56575 | val_0_auc: 0.48694 | val_1_auc: 0.47225 |  0:00:00s\n",
      "epoch 3  | loss: 0.53255 | val_0_auc: 0.44495 | val_1_auc: 0.40373 |  0:00:00s\n",
      "epoch 4  | loss: 0.52056 | val_0_auc: 0.46161 | val_1_auc: 0.39683 |  0:00:00s\n",
      "epoch 5  | loss: 0.50049 | val_0_auc: 0.46271 | val_1_auc: 0.43251 |  0:00:00s\n",
      "epoch 6  | loss: 0.50311 | val_0_auc: 0.45889 | val_1_auc: 0.44559 |  0:00:00s\n",
      "epoch 7  | loss: 0.48966 | val_0_auc: 0.48419 | val_1_auc: 0.48539 |  0:00:00s\n",
      "epoch 8  | loss: 0.48051 | val_0_auc: 0.50468 | val_1_auc: 0.53073 |  0:00:00s\n",
      "epoch 9  | loss: 0.48362 | val_0_auc: 0.50451 | val_1_auc: 0.50539 |  0:00:00s\n",
      "epoch 10 | loss: 0.47942 | val_0_auc: 0.49359 | val_1_auc: 0.48407 |  0:00:00s\n",
      "epoch 11 | loss: 0.48047 | val_0_auc: 0.50979 | val_1_auc: 0.48814 |  0:00:00s\n",
      "epoch 12 | loss: 0.47631 | val_0_auc: 0.51291 | val_1_auc: 0.47692 |  0:00:00s\n",
      "epoch 13 | loss: 0.47375 | val_0_auc: 0.49904 | val_1_auc: 0.45982 |  0:00:00s\n",
      "epoch 14 | loss: 0.47825 | val_0_auc: 0.47601 | val_1_auc: 0.45627 |  0:00:00s\n",
      "epoch 15 | loss: 0.4814  | val_0_auc: 0.46061 | val_1_auc: 0.4337  |  0:00:01s\n",
      "epoch 16 | loss: 0.46747 | val_0_auc: 0.4431  | val_1_auc: 0.4256  |  0:00:01s\n",
      "epoch 17 | loss: 0.46884 | val_0_auc: 0.43    | val_1_auc: 0.40165 |  0:00:01s\n",
      "epoch 18 | loss: 0.45859 | val_0_auc: 0.43498 | val_1_auc: 0.38618 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 8 and best_val_1_auc = 0.53073\n"
     ]
    }
   ],
   "source": [
    "model = TabNetClassifier()\n",
    "model.fit(x_tr,\n",
    "          y_tr,\n",
    "          eval_set=[(x_tr,y_tr),(x_va,y_va)],\n",
    "          \n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14544be4-d98a-4719-be3b-974a7f234b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cvでの評価用\n",
    "def train_cv(input_x,\n",
    "             input_y,\n",
    "             input_id,\n",
    "             params,\n",
    "             random_state=123,\n",
    "             n_splits=5\n",
    "            ):\n",
    "    \n",
    "    metrics = []\n",
    "    imp = pd.DataFrame()\n",
    "    cv = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state).\n",
    "              split(input_x, input_y))\n",
    "    \n",
    "    \n",
    "    for nfold in np.arange(n_splits):\n",
    "        print('-'*20, nfold, '-'*20)\n",
    "        idx_tr, idx_va = cv[nfold][0], cv[nfold][1]\n",
    "        x_tr, y_tr = input_x.loc[idx_tr, :], input_y.loc[idx_tr, :]\n",
    "        x_va, y_va = input_x.loc[idx_va, :], input_y.loc[idx_va, :]\n",
    "        print(x_tr.shape, y_tr.shape)\n",
    "        print(x_va.shape, y_va.shape)\n",
    "        print('y_train:{:.3f}, y_tr:{:.3f}, y_va{:.3f}'.\n",
    "              format(y_train['Outcome'].mean(), y_tr['Outcome'].mean(), y_va['Outcome'].mean(),))\n",
    "\n",
    "        \n",
    "        model = TabNetClassifier()\n",
    "        model.fit(x_tr.values,\n",
    "                  y_tr.values,\n",
    "                  eval_set=[(x_tr.values,y_tr.values),(x_va.values,y_va.values)],\n",
    "                  \n",
    "                 )\n",
    "\n",
    "        y_tr_pred = model.predict(x_tr)\n",
    "        y_va_pred = model.predict(x_va)\n",
    "        metric_tr = accuracy_score(y_tr, y_tr_pred)\n",
    "        metric_va = accuracy_score(y_va, y_va_pred)\n",
    "        print('[accuracy] tr: {:.2f}, va: {:2f}'.\n",
    "             format(metric_tr, metric_va))\n",
    "        metrics.append([nfold, metric_tr, metric_va])\n",
    "\n",
    "        _imp = pd.DataFrame({'col':input_x.columns, 'imp':model.feature_importances_,'nfold':nfold})\n",
    "        imp = pd.concat([imp, _imp], axis=0, ignore_index=True)\n",
    "\n",
    "    print('-'*20, 'result', '-'*20)\n",
    "    metrics = np.array(metrics)\n",
    "    print(metrics)\n",
    "\n",
    "    print('[cv] tr: {:.2f}+-{:.2f}, va: {:.2f}'.format(\n",
    "        metrics[:,1].mean(), metrics[:,1].std(),\n",
    "        metrics[:,2].mean(), metrics[:,2].std()\n",
    "    ))\n",
    "\n",
    "    imp = imp.groupby('col')['imp'].agg(['mean', 'std'])\n",
    "    imp.columns = ['imp', 'imp_std']\n",
    "    imp = imp.reset_index(drop=False)\n",
    "    imp=imp.sort_values('imp', ascending=False, ignore_index=True)\n",
    "    print('-'*20, 'imp', '-'*20)\n",
    "    print(imp)\n",
    "\n",
    "    print('Done')\n",
    "    \n",
    "    return imp, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef30ca75-3180-4cd2-b692-5f13ea9b4ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- 0 --------------------\n",
      "(2250, 11) (2250, 1)\n",
      "(750, 11) (750, 1)\n",
      "y_train:0.239, y_tr:0.239, y_va0.240\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m imp, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_cv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mtrain_cv\u001b[0;34m(input_x, input_y, input_id, params, random_state, n_splits)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_train:\u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m, y_tr:\u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m, y_va\u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m     24\u001b[0m       \u001b[38;5;28mformat\u001b[39m(y_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutcome\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(), y_tr[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutcome\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(), y_va[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutcome\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(),))\n\u001b[1;32m     27\u001b[0m model \u001b[38;5;241m=\u001b[39m TabNetClassifier()\n\u001b[0;32m---> 28\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_tr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m          \u001b[49m\u001b[43my_tr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m          \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_tr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_tr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_va\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_va\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m          \u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m         \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m y_tr_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_tr)\n\u001b[1;32m     35\u001b[0m y_va_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_va)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:200\u001b[0m, in \u001b[0;36mTabModel.fit\u001b[0;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised, warm_start, augmentations)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn \u001b[38;5;241m=\u001b[39m loss_fn\n\u001b[0;32m--> 200\u001b[0m \u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m check_warm_start(warm_start, from_unsupervised)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_fit_params(\n\u001b[1;32m    204\u001b[0m     X_train,\n\u001b[1;32m    205\u001b[0m     y_train,\n\u001b[1;32m    206\u001b[0m     eval_set,\n\u001b[1;32m    207\u001b[0m     weights,\n\u001b[1;32m    208\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pytorch_tabnet/utils.py:352\u001b[0m, in \u001b[0;36mcheck_input\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m    350\u001b[0m     err_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPandas DataFrame are not supported: apply X.values when calling fit\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(err_message)\n\u001b[0;32m--> 352\u001b[0m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/utils/validation.py:899\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    894\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    895\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    896\u001b[0m         )\n\u001b[1;32m    898\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 899\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    907\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/utils/validation.py:146\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;129;01mnot\u001b[39;00m allow_nan\n\u001b[1;32m    126\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m estimator_name\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    131\u001b[0m             \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m             msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    133\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m             )\n\u001b[0;32m--> 146\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nan:\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "imp, metrics = train_cv(X_train, y_train, id_train, params, random_state=random_state, n_splits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95a9e14-8517-4920-9ee8-54004654cf1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
