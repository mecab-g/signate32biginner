{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9828789-bec1-4170-8a76-262379ab71b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "import lightgbm as lgb\n",
    "\n",
    "#データ読み込み\n",
    "train = pd.read_csv(\"data_EDA/train.csv\")\n",
    "test = pd.read_csv(\"data_EDA/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "564c7484-3a88-48b9-8538-8ce80818836e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "      <th>BloodPressure_0</th>\n",
       "      <th>SkinThickness_0</th>\n",
       "      <th>Insulin_0</th>\n",
       "      <th>SkinThickness_na</th>\n",
       "      <th>BloodPressure_na</th>\n",
       "      <th>Insulin_na</th>\n",
       "      <th>SkinThickness_mean</th>\n",
       "      <th>BloodPressure_mean</th>\n",
       "      <th>Insulin_mean</th>\n",
       "      <th>Pregnancies_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.00000</td>\n",
       "      <td>2000.00000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1927.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2509.303000</td>\n",
       "      <td>3.584000</td>\n",
       "      <td>114.29350</td>\n",
       "      <td>68.76650</td>\n",
       "      <td>11.204000</td>\n",
       "      <td>11.859000</td>\n",
       "      <td>35.586624</td>\n",
       "      <td>0.401755</td>\n",
       "      <td>29.075500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71.371562</td>\n",
       "      <td>26.486998</td>\n",
       "      <td>146.407407</td>\n",
       "      <td>0.577000</td>\n",
       "      <td>0.036500</td>\n",
       "      <td>0.919000</td>\n",
       "      <td>26.710875</td>\n",
       "      <td>71.372937</td>\n",
       "      <td>136.508818</td>\n",
       "      <td>5.586500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1438.260835</td>\n",
       "      <td>3.053786</td>\n",
       "      <td>21.98925</td>\n",
       "      <td>16.17482</td>\n",
       "      <td>14.056037</td>\n",
       "      <td>49.826253</td>\n",
       "      <td>6.936853</td>\n",
       "      <td>0.267051</td>\n",
       "      <td>8.571729</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.247546</td>\n",
       "      <td>7.881171</td>\n",
       "      <td>104.898919</td>\n",
       "      <td>0.494159</td>\n",
       "      <td>0.187578</td>\n",
       "      <td>0.272903</td>\n",
       "      <td>5.127628</td>\n",
       "      <td>9.077127</td>\n",
       "      <td>29.914680</td>\n",
       "      <td>2.853698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>57.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.285680</td>\n",
       "      <td>0.137377</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1284.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>64.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.581209</td>\n",
       "      <td>0.234628</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>83.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26.875000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>135.636364</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2549.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>111.00000</td>\n",
       "      <td>70.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.814634</td>\n",
       "      <td>0.271275</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26.875000</td>\n",
       "      <td>71.409223</td>\n",
       "      <td>135.636364</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3743.750000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>125.00000</td>\n",
       "      <td>78.00000</td>\n",
       "      <td>24.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.694403</td>\n",
       "      <td>0.506439</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>189.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26.875000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>135.636364</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4995.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>199.00000</td>\n",
       "      <td>110.00000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>744.000000</td>\n",
       "      <td>52.960258</td>\n",
       "      <td>2.175784</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>744.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>744.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             index  Pregnancies     Glucose  BloodPressure  SkinThickness  \\\n",
       "count  2000.000000  2000.000000  2000.00000     2000.00000    2000.000000   \n",
       "mean   2509.303000     3.584000   114.29350       68.76650      11.204000   \n",
       "std    1438.260835     3.053786    21.98925       16.17482      14.056037   \n",
       "min       1.000000     0.000000    57.00000        0.00000       0.000000   \n",
       "25%    1284.750000     1.000000   100.00000       64.00000       0.000000   \n",
       "50%    2549.500000     3.000000   111.00000       70.00000       0.000000   \n",
       "75%    3743.750000     6.000000   125.00000       78.00000      24.250000   \n",
       "max    4995.000000    13.000000   199.00000      110.00000      52.000000   \n",
       "\n",
       "           Insulin          BMI  DiabetesPedigreeFunction          Age  \\\n",
       "count  2000.000000  2000.000000               2000.000000  2000.000000   \n",
       "mean     11.859000    35.586624                  0.401755    29.075500   \n",
       "std      49.826253     6.936853                  0.267051     8.571729   \n",
       "min       0.000000     9.285680                  0.137377    21.000000   \n",
       "25%       0.000000    32.581209                  0.234628    22.000000   \n",
       "50%       0.000000    33.814634                  0.271275    26.000000   \n",
       "75%       0.000000    39.694403                  0.506439    33.000000   \n",
       "max     744.000000    52.960258                  2.175784    67.000000   \n",
       "\n",
       "       Outcome  BloodPressure_0  SkinThickness_0   Insulin_0  \\\n",
       "count      0.0      1927.000000       846.000000  162.000000   \n",
       "mean       NaN        71.371562        26.486998  146.407407   \n",
       "std        NaN         9.247546         7.881171  104.898919   \n",
       "min        NaN        38.000000         8.000000   15.000000   \n",
       "25%        NaN        64.000000        19.000000   83.250000   \n",
       "50%        NaN        70.000000        27.000000  130.000000   \n",
       "75%        NaN        78.000000        32.000000  189.500000   \n",
       "max        NaN       110.000000        52.000000  744.000000   \n",
       "\n",
       "       SkinThickness_na  BloodPressure_na   Insulin_na  SkinThickness_mean  \\\n",
       "count       2000.000000       2000.000000  2000.000000         2000.000000   \n",
       "mean           0.577000          0.036500     0.919000           26.710875   \n",
       "std            0.494159          0.187578     0.272903            5.127628   \n",
       "min            0.000000          0.000000     0.000000            8.000000   \n",
       "25%            0.000000          0.000000     1.000000           26.875000   \n",
       "50%            1.000000          0.000000     1.000000           26.875000   \n",
       "75%            1.000000          0.000000     1.000000           26.875000   \n",
       "max            1.000000          1.000000     1.000000           52.000000   \n",
       "\n",
       "       BloodPressure_mean  Insulin_mean  Pregnancies_bin  \n",
       "count         2000.000000   2000.000000      2000.000000  \n",
       "mean            71.372937    136.508818         5.586500  \n",
       "std              9.077127     29.914680         2.853698  \n",
       "min             38.000000     15.000000         3.000000  \n",
       "25%             64.000000    135.636364         3.000000  \n",
       "50%             71.409223    135.636364         6.000000  \n",
       "75%             78.000000    135.636364        10.000000  \n",
       "max            110.000000    744.000000        10.000000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4df67cb-c4bd-4f8c-8fb7-ef2b18cc8a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "      <th>BloodPressure_0</th>\n",
       "      <th>SkinThickness_0</th>\n",
       "      <th>Insulin_0</th>\n",
       "      <th>SkinThickness_na</th>\n",
       "      <th>BloodPressure_na</th>\n",
       "      <th>Insulin_na</th>\n",
       "      <th>SkinThickness_mean</th>\n",
       "      <th>BloodPressure_mean</th>\n",
       "      <th>Insulin_mean</th>\n",
       "      <th>Pregnancies_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>2887.000000</td>\n",
       "      <td>1234.000000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2492.964667</td>\n",
       "      <td>3.557000</td>\n",
       "      <td>113.733667</td>\n",
       "      <td>68.743667</td>\n",
       "      <td>11.164000</td>\n",
       "      <td>11.663333</td>\n",
       "      <td>35.408959</td>\n",
       "      <td>0.400476</td>\n",
       "      <td>28.932000</td>\n",
       "      <td>0.239000</td>\n",
       "      <td>71.434361</td>\n",
       "      <td>27.141005</td>\n",
       "      <td>136.679688</td>\n",
       "      <td>0.588667</td>\n",
       "      <td>0.037667</td>\n",
       "      <td>0.914667</td>\n",
       "      <td>26.984417</td>\n",
       "      <td>71.433414</td>\n",
       "      <td>135.725394</td>\n",
       "      <td>5.594000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1447.218078</td>\n",
       "      <td>3.032131</td>\n",
       "      <td>20.885612</td>\n",
       "      <td>16.332755</td>\n",
       "      <td>14.351159</td>\n",
       "      <td>45.064090</td>\n",
       "      <td>6.990180</td>\n",
       "      <td>0.274666</td>\n",
       "      <td>8.469078</td>\n",
       "      <td>0.426544</td>\n",
       "      <td>9.215697</td>\n",
       "      <td>8.182799</td>\n",
       "      <td>82.032125</td>\n",
       "      <td>0.492157</td>\n",
       "      <td>0.190421</td>\n",
       "      <td>0.279424</td>\n",
       "      <td>5.248441</td>\n",
       "      <td>9.040411</td>\n",
       "      <td>23.922032</td>\n",
       "      <td>2.857951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.748040</td>\n",
       "      <td>0.145844</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1218.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.301920</td>\n",
       "      <td>0.230987</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>79.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26.875000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>135.636364</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2465.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>111.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.835873</td>\n",
       "      <td>0.268674</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26.875000</td>\n",
       "      <td>71.409223</td>\n",
       "      <td>135.636364</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3750.250000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.578256</td>\n",
       "      <td>0.506778</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26.875000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>135.636364</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4999.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>579.000000</td>\n",
       "      <td>53.400629</td>\n",
       "      <td>2.302072</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>579.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>579.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             index  Pregnancies      Glucose  BloodPressure  SkinThickness  \\\n",
       "count  3000.000000  3000.000000  3000.000000    3000.000000    3000.000000   \n",
       "mean   2492.964667     3.557000   113.733667      68.743667      11.164000   \n",
       "std    1447.218078     3.032131    20.885612      16.332755      14.351159   \n",
       "min       0.000000     0.000000    57.000000       0.000000       0.000000   \n",
       "25%    1218.750000     1.000000   100.000000      64.000000       0.000000   \n",
       "50%    2465.500000     3.000000   111.000000      70.000000       0.000000   \n",
       "75%    3750.250000     6.000000   125.000000      78.000000      24.000000   \n",
       "max    4999.000000    13.000000   196.000000     110.000000      49.000000   \n",
       "\n",
       "           Insulin          BMI  DiabetesPedigreeFunction          Age  \\\n",
       "count  3000.000000  3000.000000               3000.000000  3000.000000   \n",
       "mean     11.663333    35.408959                  0.400476    28.932000   \n",
       "std      45.064090     6.990180                  0.274666     8.469078   \n",
       "min       0.000000     7.748040                  0.145844    21.000000   \n",
       "25%       0.000000    32.301920                  0.230987    22.000000   \n",
       "50%       0.000000    33.835873                  0.268674    26.000000   \n",
       "75%       0.000000    39.578256                  0.506778    33.000000   \n",
       "max     579.000000    53.400629                  2.302072    67.000000   \n",
       "\n",
       "           Outcome  BloodPressure_0  SkinThickness_0   Insulin_0  \\\n",
       "count  3000.000000      2887.000000      1234.000000  256.000000   \n",
       "mean      0.239000        71.434361        27.141005  136.679688   \n",
       "std       0.426544         9.215697         8.182799   82.032125   \n",
       "min       0.000000        46.000000         7.000000   15.000000   \n",
       "25%       0.000000        64.000000        19.000000   79.250000   \n",
       "50%       0.000000        70.000000        28.000000  126.000000   \n",
       "75%       0.000000        78.000000        33.000000  180.000000   \n",
       "max       1.000000       110.000000        49.000000  579.000000   \n",
       "\n",
       "       SkinThickness_na  BloodPressure_na   Insulin_na  SkinThickness_mean  \\\n",
       "count       3000.000000       3000.000000  3000.000000         3000.000000   \n",
       "mean           0.588667          0.037667     0.914667           26.984417   \n",
       "std            0.492157          0.190421     0.279424            5.248441   \n",
       "min            0.000000          0.000000     0.000000            7.000000   \n",
       "25%            0.000000          0.000000     1.000000           26.875000   \n",
       "50%            1.000000          0.000000     1.000000           26.875000   \n",
       "75%            1.000000          0.000000     1.000000           26.875000   \n",
       "max            1.000000          1.000000     1.000000           49.000000   \n",
       "\n",
       "       BloodPressure_mean  Insulin_mean  Pregnancies_bin  \n",
       "count         3000.000000   3000.000000      3000.000000  \n",
       "mean            71.433414    135.725394         5.594000  \n",
       "std              9.040411     23.922032         2.857951  \n",
       "min             46.000000     15.000000         3.000000  \n",
       "25%             64.000000    135.636364         3.000000  \n",
       "50%             71.409223    135.636364         6.000000  \n",
       "75%             78.000000    135.636364        10.000000  \n",
       "max            110.000000    579.000000        10.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0b40f27-4ece-46b4-a6c7-38b12b2ebd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[['DiabetesPedigreeFunction',\n",
    "                 'BMI',\n",
    "                 'Glucose',\n",
    "                 'Age',\n",
    "                 'Pregnancies',\n",
    "                 'BloodPressure_mean', \n",
    "                 'SkinThickness_mean',\n",
    "                 'Insulin_mean',\n",
    "                ]]\n",
    "id_train = train[['index']]\n",
    "y_train = train[['Outcome']]\n",
    "\n",
    "X_test = test[['DiabetesPedigreeFunction',\n",
    "                 'BMI',\n",
    "                 'Glucose',\n",
    "                 'Age',\n",
    "                 'Pregnancies',\n",
    "                 'BloodPressure_mean', \n",
    "                 'SkinThickness_mean',\n",
    "                 'Insulin_mean',\n",
    "                ]]\n",
    "id_test = test[['index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c1bfd15-d3ee-4ebd-81b1-7c17ab4d0042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 標準化\n",
    "change_cloumns = ['DiabetesPedigreeFunction',\n",
    "                 'BMI',\n",
    "                 'Glucose',\n",
    "                 'Age',\n",
    "                 'Pregnancies',\n",
    "                 'BloodPressure_mean', \n",
    "                 'SkinThickness_mean',\n",
    "                 'Insulin_mean',\n",
    "                ]\n",
    "\n",
    "for col in change_cloumns :\n",
    "    value_mean = X_train[col].mean()\n",
    "    value_std = X_train[col].std()\n",
    "    X_train[col] = (X_train[col] - value_mean) /  value_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcd87649-e08e-4882-9b60-317611485b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in change_cloumns :\n",
    "    value_mean = X_test[col].mean()\n",
    "    value_std = X_test[col].std()\n",
    "    X_test[col] = (X_test[col] - value_mean) / value_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b322d24d-594b-43f5-98e9-037f78b28046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>Age</th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>BloodPressure_mean</th>\n",
       "      <th>SkinThickness_mean</th>\n",
       "      <th>Insulin_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.000000e+03</td>\n",
       "      <td>3.000000e+03</td>\n",
       "      <td>3.000000e+03</td>\n",
       "      <td>3.000000e+03</td>\n",
       "      <td>3.000000e+03</td>\n",
       "      <td>3.000000e+03</td>\n",
       "      <td>3.000000e+03</td>\n",
       "      <td>3.000000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.598721e-16</td>\n",
       "      <td>-5.796844e-16</td>\n",
       "      <td>1.006602e-16</td>\n",
       "      <td>1.563194e-16</td>\n",
       "      <td>4.618528e-17</td>\n",
       "      <td>1.539509e-17</td>\n",
       "      <td>-2.676378e-16</td>\n",
       "      <td>8.076502e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-9.270594e-01</td>\n",
       "      <td>-3.957111e+00</td>\n",
       "      <td>-2.716399e+00</td>\n",
       "      <td>-9.365836e-01</td>\n",
       "      <td>-1.173103e+00</td>\n",
       "      <td>-2.813303e+00</td>\n",
       "      <td>-3.807686e+00</td>\n",
       "      <td>-5.046619e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-6.170720e-01</td>\n",
       "      <td>-4.444863e-01</td>\n",
       "      <td>-6.575659e-01</td>\n",
       "      <td>-8.185070e-01</td>\n",
       "      <td>-8.433014e-01</td>\n",
       "      <td>-8.222429e-01</td>\n",
       "      <td>-2.084746e-02</td>\n",
       "      <td>-3.721686e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-4.798629e-01</td>\n",
       "      <td>-2.250422e-01</td>\n",
       "      <td>-1.308876e-01</td>\n",
       "      <td>-3.462006e-01</td>\n",
       "      <td>-1.836992e-01</td>\n",
       "      <td>-2.675871e-03</td>\n",
       "      <td>-2.084746e-02</td>\n",
       "      <td>-3.721686e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.870248e-01</td>\n",
       "      <td>5.964506e-01</td>\n",
       "      <td>5.394304e-01</td>\n",
       "      <td>4.803356e-01</td>\n",
       "      <td>8.057041e-01</td>\n",
       "      <td>7.263592e-01</td>\n",
       "      <td>-2.084746e-02</td>\n",
       "      <td>-3.721686e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.923302e+00</td>\n",
       "      <td>2.573849e+00</td>\n",
       "      <td>3.938900e+00</td>\n",
       "      <td>4.494940e+00</td>\n",
       "      <td>3.114312e+00</td>\n",
       "      <td>4.266021e+00</td>\n",
       "      <td>4.194690e+00</td>\n",
       "      <td>1.852997e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       DiabetesPedigreeFunction           BMI       Glucose           Age  \\\n",
       "count              3.000000e+03  3.000000e+03  3.000000e+03  3.000000e+03   \n",
       "mean               1.598721e-16 -5.796844e-16  1.006602e-16  1.563194e-16   \n",
       "std                1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min               -9.270594e-01 -3.957111e+00 -2.716399e+00 -9.365836e-01   \n",
       "25%               -6.170720e-01 -4.444863e-01 -6.575659e-01 -8.185070e-01   \n",
       "50%               -4.798629e-01 -2.250422e-01 -1.308876e-01 -3.462006e-01   \n",
       "75%                3.870248e-01  5.964506e-01  5.394304e-01  4.803356e-01   \n",
       "max                6.923302e+00  2.573849e+00  3.938900e+00  4.494940e+00   \n",
       "\n",
       "        Pregnancies  BloodPressure_mean  SkinThickness_mean  Insulin_mean  \n",
       "count  3.000000e+03        3.000000e+03        3.000000e+03  3.000000e+03  \n",
       "mean   4.618528e-17        1.539509e-17       -2.676378e-16  8.076502e-16  \n",
       "std    1.000000e+00        1.000000e+00        1.000000e+00  1.000000e+00  \n",
       "min   -1.173103e+00       -2.813303e+00       -3.807686e+00 -5.046619e+00  \n",
       "25%   -8.433014e-01       -8.222429e-01       -2.084746e-02 -3.721686e-03  \n",
       "50%   -1.836992e-01       -2.675871e-03       -2.084746e-02 -3.721686e-03  \n",
       "75%    8.057041e-01        7.263592e-01       -2.084746e-02 -3.721686e-03  \n",
       "max    3.114312e+00        4.266021e+00        4.194690e+00  1.852997e+01  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1638f98-c1b6-48ee-a765-6369e686e7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>Age</th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>BloodPressure_mean</th>\n",
       "      <th>SkinThickness_mean</th>\n",
       "      <th>Insulin_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.000000e+03</td>\n",
       "      <td>2.000000e+03</td>\n",
       "      <td>2.000000e+03</td>\n",
       "      <td>2.000000e+03</td>\n",
       "      <td>2.000000e+03</td>\n",
       "      <td>2.000000e+03</td>\n",
       "      <td>2.000000e+03</td>\n",
       "      <td>2.000000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-7.949197e-17</td>\n",
       "      <td>-6.812328e-16</td>\n",
       "      <td>2.584599e-16</td>\n",
       "      <td>-1.922906e-16</td>\n",
       "      <td>-7.993606e-18</td>\n",
       "      <td>1.421085e-16</td>\n",
       "      <td>-2.948752e-16</td>\n",
       "      <td>1.481482e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-9.899934e-01</td>\n",
       "      <td>-3.791481e+00</td>\n",
       "      <td>-2.605523e+00</td>\n",
       "      <td>-9.421087e-01</td>\n",
       "      <td>-1.173625e+00</td>\n",
       "      <td>-3.676597e+00</td>\n",
       "      <td>-3.649031e+00</td>\n",
       "      <td>-4.061846e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-6.258261e-01</td>\n",
       "      <td>-4.332533e-01</td>\n",
       "      <td>-6.500222e-01</td>\n",
       "      <td>-8.254461e-01</td>\n",
       "      <td>-8.461629e-01</td>\n",
       "      <td>-8.122545e-01</td>\n",
       "      <td>3.200798e-02</td>\n",
       "      <td>-2.916476e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-4.885987e-01</td>\n",
       "      <td>-2.554457e-01</td>\n",
       "      <td>-1.497777e-01</td>\n",
       "      <td>-3.587958e-01</td>\n",
       "      <td>-1.912380e-01</td>\n",
       "      <td>3.997571e-03</td>\n",
       "      <td>3.200798e-02</td>\n",
       "      <td>-2.916476e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.920019e-01</td>\n",
       "      <td>5.921676e-01</td>\n",
       "      <td>4.868970e-01</td>\n",
       "      <td>4.578423e-01</td>\n",
       "      <td>7.911492e-01</td>\n",
       "      <td>7.300838e-01</td>\n",
       "      <td>3.200798e-02</td>\n",
       "      <td>-2.916476e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.643048e+00</td>\n",
       "      <td>2.504541e+00</td>\n",
       "      <td>3.852178e+00</td>\n",
       "      <td>4.424370e+00</td>\n",
       "      <td>3.083386e+00</td>\n",
       "      <td>4.255429e+00</td>\n",
       "      <td>4.931934e+00</td>\n",
       "      <td>2.030746e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       DiabetesPedigreeFunction           BMI       Glucose           Age  \\\n",
       "count              2.000000e+03  2.000000e+03  2.000000e+03  2.000000e+03   \n",
       "mean              -7.949197e-17 -6.812328e-16  2.584599e-16 -1.922906e-16   \n",
       "std                1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min               -9.899934e-01 -3.791481e+00 -2.605523e+00 -9.421087e-01   \n",
       "25%               -6.258261e-01 -4.332533e-01 -6.500222e-01 -8.254461e-01   \n",
       "50%               -4.885987e-01 -2.554457e-01 -1.497777e-01 -3.587958e-01   \n",
       "75%                3.920019e-01  5.921676e-01  4.868970e-01  4.578423e-01   \n",
       "max                6.643048e+00  2.504541e+00  3.852178e+00  4.424370e+00   \n",
       "\n",
       "        Pregnancies  BloodPressure_mean  SkinThickness_mean  Insulin_mean  \n",
       "count  2.000000e+03        2.000000e+03        2.000000e+03  2.000000e+03  \n",
       "mean  -7.993606e-18        1.421085e-16       -2.948752e-16  1.481482e-15  \n",
       "std    1.000000e+00        1.000000e+00        1.000000e+00  1.000000e+00  \n",
       "min   -1.173625e+00       -3.676597e+00       -3.649031e+00 -4.061846e+00  \n",
       "25%   -8.461629e-01       -8.122545e-01        3.200798e-02 -2.916476e-02  \n",
       "50%   -1.912380e-01        3.997571e-03        3.200798e-02 -2.916476e-02  \n",
       "75%    7.911492e-01        7.300838e-01        3.200798e-02 -2.916476e-02  \n",
       "max    3.083386e+00        4.255429e+00        4.931934e+00  2.030746e+01  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b84c0a7-0533-442f-94a4-cf5d5b9720f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 19:14:11.886831: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-15 19:14:13.234390: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-15 19:14:13.234486: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-15 19:14:13.377591: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-03-15 19:14:17.048827: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-15 19:14:17.050934: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-15 19:14:17.050969: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "random_state=123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "188c5d42-7fcd-4c2c-a2d6-e9a8896de4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    session_conf = tf.compat.v1.ConfigProto(\n",
    "        intra_op_parallelism_threads=1,\n",
    "        inter_op_parallelism_threads=1\n",
    "    )\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11843a2-84ac-4a99-8f3d-bc716ae893f7",
   "metadata": {},
   "source": [
    "## NNでの評価（CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81657fd1-039d-4c1a-a464-20b69ac9ff4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    input_num = Input(shape=(8,))\n",
    "    x_num = Dense(10, activation='relu')(input_num)\n",
    "    x_num = BatchNormalization()(x_num)\n",
    "    x_num = Dropout(0.3)(x_num)\n",
    "    x_num = Dense(10, activation='relu')(x_num)\n",
    "    x_num = BatchNormalization()(x_num)\n",
    "    x_num = Dropout(0.2)(x_num)\n",
    "    x_num = Dense(5, activation='relu')(x_num)\n",
    "    x_num = BatchNormalization()(x_num)\n",
    "    x_num = Dropout(0.1)(x_num)\n",
    "    out = Dense(1, activation='sigmoid')(x_num)\n",
    "    \n",
    "    model = Model(inputs=input_num, outputs=out,)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='Adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['binary_crossentropy'],\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44798591-6ec9-4ec6-9d9a-d8fd8c8701a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                90        \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 10)               40        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 19:14:20.740339: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-15 19:14:20.741727: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-15 19:14:20.741802: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (3ed327de65ae): /proc/driver/nvidia/version does not exist\n",
      "2023-03-15 19:14:20.744296: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 10)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 10)               40        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 55        \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 5)                20        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 5)                 0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 361\n",
      "Trainable params: 311\n",
      "Non-trainable params: 50\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# モデルの確認\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79e21f82-16fc-49ed-b576-42909918c601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cvでの評価用\n",
    "def train_nn(input_x,\n",
    "              input_y,\n",
    "              input_id,\n",
    "              list_nfold=[0,1,2,3,4],\n",
    "              n_splits=5,\n",
    "              random_state=123\n",
    "            ):\n",
    "    train_oof = np.zeros(len(input_x))\n",
    "    # foldごとの推論値\n",
    "    metrics = []\n",
    "    imp = pd.DataFrame()\n",
    "                         \n",
    "    cv = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state).\n",
    "              split(input_x, input_y))\n",
    "    \n",
    "    for nfold in list_nfold:\n",
    "        print('-'*20, nfold, '-'*20)\n",
    "        \n",
    "        idx_tr, idx_va = cv[nfold][0], cv[nfold][1]\n",
    "        x_tr, y_tr = input_x.loc[idx_tr, :], input_y.loc[idx_tr, :]\n",
    "        x_va, y_va = input_x.loc[idx_va, :], input_y.loc[idx_va, :]\n",
    "        print(x_tr.shape, y_tr.shape)\n",
    "        print(x_va.shape, y_va.shape)\n",
    "        print('y_train:{:.3f}, y_tr:{:.3f}, y_va{:.3f}'.\n",
    "              format(y_train['Outcome'].mean(), y_tr['Outcome'].mean(), y_va['Outcome'].mean(),))\n",
    "\n",
    "        model = create_model()\n",
    "        model.fit(x=x_tr,\n",
    "                  y=y_tr,\n",
    "                 validation_data=(x_va, y_va),\n",
    "                 batch_size=8,\n",
    "                 epochs=1000,\n",
    "                 callbacks=[ModelCheckpoint(filepath='model_keras.h5',\n",
    "                                            moniter='val_loss',\n",
    "                                            mode='min', \n",
    "                                            verbose=1,\n",
    "                                            save_best_only=True,\n",
    "                                            ),\n",
    "                            EarlyStopping(monitor='val_loss',\n",
    "                                          mode='min',\n",
    "                                          min_delta=0,\n",
    "                                          patience=5,\n",
    "                                          verbose=1,\n",
    "                                          restore_best_weights=True),\n",
    "                            ReduceLROnPlateau(moniter='val_loss',\n",
    "                                             mode='min',\n",
    "                                             factor=0.1,\n",
    "                                             patience=5,\n",
    "                                             verbose=1),\n",
    "                           ],\n",
    "                  verbose=1,\n",
    "                 )\n",
    "\n",
    "        # モデルの保存\n",
    "        fname_nn = 'model/nn/model_nn_fold{}.pickle'.format(nfold)\n",
    "        with open(fname_nn, 'wb')as f:\n",
    "            pickle.dump(model, f, protocol=4)\n",
    "            \n",
    "            \n",
    "        # 評価\n",
    "        y_tr_pred = model.predict(x_tr)\n",
    "        y_va_pred = model.predict(x_va)\n",
    "        metric_tr = accuracy_score(y_tr, np.where(y_tr_pred>=0.5,1,0))\n",
    "        metric_va = accuracy_score(y_va, np.where(y_va_pred>=0.5,1,0))\n",
    "        print('[accuracy] tr: {:.2f}, va: {:2f}'.\n",
    "             format(metric_tr, metric_va))\n",
    "        metrics.append([nfold, metric_tr, metric_va])\n",
    "        \n",
    "        # oof\n",
    "        train_oof[idx_va] = np.squeeze(y_va_pred)\n",
    "        \n",
    "        \n",
    "        print('-'*20, 'result', '-'*20)\n",
    "    \n",
    "    # metrix出力\n",
    "    metrics = np.array(metrics)\n",
    "    print(metrics)\n",
    "    print('[cv] tr: {:.2f}+-{:.2f}, va: {:.2f}'.format(\n",
    "        metrics[:,1].mean(), metrics[:,1].std(),\n",
    "        metrics[:,2].mean(), metrics[:,2].std()\n",
    "    ))\n",
    "    print('[oof] {:.4f}'.format(\n",
    "        accuracy_score(input_y, np.where(train_oof>=0.5,1,0))))\n",
    "    # oof出力  \n",
    "    train_oof = pd.concat([\n",
    "        input_id,\n",
    "        pd.DataFrame({'pred':train_oof})]\n",
    "        ,axis=1)\n",
    "\n",
    "    print('Done')\n",
    "    \n",
    "    return train_oof, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf1c7951-6b35-4f07-894d-814aa7eca215",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- 0 --------------------\n",
      "(2400, 8) (2400, 1)\n",
      "(600, 8) (600, 1)\n",
      "y_train:0.239, y_tr:0.239, y_va0.240\n",
      "Epoch 1/1000\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6972 - binary_crossentropy: 0.6972\n",
      "Epoch 1: val_loss improved from inf to 0.54997, saving model to model_keras.h5\n",
      "300/300 [==============================] - 3s 7ms/step - loss: 0.6972 - binary_crossentropy: 0.6972 - val_loss: 0.5500 - val_binary_crossentropy: 0.5500 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "290/300 [============================>.] - ETA: 0s - loss: 0.5837 - binary_crossentropy: 0.5837\n",
      "Epoch 2: val_loss improved from 0.54997 to 0.51006, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5814 - binary_crossentropy: 0.5814 - val_loss: 0.5101 - val_binary_crossentropy: 0.5101 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.5496 - binary_crossentropy: 0.5496\n",
      "Epoch 3: val_loss improved from 0.51006 to 0.50196, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5496 - binary_crossentropy: 0.5496 - val_loss: 0.5020 - val_binary_crossentropy: 0.5020 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "289/300 [===========================>..] - ETA: 0s - loss: 0.5327 - binary_crossentropy: 0.5327\n",
      "Epoch 4: val_loss improved from 0.50196 to 0.49252, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5359 - binary_crossentropy: 0.5359 - val_loss: 0.4925 - val_binary_crossentropy: 0.4925 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "292/300 [============================>.] - ETA: 0s - loss: 0.5355 - binary_crossentropy: 0.5355\n",
      "Epoch 5: val_loss improved from 0.49252 to 0.48405, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.5329 - binary_crossentropy: 0.5329 - val_loss: 0.4841 - val_binary_crossentropy: 0.4841 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "291/300 [============================>.] - ETA: 0s - loss: 0.5230 - binary_crossentropy: 0.5230\n",
      "Epoch 6: val_loss improved from 0.48405 to 0.47597, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5214 - binary_crossentropy: 0.5214 - val_loss: 0.4760 - val_binary_crossentropy: 0.4760 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "292/300 [============================>.] - ETA: 0s - loss: 0.5090 - binary_crossentropy: 0.5090\n",
      "Epoch 7: val_loss improved from 0.47597 to 0.47120, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.5101 - binary_crossentropy: 0.5101 - val_loss: 0.4712 - val_binary_crossentropy: 0.4712 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "293/300 [============================>.] - ETA: 0s - loss: 0.5157 - binary_crossentropy: 0.5157\n",
      "Epoch 8: val_loss did not improve from 0.47120\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.5173 - binary_crossentropy: 0.5173 - val_loss: 0.4729 - val_binary_crossentropy: 0.4729 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "297/300 [============================>.] - ETA: 0s - loss: 0.5163 - binary_crossentropy: 0.5163\n",
      "Epoch 9: val_loss improved from 0.47120 to 0.47052, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5175 - binary_crossentropy: 0.5175 - val_loss: 0.4705 - val_binary_crossentropy: 0.4705 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "289/300 [===========================>..] - ETA: 0s - loss: 0.5176 - binary_crossentropy: 0.5176\n",
      "Epoch 10: val_loss did not improve from 0.47052\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5151 - binary_crossentropy: 0.5151 - val_loss: 0.4710 - val_binary_crossentropy: 0.4710 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "298/300 [============================>.] - ETA: 0s - loss: 0.4998 - binary_crossentropy: 0.4998\n",
      "Epoch 11: val_loss improved from 0.47052 to 0.46543, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5001 - binary_crossentropy: 0.5001 - val_loss: 0.4654 - val_binary_crossentropy: 0.4654 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "297/300 [============================>.] - ETA: 0s - loss: 0.5060 - binary_crossentropy: 0.5060\n",
      "Epoch 12: val_loss did not improve from 0.46543\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5069 - binary_crossentropy: 0.5069 - val_loss: 0.4655 - val_binary_crossentropy: 0.4655 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "297/300 [============================>.] - ETA: 0s - loss: 0.5007 - binary_crossentropy: 0.5007\n",
      "Epoch 13: val_loss improved from 0.46543 to 0.46512, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5000 - binary_crossentropy: 0.5000 - val_loss: 0.4651 - val_binary_crossentropy: 0.4651 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "293/300 [============================>.] - ETA: 0s - loss: 0.5062 - binary_crossentropy: 0.5062\n",
      "Epoch 14: val_loss improved from 0.46512 to 0.46493, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5037 - binary_crossentropy: 0.5037 - val_loss: 0.4649 - val_binary_crossentropy: 0.4649 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "297/300 [============================>.] - ETA: 0s - loss: 0.5022 - binary_crossentropy: 0.5022\n",
      "Epoch 15: val_loss improved from 0.46493 to 0.46286, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5009 - binary_crossentropy: 0.5009 - val_loss: 0.4629 - val_binary_crossentropy: 0.4629 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5012 - binary_crossentropy: 0.5012\n",
      "Epoch 16: val_loss improved from 0.46286 to 0.46059, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5022 - binary_crossentropy: 0.5022 - val_loss: 0.4606 - val_binary_crossentropy: 0.4606 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "295/300 [============================>.] - ETA: 0s - loss: 0.5026 - binary_crossentropy: 0.5026\n",
      "Epoch 17: val_loss did not improve from 0.46059\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5022 - binary_crossentropy: 0.5022 - val_loss: 0.4617 - val_binary_crossentropy: 0.4617 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.5028 - binary_crossentropy: 0.5028\n",
      "Epoch 18: val_loss did not improve from 0.46059\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5028 - binary_crossentropy: 0.5028 - val_loss: 0.4636 - val_binary_crossentropy: 0.4636 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "292/300 [============================>.] - ETA: 0s - loss: 0.5075 - binary_crossentropy: 0.5075\n",
      "Epoch 19: val_loss did not improve from 0.46059\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5073 - binary_crossentropy: 0.5073 - val_loss: 0.4652 - val_binary_crossentropy: 0.4652 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "294/300 [============================>.] - ETA: 0s - loss: 0.4912 - binary_crossentropy: 0.4912\n",
      "Epoch 20: val_loss improved from 0.46059 to 0.45919, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4949 - binary_crossentropy: 0.4949 - val_loss: 0.4592 - val_binary_crossentropy: 0.4592 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "293/300 [============================>.] - ETA: 0s - loss: 0.5018 - binary_crossentropy: 0.5018\n",
      "Epoch 21: val_loss did not improve from 0.45919\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5013 - binary_crossentropy: 0.5013 - val_loss: 0.4610 - val_binary_crossentropy: 0.4610 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "295/300 [============================>.] - ETA: 0s - loss: 0.4985 - binary_crossentropy: 0.4985\n",
      "Epoch 22: val_loss did not improve from 0.45919\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4977 - binary_crossentropy: 0.4977 - val_loss: 0.4616 - val_binary_crossentropy: 0.4616 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.4977 - binary_crossentropy: 0.4977\n",
      "Epoch 23: val_loss did not improve from 0.45919\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5002 - binary_crossentropy: 0.5002 - val_loss: 0.4616 - val_binary_crossentropy: 0.4616 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.5004 - binary_crossentropy: 0.5004\n",
      "Epoch 24: val_loss did not improve from 0.45919\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4997 - binary_crossentropy: 0.4997 - val_loss: 0.4629 - val_binary_crossentropy: 0.4629 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "296/300 [============================>.] - ETA: 0s - loss: 0.5001 - binary_crossentropy: 0.5001\n",
      "Epoch 25: val_loss did not improve from 0.45919\n",
      "Restoring model weights from the end of the best epoch: 20.\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4992 - binary_crossentropy: 0.4992 - val_loss: 0.4610 - val_binary_crossentropy: 0.4610 - lr: 0.0010\n",
      "Epoch 25: early stopping\n",
      "INFO:tensorflow:Assets written to: ram://2f963b42-d7de-4748-ab4c-c337c6e64e16/assets\n",
      "75/75 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "[accuracy] tr: 0.77, va: 0.756667\n",
      "-------------------- result --------------------\n",
      "-------------------- 1 --------------------\n",
      "(2400, 8) (2400, 1)\n",
      "(600, 8) (600, 1)\n",
      "y_train:0.239, y_tr:0.239, y_va0.240\n",
      "Epoch 1/1000\n",
      "293/300 [============================>.] - ETA: 0s - loss: 0.7767 - binary_crossentropy: 0.7767\n",
      "Epoch 1: val_loss improved from inf to 0.60512, saving model to model_keras.h5\n",
      "300/300 [==============================] - 3s 6ms/step - loss: 0.7754 - binary_crossentropy: 0.7754 - val_loss: 0.6051 - val_binary_crossentropy: 0.6051 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "298/300 [============================>.] - ETA: 0s - loss: 0.6091 - binary_crossentropy: 0.6091\n",
      "Epoch 2: val_loss improved from 0.60512 to 0.54427, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.6095 - binary_crossentropy: 0.6095 - val_loss: 0.5443 - val_binary_crossentropy: 0.5443 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "294/300 [============================>.] - ETA: 0s - loss: 0.5779 - binary_crossentropy: 0.5779\n",
      "Epoch 3: val_loss improved from 0.54427 to 0.52785, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5756 - binary_crossentropy: 0.5756 - val_loss: 0.5278 - val_binary_crossentropy: 0.5278 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.5541 - binary_crossentropy: 0.5541\n",
      "Epoch 4: val_loss improved from 0.52785 to 0.51672, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5541 - binary_crossentropy: 0.5541 - val_loss: 0.5167 - val_binary_crossentropy: 0.5167 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "289/300 [===========================>..] - ETA: 0s - loss: 0.5469 - binary_crossentropy: 0.5469\n",
      "Epoch 5: val_loss improved from 0.51672 to 0.50810, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.5507 - binary_crossentropy: 0.5507 - val_loss: 0.5081 - val_binary_crossentropy: 0.5081 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "294/300 [============================>.] - ETA: 0s - loss: 0.5376 - binary_crossentropy: 0.5376\n",
      "Epoch 6: val_loss improved from 0.50810 to 0.50230, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 0.5368 - binary_crossentropy: 0.5368 - val_loss: 0.5023 - val_binary_crossentropy: 0.5023 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "297/300 [============================>.] - ETA: 0s - loss: 0.5368 - binary_crossentropy: 0.5368\n",
      "Epoch 7: val_loss improved from 0.50230 to 0.49289, saving model to model_keras.h5\n",
      "300/300 [==============================] - 3s 10ms/step - loss: 0.5364 - binary_crossentropy: 0.5364 - val_loss: 0.4929 - val_binary_crossentropy: 0.4929 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.5282 - binary_crossentropy: 0.5282\n",
      "Epoch 8: val_loss improved from 0.49289 to 0.48596, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 8ms/step - loss: 0.5282 - binary_crossentropy: 0.5282 - val_loss: 0.4860 - val_binary_crossentropy: 0.4860 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "294/300 [============================>.] - ETA: 0s - loss: 0.5280 - binary_crossentropy: 0.5280\n",
      "Epoch 9: val_loss improved from 0.48596 to 0.47920, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 0.5291 - binary_crossentropy: 0.5291 - val_loss: 0.4792 - val_binary_crossentropy: 0.4792 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "292/300 [============================>.] - ETA: 0s - loss: 0.5173 - binary_crossentropy: 0.5173\n",
      "Epoch 10: val_loss improved from 0.47920 to 0.47535, saving model to model_keras.h5\n",
      "300/300 [==============================] - 3s 8ms/step - loss: 0.5184 - binary_crossentropy: 0.5184 - val_loss: 0.4754 - val_binary_crossentropy: 0.4754 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "295/300 [============================>.] - ETA: 0s - loss: 0.5160 - binary_crossentropy: 0.5160\n",
      "Epoch 11: val_loss improved from 0.47535 to 0.47174, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 8ms/step - loss: 0.5147 - binary_crossentropy: 0.5147 - val_loss: 0.4717 - val_binary_crossentropy: 0.4717 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "293/300 [============================>.] - ETA: 0s - loss: 0.5116 - binary_crossentropy: 0.5116\n",
      "Epoch 12: val_loss improved from 0.47174 to 0.46796, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 8ms/step - loss: 0.5090 - binary_crossentropy: 0.5090 - val_loss: 0.4680 - val_binary_crossentropy: 0.4680 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.5189 - binary_crossentropy: 0.5189\n",
      "Epoch 13: val_loss did not improve from 0.46796\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 0.5186 - binary_crossentropy: 0.5186 - val_loss: 0.4701 - val_binary_crossentropy: 0.4701 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "298/300 [============================>.] - ETA: 0s - loss: 0.5037 - binary_crossentropy: 0.5037\n",
      "Epoch 14: val_loss improved from 0.46796 to 0.46475, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.5034 - binary_crossentropy: 0.5034 - val_loss: 0.4648 - val_binary_crossentropy: 0.4648 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "295/300 [============================>.] - ETA: 0s - loss: 0.5065 - binary_crossentropy: 0.5065\n",
      "Epoch 15: val_loss did not improve from 0.46475\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5050 - binary_crossentropy: 0.5050 - val_loss: 0.4648 - val_binary_crossentropy: 0.4648 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5131 - binary_crossentropy: 0.5131\n",
      "Epoch 16: val_loss did not improve from 0.46475\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.5107 - binary_crossentropy: 0.5107 - val_loss: 0.4679 - val_binary_crossentropy: 0.4679 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "294/300 [============================>.] - ETA: 0s - loss: 0.5068 - binary_crossentropy: 0.5068\n",
      "Epoch 17: val_loss did not improve from 0.46475\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5083 - binary_crossentropy: 0.5083 - val_loss: 0.4665 - val_binary_crossentropy: 0.4665 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.5054 - binary_crossentropy: 0.5054\n",
      "Epoch 18: val_loss did not improve from 0.46475\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5054 - binary_crossentropy: 0.5054 - val_loss: 0.4678 - val_binary_crossentropy: 0.4678 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "293/300 [============================>.] - ETA: 0s - loss: 0.5078 - binary_crossentropy: 0.5078\n",
      "Epoch 19: val_loss did not improve from 0.46475\n",
      "Restoring model weights from the end of the best epoch: 14.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5078 - binary_crossentropy: 0.5078 - val_loss: 0.4673 - val_binary_crossentropy: 0.4673 - lr: 0.0010\n",
      "Epoch 19: early stopping\n",
      "INFO:tensorflow:Assets written to: ram://e9f6ce56-903f-4c2f-ae03-89b1bbc96bd5/assets\n",
      "75/75 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "[accuracy] tr: 0.76, va: 0.761667\n",
      "-------------------- result --------------------\n",
      "-------------------- 2 --------------------\n",
      "(2400, 8) (2400, 1)\n",
      "(600, 8) (600, 1)\n",
      "y_train:0.239, y_tr:0.239, y_va0.238\n",
      "Epoch 1/1000\n",
      "290/300 [============================>.] - ETA: 0s - loss: 0.7277 - binary_crossentropy: 0.7277\n",
      "Epoch 1: val_loss improved from inf to 0.54989, saving model to model_keras.h5\n",
      "300/300 [==============================] - 3s 7ms/step - loss: 0.7251 - binary_crossentropy: 0.7251 - val_loss: 0.5499 - val_binary_crossentropy: 0.5499 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.5939 - binary_crossentropy: 0.5939\n",
      "Epoch 2: val_loss improved from 0.54989 to 0.53098, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5939 - binary_crossentropy: 0.5939 - val_loss: 0.5310 - val_binary_crossentropy: 0.5310 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.5596 - binary_crossentropy: 0.5596\n",
      "Epoch 3: val_loss improved from 0.53098 to 0.52312, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5590 - binary_crossentropy: 0.5590 - val_loss: 0.5231 - val_binary_crossentropy: 0.5231 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "296/300 [============================>.] - ETA: 0s - loss: 0.5318 - binary_crossentropy: 0.5318\n",
      "Epoch 4: val_loss improved from 0.52312 to 0.51778, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.5312 - binary_crossentropy: 0.5312 - val_loss: 0.5178 - val_binary_crossentropy: 0.5178 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5333 - binary_crossentropy: 0.5333\n",
      "Epoch 5: val_loss improved from 0.51778 to 0.51352, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.5284 - binary_crossentropy: 0.5284 - val_loss: 0.5135 - val_binary_crossentropy: 0.5135 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.5339 - binary_crossentropy: 0.5339\n",
      "Epoch 6: val_loss improved from 0.51352 to 0.50936, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 0.5339 - binary_crossentropy: 0.5339 - val_loss: 0.5094 - val_binary_crossentropy: 0.5094 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "293/300 [============================>.] - ETA: 0s - loss: 0.5290 - binary_crossentropy: 0.5290\n",
      "Epoch 7: val_loss improved from 0.50936 to 0.50855, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.5289 - binary_crossentropy: 0.5289 - val_loss: 0.5085 - val_binary_crossentropy: 0.5085 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "296/300 [============================>.] - ETA: 0s - loss: 0.5221 - binary_crossentropy: 0.5221\n",
      "Epoch 8: val_loss improved from 0.50855 to 0.50366, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5202 - binary_crossentropy: 0.5202 - val_loss: 0.5037 - val_binary_crossentropy: 0.5037 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "295/300 [============================>.] - ETA: 0s - loss: 0.5215 - binary_crossentropy: 0.5215\n",
      "Epoch 9: val_loss improved from 0.50366 to 0.50274, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5211 - binary_crossentropy: 0.5211 - val_loss: 0.5027 - val_binary_crossentropy: 0.5027 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.5087 - binary_crossentropy: 0.5087\n",
      "Epoch 10: val_loss improved from 0.50274 to 0.50095, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5088 - binary_crossentropy: 0.5088 - val_loss: 0.5009 - val_binary_crossentropy: 0.5009 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "293/300 [============================>.] - ETA: 0s - loss: 0.5130 - binary_crossentropy: 0.5130\n",
      "Epoch 11: val_loss improved from 0.50095 to 0.49977, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5135 - binary_crossentropy: 0.5135 - val_loss: 0.4998 - val_binary_crossentropy: 0.4998 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "289/300 [===========================>..] - ETA: 0s - loss: 0.5087 - binary_crossentropy: 0.5087\n",
      "Epoch 12: val_loss improved from 0.49977 to 0.49905, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5065 - binary_crossentropy: 0.5065 - val_loss: 0.4990 - val_binary_crossentropy: 0.4990 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "289/300 [===========================>..] - ETA: 0s - loss: 0.5174 - binary_crossentropy: 0.5174\n",
      "Epoch 13: val_loss improved from 0.49905 to 0.49822, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5149 - binary_crossentropy: 0.5149 - val_loss: 0.4982 - val_binary_crossentropy: 0.4982 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.5111 - binary_crossentropy: 0.5111\n",
      "Epoch 14: val_loss improved from 0.49822 to 0.49749, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5111 - binary_crossentropy: 0.5111 - val_loss: 0.4975 - val_binary_crossentropy: 0.4975 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "289/300 [===========================>..] - ETA: 0s - loss: 0.4987 - binary_crossentropy: 0.4987\n",
      "Epoch 15: val_loss improved from 0.49749 to 0.49561, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5024 - binary_crossentropy: 0.5024 - val_loss: 0.4956 - val_binary_crossentropy: 0.4956 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "296/300 [============================>.] - ETA: 0s - loss: 0.5042 - binary_crossentropy: 0.5042\n",
      "Epoch 16: val_loss did not improve from 0.49561\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 0.5041 - binary_crossentropy: 0.5041 - val_loss: 0.4979 - val_binary_crossentropy: 0.4979 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "298/300 [============================>.] - ETA: 0s - loss: 0.4948 - binary_crossentropy: 0.4948\n",
      "Epoch 17: val_loss did not improve from 0.49561\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.4944 - binary_crossentropy: 0.4944 - val_loss: 0.4958 - val_binary_crossentropy: 0.4958 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "297/300 [============================>.] - ETA: 0s - loss: 0.4990 - binary_crossentropy: 0.4990\n",
      "Epoch 18: val_loss improved from 0.49561 to 0.49482, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.4991 - binary_crossentropy: 0.4991 - val_loss: 0.4948 - val_binary_crossentropy: 0.4948 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "292/300 [============================>.] - ETA: 0s - loss: 0.4996 - binary_crossentropy: 0.4996\n",
      "Epoch 19: val_loss did not improve from 0.49482\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.4999 - binary_crossentropy: 0.4999 - val_loss: 0.4958 - val_binary_crossentropy: 0.4958 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "296/300 [============================>.] - ETA: 0s - loss: 0.4950 - binary_crossentropy: 0.4950\n",
      "Epoch 20: val_loss improved from 0.49482 to 0.49428, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4945 - binary_crossentropy: 0.4945 - val_loss: 0.4943 - val_binary_crossentropy: 0.4943 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "291/300 [============================>.] - ETA: 0s - loss: 0.4933 - binary_crossentropy: 0.4933\n",
      "Epoch 21: val_loss improved from 0.49428 to 0.49256, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4925 - binary_crossentropy: 0.4925 - val_loss: 0.4926 - val_binary_crossentropy: 0.4926 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.4987 - binary_crossentropy: 0.4987\n",
      "Epoch 22: val_loss did not improve from 0.49256\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4987 - binary_crossentropy: 0.4987 - val_loss: 0.4937 - val_binary_crossentropy: 0.4937 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "292/300 [============================>.] - ETA: 0s - loss: 0.4906 - binary_crossentropy: 0.4906\n",
      "Epoch 23: val_loss improved from 0.49256 to 0.49098, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4897 - binary_crossentropy: 0.4897 - val_loss: 0.4910 - val_binary_crossentropy: 0.4910 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "295/300 [============================>.] - ETA: 0s - loss: 0.5018 - binary_crossentropy: 0.5018\n",
      "Epoch 24: val_loss did not improve from 0.49098\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5009 - binary_crossentropy: 0.5009 - val_loss: 0.4911 - val_binary_crossentropy: 0.4911 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "291/300 [============================>.] - ETA: 0s - loss: 0.4976 - binary_crossentropy: 0.4976\n",
      "Epoch 25: val_loss improved from 0.49098 to 0.48877, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4964 - binary_crossentropy: 0.4964 - val_loss: 0.4888 - val_binary_crossentropy: 0.4888 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "298/300 [============================>.] - ETA: 0s - loss: 0.4910 - binary_crossentropy: 0.4910\n",
      "Epoch 26: val_loss improved from 0.48877 to 0.48735, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.4901 - binary_crossentropy: 0.4901 - val_loss: 0.4873 - val_binary_crossentropy: 0.4873 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "292/300 [============================>.] - ETA: 0s - loss: 0.4890 - binary_crossentropy: 0.4890\n",
      "Epoch 27: val_loss did not improve from 0.48735\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4910 - binary_crossentropy: 0.4910 - val_loss: 0.4883 - val_binary_crossentropy: 0.4883 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "295/300 [============================>.] - ETA: 0s - loss: 0.4950 - binary_crossentropy: 0.4950\n",
      "Epoch 28: val_loss did not improve from 0.48735\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4959 - binary_crossentropy: 0.4959 - val_loss: 0.4883 - val_binary_crossentropy: 0.4883 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.4909 - binary_crossentropy: 0.4909\n",
      "Epoch 29: val_loss did not improve from 0.48735\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4914 - binary_crossentropy: 0.4914 - val_loss: 0.4880 - val_binary_crossentropy: 0.4880 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "294/300 [============================>.] - ETA: 0s - loss: 0.4899 - binary_crossentropy: 0.4899\n",
      "Epoch 30: val_loss improved from 0.48735 to 0.48694, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4884 - binary_crossentropy: 0.4884 - val_loss: 0.4869 - val_binary_crossentropy: 0.4869 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "291/300 [============================>.] - ETA: 0s - loss: 0.4951 - binary_crossentropy: 0.4951\n",
      "Epoch 31: val_loss did not improve from 0.48694\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4941 - binary_crossentropy: 0.4941 - val_loss: 0.4871 - val_binary_crossentropy: 0.4871 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "295/300 [============================>.] - ETA: 0s - loss: 0.4852 - binary_crossentropy: 0.4852\n",
      "Epoch 32: val_loss did not improve from 0.48694\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4850 - binary_crossentropy: 0.4850 - val_loss: 0.4872 - val_binary_crossentropy: 0.4872 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "293/300 [============================>.] - ETA: 0s - loss: 0.4826 - binary_crossentropy: 0.4826\n",
      "Epoch 33: val_loss did not improve from 0.48694\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4864 - binary_crossentropy: 0.4864 - val_loss: 0.4872 - val_binary_crossentropy: 0.4872 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "298/300 [============================>.] - ETA: 0s - loss: 0.4949 - binary_crossentropy: 0.4949\n",
      "Epoch 34: val_loss improved from 0.48694 to 0.48577, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.4942 - binary_crossentropy: 0.4942 - val_loss: 0.4858 - val_binary_crossentropy: 0.4858 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "292/300 [============================>.] - ETA: 0s - loss: 0.4846 - binary_crossentropy: 0.4846\n",
      "Epoch 35: val_loss did not improve from 0.48577\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4875 - binary_crossentropy: 0.4875 - val_loss: 0.4870 - val_binary_crossentropy: 0.4870 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "289/300 [===========================>..] - ETA: 0s - loss: 0.4886 - binary_crossentropy: 0.4886\n",
      "Epoch 36: val_loss improved from 0.48577 to 0.48557, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4860 - binary_crossentropy: 0.4860 - val_loss: 0.4856 - val_binary_crossentropy: 0.4856 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "295/300 [============================>.] - ETA: 0s - loss: 0.4953 - binary_crossentropy: 0.4953\n",
      "Epoch 37: val_loss did not improve from 0.48557\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4934 - binary_crossentropy: 0.4934 - val_loss: 0.4861 - val_binary_crossentropy: 0.4861 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "294/300 [============================>.] - ETA: 0s - loss: 0.4844 - binary_crossentropy: 0.4844\n",
      "Epoch 38: val_loss did not improve from 0.48557\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.4860 - binary_crossentropy: 0.4860 - val_loss: 0.4863 - val_binary_crossentropy: 0.4863 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "292/300 [============================>.] - ETA: 0s - loss: 0.4892 - binary_crossentropy: 0.4892\n",
      "Epoch 39: val_loss did not improve from 0.48557\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4881 - binary_crossentropy: 0.4881 - val_loss: 0.4860 - val_binary_crossentropy: 0.4860 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "296/300 [============================>.] - ETA: 0s - loss: 0.4837 - binary_crossentropy: 0.4837\n",
      "Epoch 40: val_loss did not improve from 0.48557\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4824 - binary_crossentropy: 0.4824 - val_loss: 0.4861 - val_binary_crossentropy: 0.4861 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "292/300 [============================>.] - ETA: 0s - loss: 0.4819 - binary_crossentropy: 0.4819\n",
      "Epoch 41: val_loss did not improve from 0.48557\n",
      "Restoring model weights from the end of the best epoch: 36.\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4839 - binary_crossentropy: 0.4839 - val_loss: 0.4861 - val_binary_crossentropy: 0.4861 - lr: 0.0010\n",
      "Epoch 41: early stopping\n",
      "INFO:tensorflow:Assets written to: ram://446838a3-0338-43c4-897b-bee88f87d696/assets\n",
      "75/75 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "[accuracy] tr: 0.78, va: 0.770000\n",
      "-------------------- result --------------------\n",
      "-------------------- 3 --------------------\n",
      "(2400, 8) (2400, 1)\n",
      "(600, 8) (600, 1)\n",
      "y_train:0.239, y_tr:0.239, y_va0.238\n",
      "Epoch 1/1000\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.6826 - binary_crossentropy: 0.6826\n",
      "Epoch 1: val_loss improved from inf to 0.58142, saving model to model_keras.h5\n",
      "300/300 [==============================] - 4s 7ms/step - loss: 0.6831 - binary_crossentropy: 0.6831 - val_loss: 0.5814 - val_binary_crossentropy: 0.5814 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "290/300 [============================>.] - ETA: 0s - loss: 0.5742 - binary_crossentropy: 0.5742\n",
      "Epoch 2: val_loss improved from 0.58142 to 0.53593, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5737 - binary_crossentropy: 0.5737 - val_loss: 0.5359 - val_binary_crossentropy: 0.5359 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.5517 - binary_crossentropy: 0.5517\n",
      "Epoch 3: val_loss improved from 0.53593 to 0.51821, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.5510 - binary_crossentropy: 0.5510 - val_loss: 0.5182 - val_binary_crossentropy: 0.5182 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "287/300 [===========================>..] - ETA: 0s - loss: 0.5323 - binary_crossentropy: 0.5323\n",
      "Epoch 4: val_loss improved from 0.51821 to 0.51080, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.5305 - binary_crossentropy: 0.5305 - val_loss: 0.5108 - val_binary_crossentropy: 0.5108 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "292/300 [============================>.] - ETA: 0s - loss: 0.5310 - binary_crossentropy: 0.5310\n",
      "Epoch 5: val_loss improved from 0.51080 to 0.50059, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5283 - binary_crossentropy: 0.5283 - val_loss: 0.5006 - val_binary_crossentropy: 0.5006 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "293/300 [============================>.] - ETA: 0s - loss: 0.5265 - binary_crossentropy: 0.5265\n",
      "Epoch 6: val_loss improved from 0.50059 to 0.49321, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5241 - binary_crossentropy: 0.5241 - val_loss: 0.4932 - val_binary_crossentropy: 0.4932 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "293/300 [============================>.] - ETA: 0s - loss: 0.5102 - binary_crossentropy: 0.5102\n",
      "Epoch 7: val_loss improved from 0.49321 to 0.48484, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5131 - binary_crossentropy: 0.5131 - val_loss: 0.4848 - val_binary_crossentropy: 0.4848 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "293/300 [============================>.] - ETA: 0s - loss: 0.5157 - binary_crossentropy: 0.5157\n",
      "Epoch 8: val_loss improved from 0.48484 to 0.48435, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5166 - binary_crossentropy: 0.5166 - val_loss: 0.4843 - val_binary_crossentropy: 0.4843 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "293/300 [============================>.] - ETA: 0s - loss: 0.5045 - binary_crossentropy: 0.5045\n",
      "Epoch 9: val_loss improved from 0.48435 to 0.48040, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5054 - binary_crossentropy: 0.5054 - val_loss: 0.4804 - val_binary_crossentropy: 0.4804 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "296/300 [============================>.] - ETA: 0s - loss: 0.5029 - binary_crossentropy: 0.5029\n",
      "Epoch 10: val_loss improved from 0.48040 to 0.47932, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5053 - binary_crossentropy: 0.5053 - val_loss: 0.4793 - val_binary_crossentropy: 0.4793 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "294/300 [============================>.] - ETA: 0s - loss: 0.5021 - binary_crossentropy: 0.5021\n",
      "Epoch 11: val_loss improved from 0.47932 to 0.47655, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.5021 - binary_crossentropy: 0.5021 - val_loss: 0.4766 - val_binary_crossentropy: 0.4766 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "287/300 [===========================>..] - ETA: 0s - loss: 0.5061 - binary_crossentropy: 0.5061\n",
      "Epoch 12: val_loss did not improve from 0.47655\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5083 - binary_crossentropy: 0.5083 - val_loss: 0.4766 - val_binary_crossentropy: 0.4766 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "297/300 [============================>.] - ETA: 0s - loss: 0.4990 - binary_crossentropy: 0.4990\n",
      "Epoch 13: val_loss improved from 0.47655 to 0.47496, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.5000 - binary_crossentropy: 0.5000 - val_loss: 0.4750 - val_binary_crossentropy: 0.4750 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "290/300 [============================>.] - ETA: 0s - loss: 0.4990 - binary_crossentropy: 0.4990\n",
      "Epoch 14: val_loss improved from 0.47496 to 0.47451, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4982 - binary_crossentropy: 0.4982 - val_loss: 0.4745 - val_binary_crossentropy: 0.4745 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.5082 - binary_crossentropy: 0.5082\n",
      "Epoch 15: val_loss improved from 0.47451 to 0.47436, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 0.5082 - binary_crossentropy: 0.5082 - val_loss: 0.4744 - val_binary_crossentropy: 0.4744 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "295/300 [============================>.] - ETA: 0s - loss: 0.4967 - binary_crossentropy: 0.4967\n",
      "Epoch 16: val_loss improved from 0.47436 to 0.47270, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4984 - binary_crossentropy: 0.4984 - val_loss: 0.4727 - val_binary_crossentropy: 0.4727 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.4972 - binary_crossentropy: 0.4972\n",
      "Epoch 17: val_loss improved from 0.47270 to 0.47144, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4972 - binary_crossentropy: 0.4972 - val_loss: 0.4714 - val_binary_crossentropy: 0.4714 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.5030 - binary_crossentropy: 0.5030\n",
      "Epoch 18: val_loss did not improve from 0.47144\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5030 - binary_crossentropy: 0.5030 - val_loss: 0.4733 - val_binary_crossentropy: 0.4733 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "298/300 [============================>.] - ETA: 0s - loss: 0.4992 - binary_crossentropy: 0.4992\n",
      "Epoch 19: val_loss did not improve from 0.47144\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4981 - binary_crossentropy: 0.4981 - val_loss: 0.4726 - val_binary_crossentropy: 0.4726 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.4934 - binary_crossentropy: 0.4934\n",
      "Epoch 20: val_loss did not improve from 0.47144\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.4939 - binary_crossentropy: 0.4939 - val_loss: 0.4717 - val_binary_crossentropy: 0.4717 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "295/300 [============================>.] - ETA: 0s - loss: 0.4877 - binary_crossentropy: 0.4877\n",
      "Epoch 21: val_loss improved from 0.47144 to 0.46984, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4895 - binary_crossentropy: 0.4895 - val_loss: 0.4698 - val_binary_crossentropy: 0.4698 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "290/300 [============================>.] - ETA: 0s - loss: 0.4897 - binary_crossentropy: 0.4897\n",
      "Epoch 22: val_loss did not improve from 0.46984\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4924 - binary_crossentropy: 0.4924 - val_loss: 0.4705 - val_binary_crossentropy: 0.4705 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.4894 - binary_crossentropy: 0.4894\n",
      "Epoch 23: val_loss improved from 0.46984 to 0.46966, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4910 - binary_crossentropy: 0.4910 - val_loss: 0.4697 - val_binary_crossentropy: 0.4697 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.4979 - binary_crossentropy: 0.4979\n",
      "Epoch 24: val_loss did not improve from 0.46966\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4979 - binary_crossentropy: 0.4979 - val_loss: 0.4710 - val_binary_crossentropy: 0.4710 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.4971 - binary_crossentropy: 0.4971\n",
      "Epoch 25: val_loss did not improve from 0.46966\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4971 - binary_crossentropy: 0.4971 - val_loss: 0.4727 - val_binary_crossentropy: 0.4727 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "295/300 [============================>.] - ETA: 0s - loss: 0.4943 - binary_crossentropy: 0.4943\n",
      "Epoch 26: val_loss did not improve from 0.46966\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4960 - binary_crossentropy: 0.4960 - val_loss: 0.4718 - val_binary_crossentropy: 0.4718 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "289/300 [===========================>..] - ETA: 0s - loss: 0.4909 - binary_crossentropy: 0.4909\n",
      "Epoch 27: val_loss did not improve from 0.46966\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4912 - binary_crossentropy: 0.4912 - val_loss: 0.4701 - val_binary_crossentropy: 0.4701 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "295/300 [============================>.] - ETA: 0s - loss: 0.4966 - binary_crossentropy: 0.4966\n",
      "Epoch 28: val_loss did not improve from 0.46966\n",
      "Restoring model weights from the end of the best epoch: 23.\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4957 - binary_crossentropy: 0.4957 - val_loss: 0.4713 - val_binary_crossentropy: 0.4713 - lr: 0.0010\n",
      "Epoch 28: early stopping\n",
      "INFO:tensorflow:Assets written to: ram://857f3b83-d0e4-4060-83c8-3edd88a91583/assets\n",
      "75/75 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "[accuracy] tr: 0.78, va: 0.760000\n",
      "-------------------- result --------------------\n",
      "-------------------- 4 --------------------\n",
      "(2400, 8) (2400, 1)\n",
      "(600, 8) (600, 1)\n",
      "y_train:0.239, y_tr:0.239, y_va0.238\n",
      "Epoch 1/1000\n",
      "297/300 [============================>.] - ETA: 0s - loss: 0.8266 - binary_crossentropy: 0.8266\n",
      "Epoch 1: val_loss improved from inf to 0.68487, saving model to model_keras.h5\n",
      "300/300 [==============================] - 3s 6ms/step - loss: 0.8273 - binary_crossentropy: 0.8273 - val_loss: 0.6849 - val_binary_crossentropy: 0.6849 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5982 - binary_crossentropy: 0.5982\n",
      "Epoch 2: val_loss improved from 0.68487 to 0.59366, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5938 - binary_crossentropy: 0.5938 - val_loss: 0.5937 - val_binary_crossentropy: 0.5937 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "292/300 [============================>.] - ETA: 0s - loss: 0.5510 - binary_crossentropy: 0.5510\n",
      "Epoch 3: val_loss improved from 0.59366 to 0.56268, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5478 - binary_crossentropy: 0.5478 - val_loss: 0.5627 - val_binary_crossentropy: 0.5627 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "298/300 [============================>.] - ETA: 0s - loss: 0.5386 - binary_crossentropy: 0.5386\n",
      "Epoch 4: val_loss improved from 0.56268 to 0.54111, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5377 - binary_crossentropy: 0.5377 - val_loss: 0.5411 - val_binary_crossentropy: 0.5411 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "293/300 [============================>.] - ETA: 0s - loss: 0.5372 - binary_crossentropy: 0.5372\n",
      "Epoch 5: val_loss improved from 0.54111 to 0.52609, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5351 - binary_crossentropy: 0.5351 - val_loss: 0.5261 - val_binary_crossentropy: 0.5261 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "289/300 [===========================>..] - ETA: 0s - loss: 0.5270 - binary_crossentropy: 0.5270\n",
      "Epoch 6: val_loss improved from 0.52609 to 0.52165, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5284 - binary_crossentropy: 0.5284 - val_loss: 0.5216 - val_binary_crossentropy: 0.5216 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "296/300 [============================>.] - ETA: 0s - loss: 0.5188 - binary_crossentropy: 0.5188\n",
      "Epoch 7: val_loss improved from 0.52165 to 0.51838, saving model to model_keras.h5\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5194 - binary_crossentropy: 0.5194 - val_loss: 0.5184 - val_binary_crossentropy: 0.5184 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.5058 - binary_crossentropy: 0.5058\n",
      "Epoch 8: val_loss improved from 0.51838 to 0.51405, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5053 - binary_crossentropy: 0.5053 - val_loss: 0.5140 - val_binary_crossentropy: 0.5140 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.5135 - binary_crossentropy: 0.5135\n",
      "Epoch 9: val_loss improved from 0.51405 to 0.50975, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.5136 - binary_crossentropy: 0.5136 - val_loss: 0.5098 - val_binary_crossentropy: 0.5098 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "295/300 [============================>.] - ETA: 0s - loss: 0.5134 - binary_crossentropy: 0.5134\n",
      "Epoch 10: val_loss improved from 0.50975 to 0.50740, saving model to model_keras.h5\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5138 - binary_crossentropy: 0.5138 - val_loss: 0.5074 - val_binary_crossentropy: 0.5074 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "298/300 [============================>.] - ETA: 0s - loss: 0.5019 - binary_crossentropy: 0.5019\n",
      "Epoch 11: val_loss improved from 0.50740 to 0.50542, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5022 - binary_crossentropy: 0.5022 - val_loss: 0.5054 - val_binary_crossentropy: 0.5054 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5148 - binary_crossentropy: 0.5148\n",
      "Epoch 12: val_loss did not improve from 0.50542\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5150 - binary_crossentropy: 0.5150 - val_loss: 0.5056 - val_binary_crossentropy: 0.5056 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "293/300 [============================>.] - ETA: 0s - loss: 0.4985 - binary_crossentropy: 0.4985\n",
      "Epoch 13: val_loss improved from 0.50542 to 0.50332, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4986 - binary_crossentropy: 0.4986 - val_loss: 0.5033 - val_binary_crossentropy: 0.5033 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "290/300 [============================>.] - ETA: 0s - loss: 0.4943 - binary_crossentropy: 0.4943\n",
      "Epoch 14: val_loss improved from 0.50332 to 0.50295, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4927 - binary_crossentropy: 0.4927 - val_loss: 0.5029 - val_binary_crossentropy: 0.5029 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "290/300 [============================>.] - ETA: 0s - loss: 0.5052 - binary_crossentropy: 0.5052\n",
      "Epoch 15: val_loss improved from 0.50295 to 0.50142, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5039 - binary_crossentropy: 0.5039 - val_loss: 0.5014 - val_binary_crossentropy: 0.5014 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.5064 - binary_crossentropy: 0.5064\n",
      "Epoch 16: val_loss improved from 0.50142 to 0.49941, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.5064 - binary_crossentropy: 0.5064 - val_loss: 0.4994 - val_binary_crossentropy: 0.4994 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "286/300 [===========================>..] - ETA: 0s - loss: 0.4954 - binary_crossentropy: 0.4954\n",
      "Epoch 17: val_loss improved from 0.49941 to 0.49830, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4947 - binary_crossentropy: 0.4947 - val_loss: 0.4983 - val_binary_crossentropy: 0.4983 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "289/300 [===========================>..] - ETA: 0s - loss: 0.4947 - binary_crossentropy: 0.4947\n",
      "Epoch 18: val_loss improved from 0.49830 to 0.49789, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4927 - binary_crossentropy: 0.4927 - val_loss: 0.4979 - val_binary_crossentropy: 0.4979 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "289/300 [===========================>..] - ETA: 0s - loss: 0.5054 - binary_crossentropy: 0.5054\n",
      "Epoch 19: val_loss improved from 0.49789 to 0.49780, saving model to model_keras.h5\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5005 - binary_crossentropy: 0.5005 - val_loss: 0.4978 - val_binary_crossentropy: 0.4978 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.4944 - binary_crossentropy: 0.4944\n",
      "Epoch 20: val_loss did not improve from 0.49780\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4944 - binary_crossentropy: 0.4944 - val_loss: 0.4990 - val_binary_crossentropy: 0.4990 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.4846 - binary_crossentropy: 0.4846\n",
      "Epoch 21: val_loss improved from 0.49780 to 0.49708, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4896 - binary_crossentropy: 0.4896 - val_loss: 0.4971 - val_binary_crossentropy: 0.4971 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.4964 - binary_crossentropy: 0.4964\n",
      "Epoch 22: val_loss improved from 0.49708 to 0.49668, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4975 - binary_crossentropy: 0.4975 - val_loss: 0.4967 - val_binary_crossentropy: 0.4967 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.4877 - binary_crossentropy: 0.4877\n",
      "Epoch 23: val_loss did not improve from 0.49668\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4877 - binary_crossentropy: 0.4877 - val_loss: 0.4983 - val_binary_crossentropy: 0.4983 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "290/300 [============================>.] - ETA: 0s - loss: 0.4912 - binary_crossentropy: 0.4912\n",
      "Epoch 24: val_loss improved from 0.49668 to 0.49600, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.4898 - binary_crossentropy: 0.4898 - val_loss: 0.4960 - val_binary_crossentropy: 0.4960 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "290/300 [============================>.] - ETA: 0s - loss: 0.4948 - binary_crossentropy: 0.4948\n",
      "Epoch 25: val_loss did not improve from 0.49600\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4919 - binary_crossentropy: 0.4919 - val_loss: 0.4961 - val_binary_crossentropy: 0.4961 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.4906 - binary_crossentropy: 0.4906\n",
      "Epoch 26: val_loss improved from 0.49600 to 0.49553, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 0.4904 - binary_crossentropy: 0.4904 - val_loss: 0.4955 - val_binary_crossentropy: 0.4955 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "295/300 [============================>.] - ETA: 0s - loss: 0.4859 - binary_crossentropy: 0.4859\n",
      "Epoch 27: val_loss did not improve from 0.49553\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4899 - binary_crossentropy: 0.4899 - val_loss: 0.4962 - val_binary_crossentropy: 0.4962 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "298/300 [============================>.] - ETA: 0s - loss: 0.4919 - binary_crossentropy: 0.4919\n",
      "Epoch 28: val_loss did not improve from 0.49553\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4915 - binary_crossentropy: 0.4915 - val_loss: 0.4956 - val_binary_crossentropy: 0.4956 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "298/300 [============================>.] - ETA: 0s - loss: 0.4829 - binary_crossentropy: 0.4829\n",
      "Epoch 29: val_loss did not improve from 0.49553\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4824 - binary_crossentropy: 0.4824 - val_loss: 0.4968 - val_binary_crossentropy: 0.4968 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "296/300 [============================>.] - ETA: 0s - loss: 0.4848 - binary_crossentropy: 0.4848\n",
      "Epoch 30: val_loss did not improve from 0.49553\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4847 - binary_crossentropy: 0.4847 - val_loss: 0.4965 - val_binary_crossentropy: 0.4965 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "296/300 [============================>.] - ETA: 0s - loss: 0.4831 - binary_crossentropy: 0.4831\n",
      "Epoch 31: val_loss did not improve from 0.49553\n",
      "Restoring model weights from the end of the best epoch: 26.\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4820 - binary_crossentropy: 0.4820 - val_loss: 0.4976 - val_binary_crossentropy: 0.4976 - lr: 0.0010\n",
      "Epoch 31: early stopping\n",
      "INFO:tensorflow:Assets written to: ram://52929f39-44f0-4fd1-8c17-0a06af4a886e/assets\n",
      "75/75 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 4ms/step\n",
      "[accuracy] tr: 0.76, va: 0.761667\n",
      "-------------------- result --------------------\n",
      "[[0.         0.77125    0.75666667]\n",
      " [1.         0.76291667 0.76166667]\n",
      " [2.         0.77541667 0.77      ]\n",
      " [3.         0.77583333 0.76      ]\n",
      " [4.         0.76083333 0.76166667]]\n",
      "[cv] tr: 0.77+-0.01, va: 0.76\n",
      "[oof] 0.7620\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "train_oof, metrics = train_nn(X_train, y_train, id_train, list_nfold=[0,1,2,3,4], n_splits=5, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "669556ca-f257-450a-8f6b-17ea79cff3ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>0.384083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3832</td>\n",
       "      <td>0.104301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4927</td>\n",
       "      <td>0.444559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index      pred\n",
       "0    200  0.384083\n",
       "1   3832  0.104301\n",
       "2   4927  0.444559"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_oof[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaffb7f-6fa3-431f-bd0a-26028e65a3f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0c6f5328-321e-4d4c-b9e3-54e304d25b0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_nn(input_x,\n",
    "               input_id,\n",
    "               list_nfold=[0,1,2,3,4],\n",
    "               ):\n",
    "    pred = np.zeros((len(input_x), len(list_nfold)))\n",
    "    for nfold in list_nfold:\n",
    "        print('-'*20, nfold, '-'*20)\n",
    "        fname_nn = 'model/nn/model_nn_fold{}.pickle'.format(nfold)\n",
    "        with open(fname_nn, 'rb')as f:\n",
    "            model = pickle.load(f)\n",
    "        pred[:,nfold] = model.predict(input_x)[:,1]\n",
    "        \n",
    "    pred = pd.concat([\n",
    "        input_id,\n",
    "        pd.DataFrame({'pred':pred.mean(axis=1)}),], axis=1)\n",
    "    \n",
    "    print('Done')\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52c0dba4-b4e5-4909-98a9-4e004551adef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- 0 --------------------\n",
      "63/63 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_pred_proba \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_nn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mid_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mlist_nfold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                   \u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36mpredict_nn\u001b[0;34m(input_x, input_id, list_nfold)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(fname_nn, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     10\u001b[0m         model \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m---> 11\u001b[0m     pred[:,nfold] \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_x\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     13\u001b[0m pred \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\n\u001b[1;32m     14\u001b[0m     input_id,\n\u001b[1;32m     15\u001b[0m     pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m'\u001b[39m:pred\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)}),], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 1 with size 1"
     ]
    }
   ],
   "source": [
    "test_pred_proba = predict_nn(X_test,\n",
    "                    id_test,\n",
    "                    list_nfold=[0,1,2,3,4],\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59db41f5-f433-4373-bf72-57fb48f2a5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 0s 2ms/step\n",
      "accuracy: 0.7517\n"
     ]
    }
   ],
   "source": [
    "# 全データのモデルの結果の確認\n",
    "y_va_pred = model.predict(x_va, batch_size=8, verbose=1)\n",
    "print('accuracy: {:.4f}'.format(accuracy_score(y_va, np.where(y_va_pred>=0.5,1,0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25b5152f-4db6-4adf-95f5-e6ce25a30cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 1s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>398</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3833</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4836</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4572</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>636</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2545</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1161</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2230</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>148</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2530</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4070</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1261</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4682</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>906</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3170</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>483</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2825</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1778</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2466</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  Outcome\n",
       "0     398        0\n",
       "1    3833        0\n",
       "2    4836        0\n",
       "3    4572        0\n",
       "4     636        0\n",
       "5    2545        0\n",
       "6    1161        0\n",
       "7    2230        0\n",
       "8     148        1\n",
       "9    2530        0\n",
       "10   4070        1\n",
       "11   1261        0\n",
       "12   4682        0\n",
       "13    333        0\n",
       "14    906        0\n",
       "15   3170        0\n",
       "16    483        0\n",
       "17   2825        0\n",
       "18   1778        0\n",
       "19   2466        0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_test_pred_proba = model.predict(X_test, batch_size=8, verbose=1)\n",
    "y_test_pred = np.where(y_test_pred_proba>=0.5,1,0)\n",
    "y_test_pred = np.squeeze(y_test_pred)\n",
    "df_sub = pd.DataFrame({'index':id_test['index'], 'Outcome':y_test_pred})\n",
    "display(df_sub.head(20))\n",
    "df_sub.to_csv('submission_nn.csv', index=None, header=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0c848410-f964-4be3-bcd6-2af32407dacb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2656804 , 0.1783321 , 0.16428877, ..., 0.5772522 , 0.18998696,\n",
       "       0.4275994 ], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred_proba =np.squeeze(y_test_pred_proba)\n",
    "y_test_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f399aa0-5484-45fe-a99e-ec7f8c58ea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    " \n",
    "with open('nn_proba.pickle', mode='wb') as fo:\n",
    "    pickle.dump(y_test_pred_proba, fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b427085-30ec-4db2-9b2b-c1822213b9cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
