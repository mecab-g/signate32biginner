{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9828789-bec1-4170-8a76-262379ab71b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "import lightgbm as lgb\n",
    "\n",
    "#データ読み込み\n",
    "train = pd.read_csv(\"data_EDA/train.csv\")\n",
    "test = pd.read_csv(\"data_EDA/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "564c7484-3a88-48b9-8538-8ce80818836e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "      <th>...</th>\n",
       "      <th>Insulin_na</th>\n",
       "      <th>Pregnancies_na</th>\n",
       "      <th>Pre/age</th>\n",
       "      <th>SkinThickness_mean</th>\n",
       "      <th>BloodPressure_mean</th>\n",
       "      <th>Insulin_dpf_mean</th>\n",
       "      <th>Pregnancies_bin_0</th>\n",
       "      <th>Pregnancies_bin_-1</th>\n",
       "      <th>Pregnancies_bin_-3</th>\n",
       "      <th>Pregnancies_bin_3-</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.00000</td>\n",
       "      <td>2000.00000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2509.303000</td>\n",
       "      <td>3.584000</td>\n",
       "      <td>114.29350</td>\n",
       "      <td>68.76650</td>\n",
       "      <td>11.204000</td>\n",
       "      <td>11.859000</td>\n",
       "      <td>35.586624</td>\n",
       "      <td>0.401755</td>\n",
       "      <td>29.075500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.919000</td>\n",
       "      <td>0.036500</td>\n",
       "      <td>0.120309</td>\n",
       "      <td>26.710875</td>\n",
       "      <td>71.372937</td>\n",
       "      <td>136.508818</td>\n",
       "      <td>0.119500</td>\n",
       "      <td>0.355000</td>\n",
       "      <td>0.273000</td>\n",
       "      <td>0.252500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1438.260835</td>\n",
       "      <td>3.053786</td>\n",
       "      <td>21.98925</td>\n",
       "      <td>16.17482</td>\n",
       "      <td>14.056037</td>\n",
       "      <td>49.826253</td>\n",
       "      <td>6.936853</td>\n",
       "      <td>0.267051</td>\n",
       "      <td>8.571729</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.272903</td>\n",
       "      <td>0.187578</td>\n",
       "      <td>0.099192</td>\n",
       "      <td>5.127628</td>\n",
       "      <td>9.077127</td>\n",
       "      <td>29.914680</td>\n",
       "      <td>0.324457</td>\n",
       "      <td>0.478633</td>\n",
       "      <td>0.445612</td>\n",
       "      <td>0.434555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>57.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.285680</td>\n",
       "      <td>0.137377</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1284.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>64.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.581209</td>\n",
       "      <td>0.234628</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>26.875000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>135.636364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2549.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>111.00000</td>\n",
       "      <td>70.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.814634</td>\n",
       "      <td>0.271275</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097168</td>\n",
       "      <td>26.875000</td>\n",
       "      <td>71.409223</td>\n",
       "      <td>135.636364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3743.750000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>125.00000</td>\n",
       "      <td>78.00000</td>\n",
       "      <td>24.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.694403</td>\n",
       "      <td>0.506439</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171675</td>\n",
       "      <td>26.875000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>135.636364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4995.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>199.00000</td>\n",
       "      <td>110.00000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>744.000000</td>\n",
       "      <td>52.960258</td>\n",
       "      <td>2.175784</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>744.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             index  Pregnancies     Glucose  BloodPressure  SkinThickness  \\\n",
       "count  2000.000000  2000.000000  2000.00000     2000.00000    2000.000000   \n",
       "mean   2509.303000     3.584000   114.29350       68.76650      11.204000   \n",
       "std    1438.260835     3.053786    21.98925       16.17482      14.056037   \n",
       "min       1.000000     0.000000    57.00000        0.00000       0.000000   \n",
       "25%    1284.750000     1.000000   100.00000       64.00000       0.000000   \n",
       "50%    2549.500000     3.000000   111.00000       70.00000       0.000000   \n",
       "75%    3743.750000     6.000000   125.00000       78.00000      24.250000   \n",
       "max    4995.000000    13.000000   199.00000      110.00000      52.000000   \n",
       "\n",
       "           Insulin          BMI  DiabetesPedigreeFunction          Age  \\\n",
       "count  2000.000000  2000.000000               2000.000000  2000.000000   \n",
       "mean     11.859000    35.586624                  0.401755    29.075500   \n",
       "std      49.826253     6.936853                  0.267051     8.571729   \n",
       "min       0.000000     9.285680                  0.137377    21.000000   \n",
       "25%       0.000000    32.581209                  0.234628    22.000000   \n",
       "50%       0.000000    33.814634                  0.271275    26.000000   \n",
       "75%       0.000000    39.694403                  0.506439    33.000000   \n",
       "max     744.000000    52.960258                  2.175784    67.000000   \n",
       "\n",
       "       Outcome  ...   Insulin_na  Pregnancies_na      Pre/age  \\\n",
       "count      0.0  ...  2000.000000     2000.000000  2000.000000   \n",
       "mean       NaN  ...     0.919000        0.036500     0.120309   \n",
       "std        NaN  ...     0.272903        0.187578     0.099192   \n",
       "min        NaN  ...     0.000000        0.000000     0.000000   \n",
       "25%        NaN  ...     1.000000        0.000000     0.045455   \n",
       "50%        NaN  ...     1.000000        0.000000     0.097168   \n",
       "75%        NaN  ...     1.000000        0.000000     0.171675   \n",
       "max        NaN  ...     1.000000        1.000000     0.590909   \n",
       "\n",
       "       SkinThickness_mean  BloodPressure_mean  Insulin_dpf_mean  \\\n",
       "count         2000.000000         2000.000000       2000.000000   \n",
       "mean            26.710875           71.372937        136.508818   \n",
       "std              5.127628            9.077127         29.914680   \n",
       "min              8.000000           38.000000         15.000000   \n",
       "25%             26.875000           64.000000        135.636364   \n",
       "50%             26.875000           71.409223        135.636364   \n",
       "75%             26.875000           78.000000        135.636364   \n",
       "max             52.000000          110.000000        744.000000   \n",
       "\n",
       "       Pregnancies_bin_0  Pregnancies_bin_-1  Pregnancies_bin_-3  \\\n",
       "count        2000.000000         2000.000000         2000.000000   \n",
       "mean            0.119500            0.355000            0.273000   \n",
       "std             0.324457            0.478633            0.445612   \n",
       "min             0.000000            0.000000            0.000000   \n",
       "25%             0.000000            0.000000            0.000000   \n",
       "50%             0.000000            0.000000            0.000000   \n",
       "75%             0.000000            1.000000            1.000000   \n",
       "max             1.000000            1.000000            1.000000   \n",
       "\n",
       "       Pregnancies_bin_3-  \n",
       "count         2000.000000  \n",
       "mean             0.252500  \n",
       "std              0.434555  \n",
       "min              0.000000  \n",
       "25%              0.000000  \n",
       "50%              0.000000  \n",
       "75%              1.000000  \n",
       "max              1.000000  \n",
       "\n",
       "[8 rows x 26 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4df67cb-c4bd-4f8c-8fb7-ef2b18cc8a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "      <th>...</th>\n",
       "      <th>Insulin_na</th>\n",
       "      <th>Pregnancies_na</th>\n",
       "      <th>Pre/age</th>\n",
       "      <th>SkinThickness_mean</th>\n",
       "      <th>BloodPressure_mean</th>\n",
       "      <th>Insulin_dpf_mean</th>\n",
       "      <th>Pregnancies_bin_0</th>\n",
       "      <th>Pregnancies_bin_-1</th>\n",
       "      <th>Pregnancies_bin_-3</th>\n",
       "      <th>Pregnancies_bin_3-</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2492.964667</td>\n",
       "      <td>3.557000</td>\n",
       "      <td>113.733667</td>\n",
       "      <td>68.743667</td>\n",
       "      <td>11.164000</td>\n",
       "      <td>11.663333</td>\n",
       "      <td>35.408959</td>\n",
       "      <td>0.400476</td>\n",
       "      <td>28.932000</td>\n",
       "      <td>0.239000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.914667</td>\n",
       "      <td>0.037667</td>\n",
       "      <td>0.120089</td>\n",
       "      <td>26.984417</td>\n",
       "      <td>71.433414</td>\n",
       "      <td>135.725394</td>\n",
       "      <td>0.143333</td>\n",
       "      <td>0.330667</td>\n",
       "      <td>0.272000</td>\n",
       "      <td>0.25400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1447.218078</td>\n",
       "      <td>3.032131</td>\n",
       "      <td>20.885612</td>\n",
       "      <td>16.332755</td>\n",
       "      <td>14.351159</td>\n",
       "      <td>45.064090</td>\n",
       "      <td>6.990180</td>\n",
       "      <td>0.274666</td>\n",
       "      <td>8.469078</td>\n",
       "      <td>0.426544</td>\n",
       "      <td>...</td>\n",
       "      <td>0.279424</td>\n",
       "      <td>0.190421</td>\n",
       "      <td>0.100340</td>\n",
       "      <td>5.248441</td>\n",
       "      <td>9.040411</td>\n",
       "      <td>23.922032</td>\n",
       "      <td>0.350471</td>\n",
       "      <td>0.470532</td>\n",
       "      <td>0.445064</td>\n",
       "      <td>0.43537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.748040</td>\n",
       "      <td>0.145844</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1218.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.301920</td>\n",
       "      <td>0.230987</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>26.875000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>135.636364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2465.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>111.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.835873</td>\n",
       "      <td>0.268674</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098780</td>\n",
       "      <td>26.875000</td>\n",
       "      <td>71.409223</td>\n",
       "      <td>135.636364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3750.250000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.578256</td>\n",
       "      <td>0.506778</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>26.875000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>135.636364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4999.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>579.000000</td>\n",
       "      <td>53.400629</td>\n",
       "      <td>2.302072</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>579.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             index  Pregnancies      Glucose  BloodPressure  SkinThickness  \\\n",
       "count  3000.000000  3000.000000  3000.000000    3000.000000    3000.000000   \n",
       "mean   2492.964667     3.557000   113.733667      68.743667      11.164000   \n",
       "std    1447.218078     3.032131    20.885612      16.332755      14.351159   \n",
       "min       0.000000     0.000000    57.000000       0.000000       0.000000   \n",
       "25%    1218.750000     1.000000   100.000000      64.000000       0.000000   \n",
       "50%    2465.500000     3.000000   111.000000      70.000000       0.000000   \n",
       "75%    3750.250000     6.000000   125.000000      78.000000      24.000000   \n",
       "max    4999.000000    13.000000   196.000000     110.000000      49.000000   \n",
       "\n",
       "           Insulin          BMI  DiabetesPedigreeFunction          Age  \\\n",
       "count  3000.000000  3000.000000               3000.000000  3000.000000   \n",
       "mean     11.663333    35.408959                  0.400476    28.932000   \n",
       "std      45.064090     6.990180                  0.274666     8.469078   \n",
       "min       0.000000     7.748040                  0.145844    21.000000   \n",
       "25%       0.000000    32.301920                  0.230987    22.000000   \n",
       "50%       0.000000    33.835873                  0.268674    26.000000   \n",
       "75%       0.000000    39.578256                  0.506778    33.000000   \n",
       "max     579.000000    53.400629                  2.302072    67.000000   \n",
       "\n",
       "           Outcome  ...   Insulin_na  Pregnancies_na      Pre/age  \\\n",
       "count  3000.000000  ...  3000.000000     3000.000000  3000.000000   \n",
       "mean      0.239000  ...     0.914667        0.037667     0.120089   \n",
       "std       0.426544  ...     0.279424        0.190421     0.100340   \n",
       "min       0.000000  ...     0.000000        0.000000     0.000000   \n",
       "25%       0.000000  ...     1.000000        0.000000     0.045455   \n",
       "50%       0.000000  ...     1.000000        0.000000     0.098780   \n",
       "75%       0.000000  ...     1.000000        0.000000     0.172414   \n",
       "max       1.000000  ...     1.000000        1.000000     0.619048   \n",
       "\n",
       "       SkinThickness_mean  BloodPressure_mean  Insulin_dpf_mean  \\\n",
       "count         3000.000000         3000.000000       3000.000000   \n",
       "mean            26.984417           71.433414        135.725394   \n",
       "std              5.248441            9.040411         23.922032   \n",
       "min              7.000000           46.000000         15.000000   \n",
       "25%             26.875000           64.000000        135.636364   \n",
       "50%             26.875000           71.409223        135.636364   \n",
       "75%             26.875000           78.000000        135.636364   \n",
       "max             49.000000          110.000000        579.000000   \n",
       "\n",
       "       Pregnancies_bin_0  Pregnancies_bin_-1  Pregnancies_bin_-3  \\\n",
       "count        3000.000000         3000.000000         3000.000000   \n",
       "mean            0.143333            0.330667            0.272000   \n",
       "std             0.350471            0.470532            0.445064   \n",
       "min             0.000000            0.000000            0.000000   \n",
       "25%             0.000000            0.000000            0.000000   \n",
       "50%             0.000000            0.000000            0.000000   \n",
       "75%             0.000000            1.000000            1.000000   \n",
       "max             1.000000            1.000000            1.000000   \n",
       "\n",
       "       Pregnancies_bin_3-  \n",
       "count          3000.00000  \n",
       "mean              0.25400  \n",
       "std               0.43537  \n",
       "min               0.00000  \n",
       "25%               0.00000  \n",
       "50%               0.00000  \n",
       "75%               1.00000  \n",
       "max               1.00000  \n",
       "\n",
       "[8 rows x 26 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d779c6c7-9002-4310-9dca-a315b29a770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# まずは少ない特徴量から検討していく\n",
    "X_train = train[['DiabetesPedigreeFunction',\n",
    "                 'BMI',\n",
    "                 'Glucose',\n",
    "                 'Age',\n",
    "                 'Pregnancies',\n",
    "                 'SkinThickness',\n",
    "                 'Insulin',\n",
    "                 'BloodPressure',\n",
    "                 'Pre/age',\n",
    "                 \n",
    "                 \n",
    "                \n",
    "             ]]\n",
    "id_train = train[['index']]\n",
    "y_train = train[['Outcome']]\n",
    "\n",
    "\n",
    "X_test = test[X_train.columns]\n",
    "id_test = test[id_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9456c50-0e72-466d-85c5-662a952f5c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiabetesPedigreeFunction\n",
      "BMI\n",
      "Glucose\n",
      "Age\n",
      "Pregnancies\n",
      "SkinThickness\n",
      "Insulin\n",
      "BloodPressure\n",
      "Pre/age\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# 数値データ\n",
    "col_num = X_train.columns[X_train.dtypes!='object'].values.tolist()\n",
    "\n",
    "dict_num = {}\n",
    "for col in col_num:\n",
    "    print(col)\n",
    "    # 欠損値を0へ\n",
    "    value_fillna = 0 \n",
    "    X_train[col] = X_train[col].fillna(value_fillna)\n",
    "    # 正規化\n",
    "    value_min = X_train[col].min()\n",
    "    value_max = X_train[col].max()\n",
    "    value_mean = X_train[col].mean()\n",
    "    value_std = X_train[col].std()\n",
    "    X_train[col] = (X_train[col] - value_min) / (value_max -value_min)\n",
    "    # X_tarin[col] = (X_train[col] - value_mean) / value_std\n",
    "    \n",
    "    dict_num[col] = {}\n",
    "    dict_num[col]['fillna'] = value_fillna\n",
    "    dict_num[col]['min'] = value_min\n",
    "    dict_num[col]['max'] = value_max\n",
    "    dict_num[col]['mean'] = value_max    \n",
    "    dict_num[col]['std'] = value_max    \n",
    "    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b441ffe4-8346-4f77-ade1-571a4a1d180d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# カテゴリデータ\n",
    "# （embedding予定でラベルエンコーダー）\n",
    "col_cat = X_train.columns[X_train.dtypes=='object'].values.tolist()\n",
    "\n",
    "dict_cat = {}\n",
    "for col in col_cat:\n",
    "    print(col)\n",
    "    value_fillna = 'unknown'\n",
    "    X_train[col] = X_train[cal].fillna(value_fillna)\n",
    "    \n",
    "    X_train[caol] = X_train[col].astype(str)\n",
    "    # strに変換\n",
    "    le = LabelEncorder()\n",
    "    le.fit(X_train[col])\n",
    "    list_labelsorted(list(set(le.classes_) | set(['unknown'])))\n",
    "    map_label = {j:i for i,j in enumerate(list_label)}\n",
    "    X_train[col] = X_train[col].map(map_label)\n",
    "    \n",
    "    dict_cat[col] = {}\n",
    "    dict_cat[col]['fillna'] = value_fillna\n",
    "    dict_cat[col]['map_label'] = map_label\n",
    "    dict_cat[col]['num_label'] = len(list_label)\n",
    "\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ca4da6d-792c-41f7-a397-68d69834b5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(input_x):\n",
    "    output_x = input_x.copy()\n",
    "    \n",
    "    for col in col_num:\n",
    "        value_fillna = dict_num[col]['fillna']\n",
    "        output_x[col] = output_x[col].fillna(value_fillna)\n",
    "        \n",
    "        value_min = dict_num[col]['min']\n",
    "        value_max = dict_num[col]['max']\n",
    "        output_x[col]  = (output_x[col] - value_min ) / (value_max - value_min)\n",
    "        \n",
    "    for col in col_cat:\n",
    "        value_fillna = dict_cat[col]['fillna']\n",
    "        output_x[col] = output_x[col].fillna(value_fillna)\n",
    "        \n",
    "        output_x[col] = output_x[col].astype(str)\n",
    "        \n",
    "        map_label = dict_catt[col]['map_label']\n",
    "        output_x[col] = output_x[col].map(map_label)\n",
    "        \n",
    "        #対応するものがない場合はunkoumn\n",
    "        output_x[col] = output_x[col].fillna(map_label['unknown'])\n",
    "        \n",
    "    return output_x\n",
    "\n",
    "X_test = transform_data(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b322d24d-594b-43f5-98e9-037f78b28046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>Age</th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>Pre/age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.118091</td>\n",
       "      <td>0.605900</td>\n",
       "      <td>0.408156</td>\n",
       "      <td>0.172435</td>\n",
       "      <td>0.273615</td>\n",
       "      <td>0.227837</td>\n",
       "      <td>0.020144</td>\n",
       "      <td>0.624942</td>\n",
       "      <td>0.193989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.127383</td>\n",
       "      <td>0.153117</td>\n",
       "      <td>0.150256</td>\n",
       "      <td>0.184110</td>\n",
       "      <td>0.233241</td>\n",
       "      <td>0.292881</td>\n",
       "      <td>0.077831</td>\n",
       "      <td>0.148480</td>\n",
       "      <td>0.162087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.039487</td>\n",
       "      <td>0.537842</td>\n",
       "      <td>0.309353</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.581818</td>\n",
       "      <td>0.073427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.056965</td>\n",
       "      <td>0.571443</td>\n",
       "      <td>0.388489</td>\n",
       "      <td>0.108696</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.159568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.167392</td>\n",
       "      <td>0.697227</td>\n",
       "      <td>0.489209</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.489796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.709091</td>\n",
       "      <td>0.278515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       DiabetesPedigreeFunction          BMI      Glucose          Age  \\\n",
       "count               3000.000000  3000.000000  3000.000000  3000.000000   \n",
       "mean                   0.118091     0.605900     0.408156     0.172435   \n",
       "std                    0.127383     0.153117     0.150256     0.184110   \n",
       "min                    0.000000     0.000000     0.000000     0.000000   \n",
       "25%                    0.039487     0.537842     0.309353     0.021739   \n",
       "50%                    0.056965     0.571443     0.388489     0.108696   \n",
       "75%                    0.167392     0.697227     0.489209     0.260870   \n",
       "max                    1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "       Pregnancies  SkinThickness      Insulin  BloodPressure      Pre/age  \n",
       "count  3000.000000    3000.000000  3000.000000    3000.000000  3000.000000  \n",
       "mean      0.273615       0.227837     0.020144       0.624942     0.193989  \n",
       "std       0.233241       0.292881     0.077831       0.148480     0.162087  \n",
       "min       0.000000       0.000000     0.000000       0.000000     0.000000  \n",
       "25%       0.076923       0.000000     0.000000       0.581818     0.073427  \n",
       "50%       0.230769       0.000000     0.000000       0.636364     0.159568  \n",
       "75%       0.461538       0.489796     0.000000       0.709091     0.278515  \n",
       "max       1.000000       1.000000     1.000000       1.000000     1.000000  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1638f98-c1b6-48ee-a765-6369e686e7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>Age</th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>Pre/age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.118685</td>\n",
       "      <td>0.609792</td>\n",
       "      <td>0.412183</td>\n",
       "      <td>0.175554</td>\n",
       "      <td>0.275692</td>\n",
       "      <td>0.228653</td>\n",
       "      <td>0.020482</td>\n",
       "      <td>0.625150</td>\n",
       "      <td>0.194345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.123851</td>\n",
       "      <td>0.151949</td>\n",
       "      <td>0.158196</td>\n",
       "      <td>0.186342</td>\n",
       "      <td>0.234907</td>\n",
       "      <td>0.286858</td>\n",
       "      <td>0.086056</td>\n",
       "      <td>0.147044</td>\n",
       "      <td>0.160233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.003927</td>\n",
       "      <td>0.033681</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.041176</td>\n",
       "      <td>0.543960</td>\n",
       "      <td>0.309353</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.581818</td>\n",
       "      <td>0.073427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.058171</td>\n",
       "      <td>0.570977</td>\n",
       "      <td>0.388489</td>\n",
       "      <td>0.108696</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.156963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.167234</td>\n",
       "      <td>0.699771</td>\n",
       "      <td>0.489209</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.494898</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.709091</td>\n",
       "      <td>0.277321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.941431</td>\n",
       "      <td>0.990354</td>\n",
       "      <td>1.021583</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.061224</td>\n",
       "      <td>1.284974</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.954545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       DiabetesPedigreeFunction          BMI      Glucose          Age  \\\n",
       "count               2000.000000  2000.000000  2000.000000  2000.000000   \n",
       "mean                   0.118685     0.609792     0.412183     0.175554   \n",
       "std                    0.123851     0.151949     0.158196     0.186342   \n",
       "min                   -0.003927     0.033681     0.000000     0.000000   \n",
       "25%                    0.041176     0.543960     0.309353     0.021739   \n",
       "50%                    0.058171     0.570977     0.388489     0.108696   \n",
       "75%                    0.167234     0.699771     0.489209     0.260870   \n",
       "max                    0.941431     0.990354     1.021583     1.000000   \n",
       "\n",
       "       Pregnancies  SkinThickness      Insulin  BloodPressure      Pre/age  \n",
       "count  2000.000000    2000.000000  2000.000000    2000.000000  2000.000000  \n",
       "mean      0.275692       0.228653     0.020482       0.625150     0.194345  \n",
       "std       0.234907       0.286858     0.086056       0.147044     0.160233  \n",
       "min       0.000000       0.000000     0.000000       0.000000     0.000000  \n",
       "25%       0.076923       0.000000     0.000000       0.581818     0.073427  \n",
       "50%       0.230769       0.000000     0.000000       0.636364     0.156963  \n",
       "75%       0.461538       0.494898     0.000000       0.709091     0.277321  \n",
       "max       1.000000       1.061224     1.284974       1.000000     0.954545  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b84c0a7-0533-442f-94a4-cf5d5b9720f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 19:14:11.886831: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-15 19:14:13.234390: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-15 19:14:13.234486: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-15 19:14:13.377591: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-03-15 19:14:17.048827: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-15 19:14:17.050934: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-15 19:14:17.050969: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "random_state=123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "188c5d42-7fcd-4c2c-a2d6-e9a8896de4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    session_conf = tf.compat.v1.ConfigProto(\n",
    "        intra_op_parallelism_threads=1,\n",
    "        inter_op_parallelism_threads=1\n",
    "    )\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11843a2-84ac-4a99-8f3d-bc716ae893f7",
   "metadata": {},
   "source": [
    "## NNでの評価（CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81657fd1-039d-4c1a-a464-20b69ac9ff4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    input_num = Input(shape=(8,))\n",
    "    x_num = Dense(10, activation='relu')(input_num)\n",
    "    x_num = BatchNormalization()(x_num)\n",
    "    x_num = Dropout(0.3)(x_num)\n",
    "    x_num = Dense(10, activation='relu')(x_num)\n",
    "    x_num = BatchNormalization()(x_num)\n",
    "    x_num = Dropout(0.2)(x_num)\n",
    "    x_num = Dense(5, activation='relu')(x_num)\n",
    "    x_num = BatchNormalization()(x_num)\n",
    "    x_num = Dropout(0.1)(x_num)\n",
    "    out = Dense(1, activation='sigmoid')(x_num)\n",
    "    \n",
    "    model = Model(inputs=input_num, outputs=out,)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='Adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['binary_crossentropy'],\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44798591-6ec9-4ec6-9d9a-d8fd8c8701a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                90        \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 10)               40        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 19:14:20.740339: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-15 19:14:20.741727: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-15 19:14:20.741802: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (3ed327de65ae): /proc/driver/nvidia/version does not exist\n",
      "2023-03-15 19:14:20.744296: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 10)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 10)               40        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 55        \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 5)                20        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 5)                 0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 361\n",
      "Trainable params: 311\n",
      "Non-trainable params: 50\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# モデルの確認\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79e21f82-16fc-49ed-b576-42909918c601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cvでの評価用\n",
    "def train_nn(input_x,\n",
    "              input_y,\n",
    "              input_id,\n",
    "              list_nfold=[0,1,2,3,4],\n",
    "              n_splits=5,\n",
    "              random_state=123\n",
    "            ):\n",
    "    train_oof = np.zeros(len(input_x))\n",
    "    # foldごとの推論値\n",
    "    metrics = []\n",
    "    imp = pd.DataFrame()\n",
    "                         \n",
    "    cv = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state).\n",
    "              split(input_x, input_y))\n",
    "    \n",
    "    for nfold in list_nfold:\n",
    "        print('-'*20, nfold, '-'*20)\n",
    "        \n",
    "        idx_tr, idx_va = cv[nfold][0], cv[nfold][1]\n",
    "        x_tr, y_tr = input_x.loc[idx_tr, :], input_y.loc[idx_tr, :]\n",
    "        x_va, y_va = input_x.loc[idx_va, :], input_y.loc[idx_va, :]\n",
    "        print(x_tr.shape, y_tr.shape)\n",
    "        print(x_va.shape, y_va.shape)\n",
    "        print('y_train:{:.3f}, y_tr:{:.3f}, y_va{:.3f}'.\n",
    "              format(y_train['Outcome'].mean(), y_tr['Outcome'].mean(), y_va['Outcome'].mean(),))\n",
    "\n",
    "        model = create_model()\n",
    "        model.fit(x=x_tr,\n",
    "                  y=y_tr,\n",
    "                 validation_data=(x_va, y_va),\n",
    "                 batch_size=8,\n",
    "                 epochs=1000,\n",
    "                 callbacks=[ModelCheckpoint(filepath='model_keras.h5',\n",
    "                                            moniter='val_loss',\n",
    "                                            mode='min', \n",
    "                                            verbose=1,\n",
    "                                            save_best_only=True,\n",
    "                                            ),\n",
    "                            EarlyStopping(monitor='val_loss',\n",
    "                                          mode='min',\n",
    "                                          min_delta=0,\n",
    "                                          patience=5,\n",
    "                                          verbose=1,\n",
    "                                          restore_best_weights=True),\n",
    "                            ReduceLROnPlateau(moniter='val_loss',\n",
    "                                             mode='min',\n",
    "                                             factor=0.1,\n",
    "                                             patience=5,\n",
    "                                             verbose=1),\n",
    "                           ],\n",
    "                  verbose=1,\n",
    "                 )\n",
    "\n",
    "        # モデルの保存\n",
    "        fname_nn = 'model/nn/model_nn_fold{}.pickle'.format(nfold)\n",
    "        with open(fname_nn, 'wb')as f:\n",
    "            pickle.dump(model, f, protocol=4)\n",
    "            \n",
    "            \n",
    "        # 評価\n",
    "        y_tr_pred = model.predict(x_tr)\n",
    "        y_va_pred = model.predict(x_va)\n",
    "        metric_tr = accuracy_score(y_tr, np.where(y_tr_pred>=0.5,1,0))\n",
    "        metric_va = accuracy_score(y_va, np.where(y_va_pred>=0.5,1,0))\n",
    "        print('[accuracy] tr: {:.2f}, va: {:2f}'.\n",
    "             format(metric_tr, metric_va))\n",
    "        metrics.append([nfold, metric_tr, metric_va])\n",
    "        \n",
    "        # oof\n",
    "        train_oof[idx_va] = np.squeeze(y_va_pred)\n",
    "        \n",
    "        \n",
    "        print('-'*20, 'result', '-'*20)\n",
    "    \n",
    "    # metrix出力\n",
    "    metrics = np.array(metrics)\n",
    "    print(metrics)\n",
    "    print('[cv] tr: {:.2f}+-{:.2f}, va: {:.2f}'.format(\n",
    "        metrics[:,1].mean(), metrics[:,1].std(),\n",
    "        metrics[:,2].mean(), metrics[:,2].std()\n",
    "    ))\n",
    "    print('[oof] {:.4f}'.format(\n",
    "        accuracy_score(input_y, np.where(train_oof>=0.5,1,0))))\n",
    "    # oof出力  \n",
    "    train_oof = pd.concat([\n",
    "        input_id,\n",
    "        pd.DataFrame({'pred':train_oof})]\n",
    "        ,axis=1)\n",
    "\n",
    "    print('Done')\n",
    "    \n",
    "    return train_oof, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "36e1f37e-5c82-4d5d-9dfa-ba64d3c759a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(n_d=8, n_a=8, n_steps=3, gamma=1.3,\n",
    "                     n_independent=2, n_shared=2,\n",
    "                     seed=SEED, lambda_sparse=1e-3, \n",
    "                     optimizer_fn=torch.optim.Adam, \n",
    "                     optimizer_params=dict(lr=2e-2),\n",
    "                     mask_type=\"entmax\",\n",
    "                     scheduler_params=dict(mode=\"min\",\n",
    "                                           patience=5,\n",
    "                                           min_lr=1e-5,\n",
    "                                           factor=0.9,),\n",
    "                     scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                     verbose=10\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf1c7951-6b35-4f07-894d-814aa7eca215",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- 0 --------------------\n",
      "(2400, 8) (2400, 1)\n",
      "(600, 8) (600, 1)\n",
      "y_train:0.239, y_tr:0.239, y_va0.240\n",
      "Epoch 1/1000\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.6972 - binary_crossentropy: 0.6972\n",
      "Epoch 1: val_loss improved from inf to 0.54997, saving model to model_keras.h5\n",
      "300/300 [==============================] - 3s 7ms/step - loss: 0.6972 - binary_crossentropy: 0.6972 - val_loss: 0.5500 - val_binary_crossentropy: 0.5500 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "290/300 [============================>.] - ETA: 0s - loss: 0.5837 - binary_crossentropy: 0.5837\n",
      "Epoch 2: val_loss improved from 0.54997 to 0.51006, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5814 - binary_crossentropy: 0.5814 - val_loss: 0.5101 - val_binary_crossentropy: 0.5101 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.5496 - binary_crossentropy: 0.5496\n",
      "Epoch 3: val_loss improved from 0.51006 to 0.50196, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5496 - binary_crossentropy: 0.5496 - val_loss: 0.5020 - val_binary_crossentropy: 0.5020 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "289/300 [===========================>..] - ETA: 0s - loss: 0.5327 - binary_crossentropy: 0.5327\n",
      "Epoch 4: val_loss improved from 0.50196 to 0.49252, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5359 - binary_crossentropy: 0.5359 - val_loss: 0.4925 - val_binary_crossentropy: 0.4925 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "292/300 [============================>.] - ETA: 0s - loss: 0.5355 - binary_crossentropy: 0.5355\n",
      "Epoch 5: val_loss improved from 0.49252 to 0.48405, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.5329 - binary_crossentropy: 0.5329 - val_loss: 0.4841 - val_binary_crossentropy: 0.4841 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "291/300 [============================>.] - ETA: 0s - loss: 0.5230 - binary_crossentropy: 0.5230\n",
      "Epoch 6: val_loss improved from 0.48405 to 0.47597, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5214 - binary_crossentropy: 0.5214 - val_loss: 0.4760 - val_binary_crossentropy: 0.4760 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "292/300 [============================>.] - ETA: 0s - loss: 0.5090 - binary_crossentropy: 0.5090\n",
      "Epoch 7: val_loss improved from 0.47597 to 0.47120, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.5101 - binary_crossentropy: 0.5101 - val_loss: 0.4712 - val_binary_crossentropy: 0.4712 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "293/300 [============================>.] - ETA: 0s - loss: 0.5157 - binary_crossentropy: 0.5157\n",
      "Epoch 8: val_loss did not improve from 0.47120\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.5173 - binary_crossentropy: 0.5173 - val_loss: 0.4729 - val_binary_crossentropy: 0.4729 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "297/300 [============================>.] - ETA: 0s - loss: 0.5163 - binary_crossentropy: 0.5163\n",
      "Epoch 9: val_loss improved from 0.47120 to 0.47052, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5175 - binary_crossentropy: 0.5175 - val_loss: 0.4705 - val_binary_crossentropy: 0.4705 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "289/300 [===========================>..] - ETA: 0s - loss: 0.5176 - binary_crossentropy: 0.5176\n",
      "Epoch 10: val_loss did not improve from 0.47052\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5151 - binary_crossentropy: 0.5151 - val_loss: 0.4710 - val_binary_crossentropy: 0.4710 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "298/300 [============================>.] - ETA: 0s - loss: 0.4998 - binary_crossentropy: 0.4998\n",
      "Epoch 11: val_loss improved from 0.47052 to 0.46543, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5001 - binary_crossentropy: 0.5001 - val_loss: 0.4654 - val_binary_crossentropy: 0.4654 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "297/300 [============================>.] - ETA: 0s - loss: 0.5060 - binary_crossentropy: 0.5060\n",
      "Epoch 12: val_loss did not improve from 0.46543\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5069 - binary_crossentropy: 0.5069 - val_loss: 0.4655 - val_binary_crossentropy: 0.4655 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "297/300 [============================>.] - ETA: 0s - loss: 0.5007 - binary_crossentropy: 0.5007\n",
      "Epoch 13: val_loss improved from 0.46543 to 0.46512, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5000 - binary_crossentropy: 0.5000 - val_loss: 0.4651 - val_binary_crossentropy: 0.4651 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "293/300 [============================>.] - ETA: 0s - loss: 0.5062 - binary_crossentropy: 0.5062\n",
      "Epoch 14: val_loss improved from 0.46512 to 0.46493, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5037 - binary_crossentropy: 0.5037 - val_loss: 0.4649 - val_binary_crossentropy: 0.4649 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "297/300 [============================>.] - ETA: 0s - loss: 0.5022 - binary_crossentropy: 0.5022\n",
      "Epoch 15: val_loss improved from 0.46493 to 0.46286, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5009 - binary_crossentropy: 0.5009 - val_loss: 0.4629 - val_binary_crossentropy: 0.4629 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5012 - binary_crossentropy: 0.5012\n",
      "Epoch 16: val_loss improved from 0.46286 to 0.46059, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5022 - binary_crossentropy: 0.5022 - val_loss: 0.4606 - val_binary_crossentropy: 0.4606 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "295/300 [============================>.] - ETA: 0s - loss: 0.5026 - binary_crossentropy: 0.5026\n",
      "Epoch 17: val_loss did not improve from 0.46059\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5022 - binary_crossentropy: 0.5022 - val_loss: 0.4617 - val_binary_crossentropy: 0.4617 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.5028 - binary_crossentropy: 0.5028\n",
      "Epoch 18: val_loss did not improve from 0.46059\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5028 - binary_crossentropy: 0.5028 - val_loss: 0.4636 - val_binary_crossentropy: 0.4636 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "292/300 [============================>.] - ETA: 0s - loss: 0.5075 - binary_crossentropy: 0.5075\n",
      "Epoch 19: val_loss did not improve from 0.46059\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5073 - binary_crossentropy: 0.5073 - val_loss: 0.4652 - val_binary_crossentropy: 0.4652 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "294/300 [============================>.] - ETA: 0s - loss: 0.4912 - binary_crossentropy: 0.4912\n",
      "Epoch 20: val_loss improved from 0.46059 to 0.45919, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4949 - binary_crossentropy: 0.4949 - val_loss: 0.4592 - val_binary_crossentropy: 0.4592 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "293/300 [============================>.] - ETA: 0s - loss: 0.5018 - binary_crossentropy: 0.5018\n",
      "Epoch 21: val_loss did not improve from 0.45919\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5013 - binary_crossentropy: 0.5013 - val_loss: 0.4610 - val_binary_crossentropy: 0.4610 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "295/300 [============================>.] - ETA: 0s - loss: 0.4985 - binary_crossentropy: 0.4985\n",
      "Epoch 22: val_loss did not improve from 0.45919\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4977 - binary_crossentropy: 0.4977 - val_loss: 0.4616 - val_binary_crossentropy: 0.4616 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.4977 - binary_crossentropy: 0.4977\n",
      "Epoch 23: val_loss did not improve from 0.45919\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5002 - binary_crossentropy: 0.5002 - val_loss: 0.4616 - val_binary_crossentropy: 0.4616 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.5004 - binary_crossentropy: 0.5004\n",
      "Epoch 24: val_loss did not improve from 0.45919\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4997 - binary_crossentropy: 0.4997 - val_loss: 0.4629 - val_binary_crossentropy: 0.4629 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "296/300 [============================>.] - ETA: 0s - loss: 0.5001 - binary_crossentropy: 0.5001\n",
      "Epoch 25: val_loss did not improve from 0.45919\n",
      "Restoring model weights from the end of the best epoch: 20.\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4992 - binary_crossentropy: 0.4992 - val_loss: 0.4610 - val_binary_crossentropy: 0.4610 - lr: 0.0010\n",
      "Epoch 25: early stopping\n",
      "INFO:tensorflow:Assets written to: ram://2f963b42-d7de-4748-ab4c-c337c6e64e16/assets\n",
      "75/75 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "[accuracy] tr: 0.77, va: 0.756667\n",
      "-------------------- result --------------------\n",
      "-------------------- 1 --------------------\n",
      "(2400, 8) (2400, 1)\n",
      "(600, 8) (600, 1)\n",
      "y_train:0.239, y_tr:0.239, y_va0.240\n",
      "Epoch 1/1000\n",
      "293/300 [============================>.] - ETA: 0s - loss: 0.7767 - binary_crossentropy: 0.7767\n",
      "Epoch 1: val_loss improved from inf to 0.60512, saving model to model_keras.h5\n",
      "300/300 [==============================] - 3s 6ms/step - loss: 0.7754 - binary_crossentropy: 0.7754 - val_loss: 0.6051 - val_binary_crossentropy: 0.6051 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "298/300 [============================>.] - ETA: 0s - loss: 0.6091 - binary_crossentropy: 0.6091\n",
      "Epoch 2: val_loss improved from 0.60512 to 0.54427, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.6095 - binary_crossentropy: 0.6095 - val_loss: 0.5443 - val_binary_crossentropy: 0.5443 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "294/300 [============================>.] - ETA: 0s - loss: 0.5779 - binary_crossentropy: 0.5779\n",
      "Epoch 3: val_loss improved from 0.54427 to 0.52785, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5756 - binary_crossentropy: 0.5756 - val_loss: 0.5278 - val_binary_crossentropy: 0.5278 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.5541 - binary_crossentropy: 0.5541\n",
      "Epoch 4: val_loss improved from 0.52785 to 0.51672, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5541 - binary_crossentropy: 0.5541 - val_loss: 0.5167 - val_binary_crossentropy: 0.5167 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "289/300 [===========================>..] - ETA: 0s - loss: 0.5469 - binary_crossentropy: 0.5469\n",
      "Epoch 5: val_loss improved from 0.51672 to 0.50810, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.5507 - binary_crossentropy: 0.5507 - val_loss: 0.5081 - val_binary_crossentropy: 0.5081 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "294/300 [============================>.] - ETA: 0s - loss: 0.5376 - binary_crossentropy: 0.5376\n",
      "Epoch 6: val_loss improved from 0.50810 to 0.50230, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 0.5368 - binary_crossentropy: 0.5368 - val_loss: 0.5023 - val_binary_crossentropy: 0.5023 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "297/300 [============================>.] - ETA: 0s - loss: 0.5368 - binary_crossentropy: 0.5368\n",
      "Epoch 7: val_loss improved from 0.50230 to 0.49289, saving model to model_keras.h5\n",
      "300/300 [==============================] - 3s 10ms/step - loss: 0.5364 - binary_crossentropy: 0.5364 - val_loss: 0.4929 - val_binary_crossentropy: 0.4929 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.5282 - binary_crossentropy: 0.5282\n",
      "Epoch 8: val_loss improved from 0.49289 to 0.48596, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 8ms/step - loss: 0.5282 - binary_crossentropy: 0.5282 - val_loss: 0.4860 - val_binary_crossentropy: 0.4860 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "294/300 [============================>.] - ETA: 0s - loss: 0.5280 - binary_crossentropy: 0.5280\n",
      "Epoch 9: val_loss improved from 0.48596 to 0.47920, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 0.5291 - binary_crossentropy: 0.5291 - val_loss: 0.4792 - val_binary_crossentropy: 0.4792 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "292/300 [============================>.] - ETA: 0s - loss: 0.5173 - binary_crossentropy: 0.5173\n",
      "Epoch 10: val_loss improved from 0.47920 to 0.47535, saving model to model_keras.h5\n",
      "300/300 [==============================] - 3s 8ms/step - loss: 0.5184 - binary_crossentropy: 0.5184 - val_loss: 0.4754 - val_binary_crossentropy: 0.4754 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "295/300 [============================>.] - ETA: 0s - loss: 0.5160 - binary_crossentropy: 0.5160\n",
      "Epoch 11: val_loss improved from 0.47535 to 0.47174, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 8ms/step - loss: 0.5147 - binary_crossentropy: 0.5147 - val_loss: 0.4717 - val_binary_crossentropy: 0.4717 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "293/300 [============================>.] - ETA: 0s - loss: 0.5116 - binary_crossentropy: 0.5116\n",
      "Epoch 12: val_loss improved from 0.47174 to 0.46796, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 8ms/step - loss: 0.5090 - binary_crossentropy: 0.5090 - val_loss: 0.4680 - val_binary_crossentropy: 0.4680 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.5189 - binary_crossentropy: 0.5189\n",
      "Epoch 13: val_loss did not improve from 0.46796\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 0.5186 - binary_crossentropy: 0.5186 - val_loss: 0.4701 - val_binary_crossentropy: 0.4701 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "298/300 [============================>.] - ETA: 0s - loss: 0.5037 - binary_crossentropy: 0.5037\n",
      "Epoch 14: val_loss improved from 0.46796 to 0.46475, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.5034 - binary_crossentropy: 0.5034 - val_loss: 0.4648 - val_binary_crossentropy: 0.4648 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "295/300 [============================>.] - ETA: 0s - loss: 0.5065 - binary_crossentropy: 0.5065\n",
      "Epoch 15: val_loss did not improve from 0.46475\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5050 - binary_crossentropy: 0.5050 - val_loss: 0.4648 - val_binary_crossentropy: 0.4648 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5131 - binary_crossentropy: 0.5131\n",
      "Epoch 16: val_loss did not improve from 0.46475\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.5107 - binary_crossentropy: 0.5107 - val_loss: 0.4679 - val_binary_crossentropy: 0.4679 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "294/300 [============================>.] - ETA: 0s - loss: 0.5068 - binary_crossentropy: 0.5068\n",
      "Epoch 17: val_loss did not improve from 0.46475\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5083 - binary_crossentropy: 0.5083 - val_loss: 0.4665 - val_binary_crossentropy: 0.4665 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.5054 - binary_crossentropy: 0.5054\n",
      "Epoch 18: val_loss did not improve from 0.46475\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5054 - binary_crossentropy: 0.5054 - val_loss: 0.4678 - val_binary_crossentropy: 0.4678 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "293/300 [============================>.] - ETA: 0s - loss: 0.5078 - binary_crossentropy: 0.5078\n",
      "Epoch 19: val_loss did not improve from 0.46475\n",
      "Restoring model weights from the end of the best epoch: 14.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5078 - binary_crossentropy: 0.5078 - val_loss: 0.4673 - val_binary_crossentropy: 0.4673 - lr: 0.0010\n",
      "Epoch 19: early stopping\n",
      "INFO:tensorflow:Assets written to: ram://e9f6ce56-903f-4c2f-ae03-89b1bbc96bd5/assets\n",
      "75/75 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "[accuracy] tr: 0.76, va: 0.761667\n",
      "-------------------- result --------------------\n",
      "-------------------- 2 --------------------\n",
      "(2400, 8) (2400, 1)\n",
      "(600, 8) (600, 1)\n",
      "y_train:0.239, y_tr:0.239, y_va0.238\n",
      "Epoch 1/1000\n",
      "290/300 [============================>.] - ETA: 0s - loss: 0.7277 - binary_crossentropy: 0.7277\n",
      "Epoch 1: val_loss improved from inf to 0.54989, saving model to model_keras.h5\n",
      "300/300 [==============================] - 3s 7ms/step - loss: 0.7251 - binary_crossentropy: 0.7251 - val_loss: 0.5499 - val_binary_crossentropy: 0.5499 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.5939 - binary_crossentropy: 0.5939\n",
      "Epoch 2: val_loss improved from 0.54989 to 0.53098, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5939 - binary_crossentropy: 0.5939 - val_loss: 0.5310 - val_binary_crossentropy: 0.5310 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.5596 - binary_crossentropy: 0.5596\n",
      "Epoch 3: val_loss improved from 0.53098 to 0.52312, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5590 - binary_crossentropy: 0.5590 - val_loss: 0.5231 - val_binary_crossentropy: 0.5231 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "296/300 [============================>.] - ETA: 0s - loss: 0.5318 - binary_crossentropy: 0.5318\n",
      "Epoch 4: val_loss improved from 0.52312 to 0.51778, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.5312 - binary_crossentropy: 0.5312 - val_loss: 0.5178 - val_binary_crossentropy: 0.5178 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5333 - binary_crossentropy: 0.5333\n",
      "Epoch 5: val_loss improved from 0.51778 to 0.51352, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.5284 - binary_crossentropy: 0.5284 - val_loss: 0.5135 - val_binary_crossentropy: 0.5135 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.5339 - binary_crossentropy: 0.5339\n",
      "Epoch 6: val_loss improved from 0.51352 to 0.50936, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 0.5339 - binary_crossentropy: 0.5339 - val_loss: 0.5094 - val_binary_crossentropy: 0.5094 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "293/300 [============================>.] - ETA: 0s - loss: 0.5290 - binary_crossentropy: 0.5290\n",
      "Epoch 7: val_loss improved from 0.50936 to 0.50855, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.5289 - binary_crossentropy: 0.5289 - val_loss: 0.5085 - val_binary_crossentropy: 0.5085 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "296/300 [============================>.] - ETA: 0s - loss: 0.5221 - binary_crossentropy: 0.5221\n",
      "Epoch 8: val_loss improved from 0.50855 to 0.50366, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5202 - binary_crossentropy: 0.5202 - val_loss: 0.5037 - val_binary_crossentropy: 0.5037 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "295/300 [============================>.] - ETA: 0s - loss: 0.5215 - binary_crossentropy: 0.5215\n",
      "Epoch 9: val_loss improved from 0.50366 to 0.50274, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5211 - binary_crossentropy: 0.5211 - val_loss: 0.5027 - val_binary_crossentropy: 0.5027 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.5087 - binary_crossentropy: 0.5087\n",
      "Epoch 10: val_loss improved from 0.50274 to 0.50095, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5088 - binary_crossentropy: 0.5088 - val_loss: 0.5009 - val_binary_crossentropy: 0.5009 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "293/300 [============================>.] - ETA: 0s - loss: 0.5130 - binary_crossentropy: 0.5130\n",
      "Epoch 11: val_loss improved from 0.50095 to 0.49977, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5135 - binary_crossentropy: 0.5135 - val_loss: 0.4998 - val_binary_crossentropy: 0.4998 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "289/300 [===========================>..] - ETA: 0s - loss: 0.5087 - binary_crossentropy: 0.5087\n",
      "Epoch 12: val_loss improved from 0.49977 to 0.49905, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5065 - binary_crossentropy: 0.5065 - val_loss: 0.4990 - val_binary_crossentropy: 0.4990 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "289/300 [===========================>..] - ETA: 0s - loss: 0.5174 - binary_crossentropy: 0.5174\n",
      "Epoch 13: val_loss improved from 0.49905 to 0.49822, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5149 - binary_crossentropy: 0.5149 - val_loss: 0.4982 - val_binary_crossentropy: 0.4982 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.5111 - binary_crossentropy: 0.5111\n",
      "Epoch 14: val_loss improved from 0.49822 to 0.49749, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5111 - binary_crossentropy: 0.5111 - val_loss: 0.4975 - val_binary_crossentropy: 0.4975 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "289/300 [===========================>..] - ETA: 0s - loss: 0.4987 - binary_crossentropy: 0.4987\n",
      "Epoch 15: val_loss improved from 0.49749 to 0.49561, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5024 - binary_crossentropy: 0.5024 - val_loss: 0.4956 - val_binary_crossentropy: 0.4956 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "296/300 [============================>.] - ETA: 0s - loss: 0.5042 - binary_crossentropy: 0.5042\n",
      "Epoch 16: val_loss did not improve from 0.49561\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 0.5041 - binary_crossentropy: 0.5041 - val_loss: 0.4979 - val_binary_crossentropy: 0.4979 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "298/300 [============================>.] - ETA: 0s - loss: 0.4948 - binary_crossentropy: 0.4948\n",
      "Epoch 17: val_loss did not improve from 0.49561\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.4944 - binary_crossentropy: 0.4944 - val_loss: 0.4958 - val_binary_crossentropy: 0.4958 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "297/300 [============================>.] - ETA: 0s - loss: 0.4990 - binary_crossentropy: 0.4990\n",
      "Epoch 18: val_loss improved from 0.49561 to 0.49482, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.4991 - binary_crossentropy: 0.4991 - val_loss: 0.4948 - val_binary_crossentropy: 0.4948 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "292/300 [============================>.] - ETA: 0s - loss: 0.4996 - binary_crossentropy: 0.4996\n",
      "Epoch 19: val_loss did not improve from 0.49482\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.4999 - binary_crossentropy: 0.4999 - val_loss: 0.4958 - val_binary_crossentropy: 0.4958 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "296/300 [============================>.] - ETA: 0s - loss: 0.4950 - binary_crossentropy: 0.4950\n",
      "Epoch 20: val_loss improved from 0.49482 to 0.49428, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4945 - binary_crossentropy: 0.4945 - val_loss: 0.4943 - val_binary_crossentropy: 0.4943 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "291/300 [============================>.] - ETA: 0s - loss: 0.4933 - binary_crossentropy: 0.4933\n",
      "Epoch 21: val_loss improved from 0.49428 to 0.49256, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4925 - binary_crossentropy: 0.4925 - val_loss: 0.4926 - val_binary_crossentropy: 0.4926 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.4987 - binary_crossentropy: 0.4987\n",
      "Epoch 22: val_loss did not improve from 0.49256\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4987 - binary_crossentropy: 0.4987 - val_loss: 0.4937 - val_binary_crossentropy: 0.4937 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "292/300 [============================>.] - ETA: 0s - loss: 0.4906 - binary_crossentropy: 0.4906\n",
      "Epoch 23: val_loss improved from 0.49256 to 0.49098, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4897 - binary_crossentropy: 0.4897 - val_loss: 0.4910 - val_binary_crossentropy: 0.4910 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "295/300 [============================>.] - ETA: 0s - loss: 0.5018 - binary_crossentropy: 0.5018\n",
      "Epoch 24: val_loss did not improve from 0.49098\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5009 - binary_crossentropy: 0.5009 - val_loss: 0.4911 - val_binary_crossentropy: 0.4911 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "291/300 [============================>.] - ETA: 0s - loss: 0.4976 - binary_crossentropy: 0.4976\n",
      "Epoch 25: val_loss improved from 0.49098 to 0.48877, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4964 - binary_crossentropy: 0.4964 - val_loss: 0.4888 - val_binary_crossentropy: 0.4888 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "298/300 [============================>.] - ETA: 0s - loss: 0.4910 - binary_crossentropy: 0.4910\n",
      "Epoch 26: val_loss improved from 0.48877 to 0.48735, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.4901 - binary_crossentropy: 0.4901 - val_loss: 0.4873 - val_binary_crossentropy: 0.4873 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "292/300 [============================>.] - ETA: 0s - loss: 0.4890 - binary_crossentropy: 0.4890\n",
      "Epoch 27: val_loss did not improve from 0.48735\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4910 - binary_crossentropy: 0.4910 - val_loss: 0.4883 - val_binary_crossentropy: 0.4883 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "295/300 [============================>.] - ETA: 0s - loss: 0.4950 - binary_crossentropy: 0.4950\n",
      "Epoch 28: val_loss did not improve from 0.48735\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4959 - binary_crossentropy: 0.4959 - val_loss: 0.4883 - val_binary_crossentropy: 0.4883 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.4909 - binary_crossentropy: 0.4909\n",
      "Epoch 29: val_loss did not improve from 0.48735\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4914 - binary_crossentropy: 0.4914 - val_loss: 0.4880 - val_binary_crossentropy: 0.4880 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "294/300 [============================>.] - ETA: 0s - loss: 0.4899 - binary_crossentropy: 0.4899\n",
      "Epoch 30: val_loss improved from 0.48735 to 0.48694, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4884 - binary_crossentropy: 0.4884 - val_loss: 0.4869 - val_binary_crossentropy: 0.4869 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "291/300 [============================>.] - ETA: 0s - loss: 0.4951 - binary_crossentropy: 0.4951\n",
      "Epoch 31: val_loss did not improve from 0.48694\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4941 - binary_crossentropy: 0.4941 - val_loss: 0.4871 - val_binary_crossentropy: 0.4871 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "295/300 [============================>.] - ETA: 0s - loss: 0.4852 - binary_crossentropy: 0.4852\n",
      "Epoch 32: val_loss did not improve from 0.48694\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4850 - binary_crossentropy: 0.4850 - val_loss: 0.4872 - val_binary_crossentropy: 0.4872 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "293/300 [============================>.] - ETA: 0s - loss: 0.4826 - binary_crossentropy: 0.4826\n",
      "Epoch 33: val_loss did not improve from 0.48694\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4864 - binary_crossentropy: 0.4864 - val_loss: 0.4872 - val_binary_crossentropy: 0.4872 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "298/300 [============================>.] - ETA: 0s - loss: 0.4949 - binary_crossentropy: 0.4949\n",
      "Epoch 34: val_loss improved from 0.48694 to 0.48577, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.4942 - binary_crossentropy: 0.4942 - val_loss: 0.4858 - val_binary_crossentropy: 0.4858 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "292/300 [============================>.] - ETA: 0s - loss: 0.4846 - binary_crossentropy: 0.4846\n",
      "Epoch 35: val_loss did not improve from 0.48577\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4875 - binary_crossentropy: 0.4875 - val_loss: 0.4870 - val_binary_crossentropy: 0.4870 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "289/300 [===========================>..] - ETA: 0s - loss: 0.4886 - binary_crossentropy: 0.4886\n",
      "Epoch 36: val_loss improved from 0.48577 to 0.48557, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4860 - binary_crossentropy: 0.4860 - val_loss: 0.4856 - val_binary_crossentropy: 0.4856 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "295/300 [============================>.] - ETA: 0s - loss: 0.4953 - binary_crossentropy: 0.4953\n",
      "Epoch 37: val_loss did not improve from 0.48557\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4934 - binary_crossentropy: 0.4934 - val_loss: 0.4861 - val_binary_crossentropy: 0.4861 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "294/300 [============================>.] - ETA: 0s - loss: 0.4844 - binary_crossentropy: 0.4844\n",
      "Epoch 38: val_loss did not improve from 0.48557\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.4860 - binary_crossentropy: 0.4860 - val_loss: 0.4863 - val_binary_crossentropy: 0.4863 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "292/300 [============================>.] - ETA: 0s - loss: 0.4892 - binary_crossentropy: 0.4892\n",
      "Epoch 39: val_loss did not improve from 0.48557\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4881 - binary_crossentropy: 0.4881 - val_loss: 0.4860 - val_binary_crossentropy: 0.4860 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "296/300 [============================>.] - ETA: 0s - loss: 0.4837 - binary_crossentropy: 0.4837\n",
      "Epoch 40: val_loss did not improve from 0.48557\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4824 - binary_crossentropy: 0.4824 - val_loss: 0.4861 - val_binary_crossentropy: 0.4861 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "292/300 [============================>.] - ETA: 0s - loss: 0.4819 - binary_crossentropy: 0.4819\n",
      "Epoch 41: val_loss did not improve from 0.48557\n",
      "Restoring model weights from the end of the best epoch: 36.\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4839 - binary_crossentropy: 0.4839 - val_loss: 0.4861 - val_binary_crossentropy: 0.4861 - lr: 0.0010\n",
      "Epoch 41: early stopping\n",
      "INFO:tensorflow:Assets written to: ram://446838a3-0338-43c4-897b-bee88f87d696/assets\n",
      "75/75 [==============================] - 0s 2ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "[accuracy] tr: 0.78, va: 0.770000\n",
      "-------------------- result --------------------\n",
      "-------------------- 3 --------------------\n",
      "(2400, 8) (2400, 1)\n",
      "(600, 8) (600, 1)\n",
      "y_train:0.239, y_tr:0.239, y_va0.238\n",
      "Epoch 1/1000\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.6826 - binary_crossentropy: 0.6826\n",
      "Epoch 1: val_loss improved from inf to 0.58142, saving model to model_keras.h5\n",
      "300/300 [==============================] - 4s 7ms/step - loss: 0.6831 - binary_crossentropy: 0.6831 - val_loss: 0.5814 - val_binary_crossentropy: 0.5814 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "290/300 [============================>.] - ETA: 0s - loss: 0.5742 - binary_crossentropy: 0.5742\n",
      "Epoch 2: val_loss improved from 0.58142 to 0.53593, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5737 - binary_crossentropy: 0.5737 - val_loss: 0.5359 - val_binary_crossentropy: 0.5359 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.5517 - binary_crossentropy: 0.5517\n",
      "Epoch 3: val_loss improved from 0.53593 to 0.51821, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.5510 - binary_crossentropy: 0.5510 - val_loss: 0.5182 - val_binary_crossentropy: 0.5182 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "287/300 [===========================>..] - ETA: 0s - loss: 0.5323 - binary_crossentropy: 0.5323\n",
      "Epoch 4: val_loss improved from 0.51821 to 0.51080, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.5305 - binary_crossentropy: 0.5305 - val_loss: 0.5108 - val_binary_crossentropy: 0.5108 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "292/300 [============================>.] - ETA: 0s - loss: 0.5310 - binary_crossentropy: 0.5310\n",
      "Epoch 5: val_loss improved from 0.51080 to 0.50059, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5283 - binary_crossentropy: 0.5283 - val_loss: 0.5006 - val_binary_crossentropy: 0.5006 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "293/300 [============================>.] - ETA: 0s - loss: 0.5265 - binary_crossentropy: 0.5265\n",
      "Epoch 6: val_loss improved from 0.50059 to 0.49321, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5241 - binary_crossentropy: 0.5241 - val_loss: 0.4932 - val_binary_crossentropy: 0.4932 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "293/300 [============================>.] - ETA: 0s - loss: 0.5102 - binary_crossentropy: 0.5102\n",
      "Epoch 7: val_loss improved from 0.49321 to 0.48484, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5131 - binary_crossentropy: 0.5131 - val_loss: 0.4848 - val_binary_crossentropy: 0.4848 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "293/300 [============================>.] - ETA: 0s - loss: 0.5157 - binary_crossentropy: 0.5157\n",
      "Epoch 8: val_loss improved from 0.48484 to 0.48435, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5166 - binary_crossentropy: 0.5166 - val_loss: 0.4843 - val_binary_crossentropy: 0.4843 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "293/300 [============================>.] - ETA: 0s - loss: 0.5045 - binary_crossentropy: 0.5045\n",
      "Epoch 9: val_loss improved from 0.48435 to 0.48040, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5054 - binary_crossentropy: 0.5054 - val_loss: 0.4804 - val_binary_crossentropy: 0.4804 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "296/300 [============================>.] - ETA: 0s - loss: 0.5029 - binary_crossentropy: 0.5029\n",
      "Epoch 10: val_loss improved from 0.48040 to 0.47932, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5053 - binary_crossentropy: 0.5053 - val_loss: 0.4793 - val_binary_crossentropy: 0.4793 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "294/300 [============================>.] - ETA: 0s - loss: 0.5021 - binary_crossentropy: 0.5021\n",
      "Epoch 11: val_loss improved from 0.47932 to 0.47655, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.5021 - binary_crossentropy: 0.5021 - val_loss: 0.4766 - val_binary_crossentropy: 0.4766 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "287/300 [===========================>..] - ETA: 0s - loss: 0.5061 - binary_crossentropy: 0.5061\n",
      "Epoch 12: val_loss did not improve from 0.47655\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5083 - binary_crossentropy: 0.5083 - val_loss: 0.4766 - val_binary_crossentropy: 0.4766 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "297/300 [============================>.] - ETA: 0s - loss: 0.4990 - binary_crossentropy: 0.4990\n",
      "Epoch 13: val_loss improved from 0.47655 to 0.47496, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.5000 - binary_crossentropy: 0.5000 - val_loss: 0.4750 - val_binary_crossentropy: 0.4750 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "290/300 [============================>.] - ETA: 0s - loss: 0.4990 - binary_crossentropy: 0.4990\n",
      "Epoch 14: val_loss improved from 0.47496 to 0.47451, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4982 - binary_crossentropy: 0.4982 - val_loss: 0.4745 - val_binary_crossentropy: 0.4745 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.5082 - binary_crossentropy: 0.5082\n",
      "Epoch 15: val_loss improved from 0.47451 to 0.47436, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 0.5082 - binary_crossentropy: 0.5082 - val_loss: 0.4744 - val_binary_crossentropy: 0.4744 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "295/300 [============================>.] - ETA: 0s - loss: 0.4967 - binary_crossentropy: 0.4967\n",
      "Epoch 16: val_loss improved from 0.47436 to 0.47270, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4984 - binary_crossentropy: 0.4984 - val_loss: 0.4727 - val_binary_crossentropy: 0.4727 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.4972 - binary_crossentropy: 0.4972\n",
      "Epoch 17: val_loss improved from 0.47270 to 0.47144, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4972 - binary_crossentropy: 0.4972 - val_loss: 0.4714 - val_binary_crossentropy: 0.4714 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.5030 - binary_crossentropy: 0.5030\n",
      "Epoch 18: val_loss did not improve from 0.47144\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5030 - binary_crossentropy: 0.5030 - val_loss: 0.4733 - val_binary_crossentropy: 0.4733 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "298/300 [============================>.] - ETA: 0s - loss: 0.4992 - binary_crossentropy: 0.4992\n",
      "Epoch 19: val_loss did not improve from 0.47144\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4981 - binary_crossentropy: 0.4981 - val_loss: 0.4726 - val_binary_crossentropy: 0.4726 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.4934 - binary_crossentropy: 0.4934\n",
      "Epoch 20: val_loss did not improve from 0.47144\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.4939 - binary_crossentropy: 0.4939 - val_loss: 0.4717 - val_binary_crossentropy: 0.4717 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "295/300 [============================>.] - ETA: 0s - loss: 0.4877 - binary_crossentropy: 0.4877\n",
      "Epoch 21: val_loss improved from 0.47144 to 0.46984, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4895 - binary_crossentropy: 0.4895 - val_loss: 0.4698 - val_binary_crossentropy: 0.4698 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "290/300 [============================>.] - ETA: 0s - loss: 0.4897 - binary_crossentropy: 0.4897\n",
      "Epoch 22: val_loss did not improve from 0.46984\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4924 - binary_crossentropy: 0.4924 - val_loss: 0.4705 - val_binary_crossentropy: 0.4705 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.4894 - binary_crossentropy: 0.4894\n",
      "Epoch 23: val_loss improved from 0.46984 to 0.46966, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4910 - binary_crossentropy: 0.4910 - val_loss: 0.4697 - val_binary_crossentropy: 0.4697 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.4979 - binary_crossentropy: 0.4979\n",
      "Epoch 24: val_loss did not improve from 0.46966\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4979 - binary_crossentropy: 0.4979 - val_loss: 0.4710 - val_binary_crossentropy: 0.4710 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.4971 - binary_crossentropy: 0.4971\n",
      "Epoch 25: val_loss did not improve from 0.46966\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4971 - binary_crossentropy: 0.4971 - val_loss: 0.4727 - val_binary_crossentropy: 0.4727 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "295/300 [============================>.] - ETA: 0s - loss: 0.4943 - binary_crossentropy: 0.4943\n",
      "Epoch 26: val_loss did not improve from 0.46966\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4960 - binary_crossentropy: 0.4960 - val_loss: 0.4718 - val_binary_crossentropy: 0.4718 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "289/300 [===========================>..] - ETA: 0s - loss: 0.4909 - binary_crossentropy: 0.4909\n",
      "Epoch 27: val_loss did not improve from 0.46966\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4912 - binary_crossentropy: 0.4912 - val_loss: 0.4701 - val_binary_crossentropy: 0.4701 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "295/300 [============================>.] - ETA: 0s - loss: 0.4966 - binary_crossentropy: 0.4966\n",
      "Epoch 28: val_loss did not improve from 0.46966\n",
      "Restoring model weights from the end of the best epoch: 23.\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4957 - binary_crossentropy: 0.4957 - val_loss: 0.4713 - val_binary_crossentropy: 0.4713 - lr: 0.0010\n",
      "Epoch 28: early stopping\n",
      "INFO:tensorflow:Assets written to: ram://857f3b83-d0e4-4060-83c8-3edd88a91583/assets\n",
      "75/75 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n",
      "[accuracy] tr: 0.78, va: 0.760000\n",
      "-------------------- result --------------------\n",
      "-------------------- 4 --------------------\n",
      "(2400, 8) (2400, 1)\n",
      "(600, 8) (600, 1)\n",
      "y_train:0.239, y_tr:0.239, y_va0.238\n",
      "Epoch 1/1000\n",
      "297/300 [============================>.] - ETA: 0s - loss: 0.8266 - binary_crossentropy: 0.8266\n",
      "Epoch 1: val_loss improved from inf to 0.68487, saving model to model_keras.h5\n",
      "300/300 [==============================] - 3s 6ms/step - loss: 0.8273 - binary_crossentropy: 0.8273 - val_loss: 0.6849 - val_binary_crossentropy: 0.6849 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5982 - binary_crossentropy: 0.5982\n",
      "Epoch 2: val_loss improved from 0.68487 to 0.59366, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5938 - binary_crossentropy: 0.5938 - val_loss: 0.5937 - val_binary_crossentropy: 0.5937 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "292/300 [============================>.] - ETA: 0s - loss: 0.5510 - binary_crossentropy: 0.5510\n",
      "Epoch 3: val_loss improved from 0.59366 to 0.56268, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5478 - binary_crossentropy: 0.5478 - val_loss: 0.5627 - val_binary_crossentropy: 0.5627 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "298/300 [============================>.] - ETA: 0s - loss: 0.5386 - binary_crossentropy: 0.5386\n",
      "Epoch 4: val_loss improved from 0.56268 to 0.54111, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5377 - binary_crossentropy: 0.5377 - val_loss: 0.5411 - val_binary_crossentropy: 0.5411 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "293/300 [============================>.] - ETA: 0s - loss: 0.5372 - binary_crossentropy: 0.5372\n",
      "Epoch 5: val_loss improved from 0.54111 to 0.52609, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5351 - binary_crossentropy: 0.5351 - val_loss: 0.5261 - val_binary_crossentropy: 0.5261 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "289/300 [===========================>..] - ETA: 0s - loss: 0.5270 - binary_crossentropy: 0.5270\n",
      "Epoch 6: val_loss improved from 0.52609 to 0.52165, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5284 - binary_crossentropy: 0.5284 - val_loss: 0.5216 - val_binary_crossentropy: 0.5216 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "296/300 [============================>.] - ETA: 0s - loss: 0.5188 - binary_crossentropy: 0.5188\n",
      "Epoch 7: val_loss improved from 0.52165 to 0.51838, saving model to model_keras.h5\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5194 - binary_crossentropy: 0.5194 - val_loss: 0.5184 - val_binary_crossentropy: 0.5184 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.5058 - binary_crossentropy: 0.5058\n",
      "Epoch 8: val_loss improved from 0.51838 to 0.51405, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5053 - binary_crossentropy: 0.5053 - val_loss: 0.5140 - val_binary_crossentropy: 0.5140 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.5135 - binary_crossentropy: 0.5135\n",
      "Epoch 9: val_loss improved from 0.51405 to 0.50975, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.5136 - binary_crossentropy: 0.5136 - val_loss: 0.5098 - val_binary_crossentropy: 0.5098 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "295/300 [============================>.] - ETA: 0s - loss: 0.5134 - binary_crossentropy: 0.5134\n",
      "Epoch 10: val_loss improved from 0.50975 to 0.50740, saving model to model_keras.h5\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5138 - binary_crossentropy: 0.5138 - val_loss: 0.5074 - val_binary_crossentropy: 0.5074 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "298/300 [============================>.] - ETA: 0s - loss: 0.5019 - binary_crossentropy: 0.5019\n",
      "Epoch 11: val_loss improved from 0.50740 to 0.50542, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5022 - binary_crossentropy: 0.5022 - val_loss: 0.5054 - val_binary_crossentropy: 0.5054 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.5148 - binary_crossentropy: 0.5148\n",
      "Epoch 12: val_loss did not improve from 0.50542\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5150 - binary_crossentropy: 0.5150 - val_loss: 0.5056 - val_binary_crossentropy: 0.5056 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "293/300 [============================>.] - ETA: 0s - loss: 0.4985 - binary_crossentropy: 0.4985\n",
      "Epoch 13: val_loss improved from 0.50542 to 0.50332, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4986 - binary_crossentropy: 0.4986 - val_loss: 0.5033 - val_binary_crossentropy: 0.5033 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "290/300 [============================>.] - ETA: 0s - loss: 0.4943 - binary_crossentropy: 0.4943\n",
      "Epoch 14: val_loss improved from 0.50332 to 0.50295, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4927 - binary_crossentropy: 0.4927 - val_loss: 0.5029 - val_binary_crossentropy: 0.5029 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "290/300 [============================>.] - ETA: 0s - loss: 0.5052 - binary_crossentropy: 0.5052\n",
      "Epoch 15: val_loss improved from 0.50295 to 0.50142, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5039 - binary_crossentropy: 0.5039 - val_loss: 0.5014 - val_binary_crossentropy: 0.5014 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.5064 - binary_crossentropy: 0.5064\n",
      "Epoch 16: val_loss improved from 0.50142 to 0.49941, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.5064 - binary_crossentropy: 0.5064 - val_loss: 0.4994 - val_binary_crossentropy: 0.4994 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "286/300 [===========================>..] - ETA: 0s - loss: 0.4954 - binary_crossentropy: 0.4954\n",
      "Epoch 17: val_loss improved from 0.49941 to 0.49830, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4947 - binary_crossentropy: 0.4947 - val_loss: 0.4983 - val_binary_crossentropy: 0.4983 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "289/300 [===========================>..] - ETA: 0s - loss: 0.4947 - binary_crossentropy: 0.4947\n",
      "Epoch 18: val_loss improved from 0.49830 to 0.49789, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4927 - binary_crossentropy: 0.4927 - val_loss: 0.4979 - val_binary_crossentropy: 0.4979 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "289/300 [===========================>..] - ETA: 0s - loss: 0.5054 - binary_crossentropy: 0.5054\n",
      "Epoch 19: val_loss improved from 0.49789 to 0.49780, saving model to model_keras.h5\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5005 - binary_crossentropy: 0.5005 - val_loss: 0.4978 - val_binary_crossentropy: 0.4978 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.4944 - binary_crossentropy: 0.4944\n",
      "Epoch 20: val_loss did not improve from 0.49780\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4944 - binary_crossentropy: 0.4944 - val_loss: 0.4990 - val_binary_crossentropy: 0.4990 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.4846 - binary_crossentropy: 0.4846\n",
      "Epoch 21: val_loss improved from 0.49780 to 0.49708, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4896 - binary_crossentropy: 0.4896 - val_loss: 0.4971 - val_binary_crossentropy: 0.4971 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.4964 - binary_crossentropy: 0.4964\n",
      "Epoch 22: val_loss improved from 0.49708 to 0.49668, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4975 - binary_crossentropy: 0.4975 - val_loss: 0.4967 - val_binary_crossentropy: 0.4967 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.4877 - binary_crossentropy: 0.4877\n",
      "Epoch 23: val_loss did not improve from 0.49668\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4877 - binary_crossentropy: 0.4877 - val_loss: 0.4983 - val_binary_crossentropy: 0.4983 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "290/300 [============================>.] - ETA: 0s - loss: 0.4912 - binary_crossentropy: 0.4912\n",
      "Epoch 24: val_loss improved from 0.49668 to 0.49600, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.4898 - binary_crossentropy: 0.4898 - val_loss: 0.4960 - val_binary_crossentropy: 0.4960 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "290/300 [============================>.] - ETA: 0s - loss: 0.4948 - binary_crossentropy: 0.4948\n",
      "Epoch 25: val_loss did not improve from 0.49600\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4919 - binary_crossentropy: 0.4919 - val_loss: 0.4961 - val_binary_crossentropy: 0.4961 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.4906 - binary_crossentropy: 0.4906\n",
      "Epoch 26: val_loss improved from 0.49600 to 0.49553, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 0.4904 - binary_crossentropy: 0.4904 - val_loss: 0.4955 - val_binary_crossentropy: 0.4955 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "295/300 [============================>.] - ETA: 0s - loss: 0.4859 - binary_crossentropy: 0.4859\n",
      "Epoch 27: val_loss did not improve from 0.49553\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4899 - binary_crossentropy: 0.4899 - val_loss: 0.4962 - val_binary_crossentropy: 0.4962 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "298/300 [============================>.] - ETA: 0s - loss: 0.4919 - binary_crossentropy: 0.4919\n",
      "Epoch 28: val_loss did not improve from 0.49553\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4915 - binary_crossentropy: 0.4915 - val_loss: 0.4956 - val_binary_crossentropy: 0.4956 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "298/300 [============================>.] - ETA: 0s - loss: 0.4829 - binary_crossentropy: 0.4829\n",
      "Epoch 29: val_loss did not improve from 0.49553\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4824 - binary_crossentropy: 0.4824 - val_loss: 0.4968 - val_binary_crossentropy: 0.4968 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "296/300 [============================>.] - ETA: 0s - loss: 0.4848 - binary_crossentropy: 0.4848\n",
      "Epoch 30: val_loss did not improve from 0.49553\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4847 - binary_crossentropy: 0.4847 - val_loss: 0.4965 - val_binary_crossentropy: 0.4965 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "296/300 [============================>.] - ETA: 0s - loss: 0.4831 - binary_crossentropy: 0.4831\n",
      "Epoch 31: val_loss did not improve from 0.49553\n",
      "Restoring model weights from the end of the best epoch: 26.\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4820 - binary_crossentropy: 0.4820 - val_loss: 0.4976 - val_binary_crossentropy: 0.4976 - lr: 0.0010\n",
      "Epoch 31: early stopping\n",
      "INFO:tensorflow:Assets written to: ram://52929f39-44f0-4fd1-8c17-0a06af4a886e/assets\n",
      "75/75 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 4ms/step\n",
      "[accuracy] tr: 0.76, va: 0.761667\n",
      "-------------------- result --------------------\n",
      "[[0.         0.77125    0.75666667]\n",
      " [1.         0.76291667 0.76166667]\n",
      " [2.         0.77541667 0.77      ]\n",
      " [3.         0.77583333 0.76      ]\n",
      " [4.         0.76083333 0.76166667]]\n",
      "[cv] tr: 0.77+-0.01, va: 0.76\n",
      "[oof] 0.7620\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "train_oof, metrics = train_nn(X_train, y_train, id_train, list_nfold=[0,1,2,3,4], n_splits=5, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "669556ca-f257-450a-8f6b-17ea79cff3ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>0.384083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3832</td>\n",
       "      <td>0.104301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4927</td>\n",
       "      <td>0.444559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index      pred\n",
       "0    200  0.384083\n",
       "1   3832  0.104301\n",
       "2   4927  0.444559"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_oof[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaffb7f-6fa3-431f-bd0a-26028e65a3f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c6f5328-321e-4d4c-b9e3-54e304d25b0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_nn(input_x,\n",
    "               input_id,\n",
    "               list_nfold=[0,1,2,3,4],\n",
    "               ):\n",
    "    pred = np.zeros((len(input_x), len(list_nfold)))\n",
    "    for nfold in list_nfold:\n",
    "        print('-'*20, nfold, '-'*20)\n",
    "        fname_nn = 'model/nn/model_nn_fold{}.pickle'.format(nfold)\n",
    "        with open(fname_nn, 'rb')as f:\n",
    "            model = pickle.load(f)\n",
    "        pred[:,nfold] = np.squeeze(model.predict(input_x))\n",
    "        \n",
    "    pred = pd.concat([\n",
    "        input_id,\n",
    "        pd.DataFrame({'pred':pred.mean(axis=1)}),], axis=1)\n",
    "    \n",
    "    print('Done')\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52c0dba4-b4e5-4909-98a9-4e004551adef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- 0 --------------------\n",
      "63/63 [==============================] - 0s 3ms/step\n",
      "-------------------- 1 --------------------\n",
      "63/63 [==============================] - 0s 3ms/step\n",
      "-------------------- 2 --------------------\n",
      "63/63 [==============================] - 0s 3ms/step\n",
      "-------------------- 3 --------------------\n",
      "63/63 [==============================] - 0s 3ms/step\n",
      "-------------------- 4 --------------------\n",
      "63/63 [==============================] - 0s 3ms/step\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "test_pred_proba = predict_nn(X_test,\n",
    "                    id_test,\n",
    "                    list_nfold=[0,1,2,3,4],\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6c5584d0-f263-4fdf-a668-3baa08295883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>398</td>\n",
       "      <td>0.221906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3833</td>\n",
       "      <td>0.103745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4836</td>\n",
       "      <td>0.110308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index      pred\n",
       "0    398  0.221906\n",
       "1   3833  0.103745\n",
       "2   4836  0.110308"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_proba[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "107ce42c-c7be-4a6d-9cdf-a96ae39d26ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>398</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3833</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4836</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  pred\n",
       "0    398     0\n",
       "1   3833     0\n",
       "2   4836     0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = test_pred_proba.copy()  \n",
    "test_pred['pred']=np.where(test_pred['pred'] < 0.5, 0, 1)\n",
    "test_pred[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce77972c-1165-472e-b3ef-a618d3337493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>0.384083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3832</td>\n",
       "      <td>0.104301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4927</td>\n",
       "      <td>0.444559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index      pred\n",
       "0    200  0.384083\n",
       "1   3832  0.104301\n",
       "2   4927  0.444559"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_oof[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "25b5152f-4db6-4adf-95f5-e6ce25a30cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred.to_csv('sub/submission_nn.csv', index=None, header=False,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92863fdc-93b2-4de1-8c86-bc5de5eef7c5",
   "metadata": {},
   "source": [
    "## アンサンブル用データ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0c848410-f964-4be3-bcd6-2af32407dacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    " \n",
    "with open('ensamble/nn_train.pickle', mode='wb') as fo:\n",
    "    pickle.dump(train_oof, fo)\n",
    "    \n",
    "with open('ensamble/nn_test.pickle', mode='wb') as fo:\n",
    "    pickle.dump(test_pred_proba, fo)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d692713-afd8-49ea-ad14-b8521a8744de",
   "metadata": {},
   "source": [
    "## ベースライン検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d92d4c-6746-4881-be65-1672bae19199",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, x_va2, y_tr, y_va2 = train_test_split(X_train,\n",
    "                                           y_train,\n",
    "                                           test_size=0.2,\n",
    "                                           shuffle=True,\n",
    "                                           stratify=y_train,\n",
    "                                           random_state=random_state)\n",
    "print('検証データ: ',x_tr.shape, y_tr.shape)\n",
    "print('ベースライン検証データ: ',x_va2.shape, y_va2.shape)\n",
    "\n",
    "x_tr1, x_va1, y_tr1, y_va1 = train_test_split(x_tr,\n",
    "                                              y_tr,\n",
    "                                              test_size=0.2,\n",
    "                                              shuffle=True,\n",
    "                                              stratify=y_tr,\n",
    "                                              random_state=random_state)\n",
    "print('検証データ(train): ',x_tr1.shape, y_tr1.shape)\n",
    "print('検証データ(test): ',x_va1.shape, y_va1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a8ab0e25-c221-4477-9610-d177b1a72b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 5.56109 | val_0_unsup_loss_numpy: 164.60769653320312|  0:00:00s\n",
      "epoch 10 | loss: 0.91191 | val_0_unsup_loss_numpy: 0.9158899784088135|  0:00:02s\n",
      "epoch 20 | loss: 0.91583 | val_0_unsup_loss_numpy: 0.8389000296592712|  0:00:04s\n",
      "epoch 30 | loss: 0.8828  | val_0_unsup_loss_numpy: 0.8041800260543823|  0:00:06s\n",
      "epoch 40 | loss: 0.82442 | val_0_unsup_loss_numpy: 0.8322700262069702|  0:00:07s\n",
      "epoch 50 | loss: 0.86601 | val_0_unsup_loss_numpy: 0.8119699954986572|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 53 with best_epoch = 33 and best_val_0_unsup_loss_numpy = 0.7945299744606018\n",
      "epoch 0  | loss: 1.27547 | valid_auc: 0.66521 |  0:00:00s\n",
      "epoch 10 | loss: 0.46441 | valid_auc: 0.70492 |  0:00:01s\n",
      "epoch 20 | loss: 0.43486 | valid_auc: 0.67533 |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 7 and best_valid_auc = 0.72117\n"
     ]
    }
   ],
   "source": [
    "#validation結果\n",
    "pretrainer = TabNetPretrainer(**params)\n",
    "pretrainer.fit(\n",
    "    X_train=x_tr1,\n",
    "    eval_set=[x_va1],\n",
    "    max_epochs=200,\n",
    "    patience=20, batch_size=256, virtual_batch_size=128,\n",
    "    num_workers=1, drop_last=True)\n",
    "model = TabNetClassifier(**params)\n",
    "model.fit(\n",
    "    X_train=x_tr1,\n",
    "    y_train=y_tr1,\n",
    "    eval_set=[(x_va1, y_va1)],\n",
    "    eval_name = [\"valid\"],\n",
    "    eval_metric = [\"auc\"],\n",
    "    max_epochs=200,\n",
    "    patience=20, \n",
    "    batch_size=256,\n",
    "    virtual_batch_size=128,\n",
    "    num_workers=0, \n",
    "    drop_last=False,\n",
    "    from_unsupervised=pretrainer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "31d4307d-4a67-4228-85a7-140c68b0f999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[検証データ] acc: 0.7729\n",
      "[ベースライン検証データ] acc: 0.7667\n",
      "[検証データ] auc: 0.7212\n",
      "[ベースライン検証データ] auc: 0.7820\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#評価指標の差\n",
    "y_va1_pred = model.predict(x_va1)\n",
    "y_va2_pred = model.predict(x_va2)\n",
    "print('[検証データ] acc: {:.4f}'.format(accuracy_score(y_va1, y_va1_pred)))\n",
    "print('[ベースライン検証データ] acc: {:.4f}'.format(accuracy_score(y_va2, y_va2_pred)))\n",
    "\n",
    "y_va1_pred_proba = model.predict_proba(x_va1)\n",
    "y_va2_pred_proba = model.predict_proba(x_va2)\n",
    "print('[検証データ] auc: {:.4f}'.format(roc_auc_score(y_va1, y_va1_pred_proba[:,1])))\n",
    "print('[ベースライン検証データ] auc: {:.4f}'.format(roc_auc_score(y_va2, y_va2_pred_proba[:,1])))\n",
    "\n",
    "\n",
    "for param in ['loss', 'valid_auc']:\n",
    "    plt.plot(model.history[param], label=param)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "92b503ae-74e3-4b01-a728-9919ca203629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "検証データ\n",
      "[[324  41]\n",
      " [ 68  47]]\n",
      "[[0.675      0.08541667]\n",
      " [0.14166667 0.09791667]]\n",
      "ベースライン検証データ\n",
      "[[413  44]\n",
      " [ 96  47]]\n",
      "[[0.68833333 0.07333333]\n",
      " [0.16       0.07833333]]\n"
     ]
    }
   ],
   "source": [
    "#誤分類の分布\n",
    "print('検証データ')\n",
    "print(confusion_matrix(y_va1, y_va1_pred))\n",
    "print(confusion_matrix(y_va1, y_va1_pred, normalize='all'))\n",
    "\n",
    "print('ベースライン検証データ')\n",
    "print(confusion_matrix(y_va2, y_va2_pred))\n",
    "print(confusion_matrix(y_va2, y_va2_pred, normalize='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1da92e26-8aee-4c0d-983d-d5e7271a59b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0xffff38fc0880>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 予測値の分布\n",
    "y_va1_pred_prob = model.predict_proba(x_va1)[:,1]\n",
    "y_va2_pred_prob = model.predict_proba(x_va2)[:,1]\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "fig.add_subplot(2,1,1)\n",
    "plt.title('validation_data')\n",
    "plt.hist(y_va1_pred_prob[np.array(y_va1).reshape(-1)==1], bins=10, alpha=0.5, label='1')\n",
    "plt.hist(y_va1_pred_prob[np.array(y_va1).reshape(-1)==0], bins=10, alpha=0.5, label='0')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "fig.add_subplot(2,1,2)\n",
    "plt.title('basreline_validation_data')\n",
    "plt.hist(y_va2_pred_prob[np.array(y_va2).reshape(-1)==1], bins=10, alpha=0.5, label='1')\n",
    "plt.hist(y_va2_pred_prob[np.array(y_va2).reshape(-1)==0], bins=10, alpha=0.5, label='0')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537739d4-699c-4dde-ae15-fe05b29a4163",
   "metadata": {},
   "source": [
    "## チューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "19c70df6-ccf0-4d7b-87cb-e03980d2e2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "70364048-9098-4c29-aae7-26d2fc215125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 探索しないパラメータ\n",
    "\n",
    "params_base = {\n",
    "   'optimizer_fn': torch.optim.Adam,\n",
    "   'optimizer_params': {'lr':2e-2,'weight_decay':1e-5},\n",
    "   'mask_type': \"entmax\",#AttentiveTransformerでマスク作るのにどっちの関数を使うか'sparsemax'or'entmax'\n",
    "   'scheduler_params':{'mode': \"min\",'patience': 5,'min_lr': 1e-5,'factor': 0.9, 'scheduler_fn': torch.optim.lr_scheduler.ReduceLROnPlateau,},\n",
    "   'verbose':10,\n",
    "   'seed': 123,\n",
    "}\n",
    "\n",
    "def objective(trial):\n",
    "    # 探索するパラメータ\n",
    "    params_tuning = {\n",
    "        'n_d': trial.suggest_int('n_d',8,64),\n",
    "        'n_a': trial.suggest_int('n_a',8,64),\n",
    "        'n_steps': trial.suggest_int('n_steps', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 1.0, 2.0),\n",
    "        'mask_type': trial.suggest_categorical('mask_type', ['entmatx','sparsemax']),\n",
    "    }\n",
    "    params_tuning.update(params_base)\n",
    "    \n",
    "    # モデル学習・評価\n",
    "    list_metrics = []\n",
    "    cv = list(StratifiedKFold(n_splits=4, shuffle=True, random_state=random_state).split(X_train, y_train))\n",
    "    for nfold in np.arange(4):\n",
    "        idx_tr, idx_va = cv[nfold][0], cv[nfold][1]\n",
    "        x_tr, y_tr = X_train.loc[idx_tr, :], y_train.loc[idx_tr, :]\n",
    "        x_va, y_va = X_train.loc[idx_va, :], y_train.loc[idx_va, :]\n",
    "       \n",
    "        model = TabNetClassifier(**params)\n",
    "        model.fit(\n",
    "            X_train=x_tr,\n",
    "            y_train=y_tr,\n",
    "            eval_set=[(x_va, y_va)],\n",
    "            eval_name = [\"valid\"],\n",
    "            eval_metric = [\"auc\"],\n",
    "            max_epochs=200,\n",
    "            patience=20, \n",
    "            batch_size=256,\n",
    "            virtual_batch_size=128,\n",
    "            num_workers=0, \n",
    "            drop_last=False,\n",
    "            from_unsupervised=pretrainer\n",
    "        )\n",
    "        y_va_pred = model.predict_proba(x_va)[:,1]\n",
    "        metric_va = accuracy_score(y_va, np.where(y_va_pred>0.5, 1, 0))\n",
    "        list_metrics.append(metric_va)\n",
    "        \n",
    "    # 評価値の計算\n",
    "    metrics = np.mean(list_metrics)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "690a0925-a533-485e-94d9-ac5e95f7fab5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 14:25:48,430]\u001b[0m A new study created in memory with name: no-name-c35efe1d-ef61-45ef-ad8a-57bab052cf43\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 110376.18994| val_0_unsup_loss_numpy: 73749.171875|  0:00:00s\n",
      "epoch 10 | loss: 711.68524| val_0_unsup_loss_numpy: 1904.2930908203125|  0:00:01s\n",
      "epoch 20 | loss: 38.57199| val_0_unsup_loss_numpy: 564.6282958984375|  0:00:03s\n",
      "epoch 30 | loss: 33.08686| val_0_unsup_loss_numpy: 261.15325927734375|  0:00:04s\n",
      "epoch 40 | loss: 56.23543| val_0_unsup_loss_numpy: 73.16909790039062|  0:00:06s\n",
      "epoch 50 | loss: 17.6799 | val_0_unsup_loss_numpy: 213.44993591308594|  0:00:07s\n",
      "epoch 60 | loss: 63.28276| val_0_unsup_loss_numpy: 203.166259765625|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_unsup_loss_numpy = 21.714920043945312\n",
      "epoch 0  | loss: 0.65593 | valid_auc: 0.52988 |  0:00:00s\n",
      "epoch 10 | loss: 0.46077 | valid_auc: 0.48522 |  0:00:01s\n",
      "epoch 20 | loss: 0.44638 | valid_auc: 0.6017  |  0:00:01s\n",
      "epoch 30 | loss: 0.42583 | valid_auc: 0.68155 |  0:00:02s\n",
      "epoch 40 | loss: 0.41678 | valid_auc: 0.71286 |  0:00:03s\n",
      "epoch 50 | loss: 0.41089 | valid_auc: 0.73018 |  0:00:04s\n",
      "epoch 60 | loss: 0.40975 | valid_auc: 0.73012 |  0:00:04s\n",
      "epoch 70 | loss: 0.39049 | valid_auc: 0.74623 |  0:00:05s\n",
      "epoch 80 | loss: 0.38759 | valid_auc: 0.74778 |  0:00:06s\n",
      "epoch 90 | loss: 0.37898 | valid_auc: 0.73636 |  0:00:07s\n",
      "epoch 100| loss: 0.37382 | valid_auc: 0.74636 |  0:00:07s\n",
      "epoch 110| loss: 0.3718  | valid_auc: 0.74494 |  0:00:08s\n",
      "epoch 120| loss: 0.35987 | valid_auc: 0.75244 |  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 102 and best_valid_auc = 0.75667\n",
      "epoch 0  | loss: 94370.03027| val_0_unsup_loss_numpy: 17508.9921875|  0:00:00s\n",
      "epoch 10 | loss: 50.14755| val_0_unsup_loss_numpy: 386726.625|  0:00:01s\n",
      "epoch 20 | loss: 26.16672| val_0_unsup_loss_numpy: 3018.63134765625|  0:00:03s\n",
      "epoch 30 | loss: 66.98974| val_0_unsup_loss_numpy: 114.78819274902344|  0:00:04s\n",
      "epoch 40 | loss: 21.48916| val_0_unsup_loss_numpy: 5724.16064453125|  0:00:06s\n",
      "epoch 50 | loss: 14.70267| val_0_unsup_loss_numpy: 458.03485107421875|  0:00:07s\n",
      "epoch 60 | loss: 5.83688 | val_0_unsup_loss_numpy: 18.403139114379883|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_unsup_loss_numpy = 4.336289882659912\n",
      "epoch 0  | loss: 0.61599 | valid_auc: 0.5856  |  0:00:00s\n",
      "epoch 10 | loss: 0.44422 | valid_auc: 0.68138 |  0:00:00s\n",
      "epoch 20 | loss: 0.43285 | valid_auc: 0.68199 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_valid_auc = 0.7296\n",
      "epoch 0  | loss: 172606.04688| val_0_unsup_loss_numpy: 443988.1875|  0:00:00s\n",
      "epoch 10 | loss: 302.13637| val_0_unsup_loss_numpy: 122103.421875|  0:00:01s\n",
      "epoch 20 | loss: 50.9832 | val_0_unsup_loss_numpy: 21448.150390625|  0:00:03s\n",
      "epoch 30 | loss: 101.98731| val_0_unsup_loss_numpy: 1743.913330078125|  0:00:04s\n",
      "epoch 40 | loss: 14.66358| val_0_unsup_loss_numpy: 7841.11767578125|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_unsup_loss_numpy = 1078.7030029296875\n",
      "epoch 0  | loss: 0.63946 | valid_auc: 0.49185 |  0:00:00s\n",
      "epoch 10 | loss: 0.45231 | valid_auc: 0.6158  |  0:00:00s\n",
      "epoch 20 | loss: 0.44445 | valid_auc: 0.66228 |  0:00:01s\n",
      "epoch 30 | loss: 0.42855 | valid_auc: 0.6914  |  0:00:02s\n",
      "epoch 40 | loss: 0.41515 | valid_auc: 0.71254 |  0:00:03s\n",
      "epoch 50 | loss: 0.40289 | valid_auc: 0.72965 |  0:00:03s\n",
      "epoch 60 | loss: 0.38184 | valid_auc: 0.7246  |  0:00:04s\n",
      "epoch 70 | loss: 0.3712  | valid_auc: 0.72264 |  0:00:05s\n",
      "epoch 80 | loss: 0.37229 | valid_auc: 0.69919 |  0:00:05s\n",
      "\n",
      "Early stopping occurred at epoch 85 with best_epoch = 65 and best_valid_auc = 0.73217\n",
      "epoch 0  | loss: 94648.92773| val_0_unsup_loss_numpy: 26948.091796875|  0:00:00s\n",
      "epoch 10 | loss: 66.31772| val_0_unsup_loss_numpy: 398369.0|  0:00:01s\n",
      "epoch 20 | loss: 58.88721| val_0_unsup_loss_numpy: 15123.94921875|  0:00:03s\n",
      "epoch 30 | loss: 253.05495| val_0_unsup_loss_numpy: 10266.33984375|  0:00:04s\n",
      "epoch 40 | loss: 15.11711| val_0_unsup_loss_numpy: 19079.060546875|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_unsup_loss_numpy = 202.38461303710938\n",
      "epoch 0  | loss: 0.66325 | valid_auc: 0.59274 |  0:00:00s\n",
      "epoch 10 | loss: 0.44115 | valid_auc: 0.64124 |  0:00:00s\n",
      "epoch 20 | loss: 0.43025 | valid_auc: 0.5524  |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 14:26:41,692]\u001b[0m Trial 0 finished with value: 0.7656666666666667 and parameters: {'n_d': 47, 'n_a': 24, 'n_step': 3, 'gamma': 1.5513147690828912, 'mask_type': 'entmatx'}. Best is trial 0 with value: 0.7656666666666667.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 | loss: 0.42158 | valid_auc: 0.6189  |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 12 and best_valid_auc = 0.67009\n",
      "epoch 0  | loss: 110376.18994| val_0_unsup_loss_numpy: 73749.171875|  0:00:00s\n",
      "epoch 10 | loss: 711.68524| val_0_unsup_loss_numpy: 1904.2930908203125|  0:00:01s\n",
      "epoch 20 | loss: 38.57199| val_0_unsup_loss_numpy: 564.6282958984375|  0:00:03s\n",
      "epoch 30 | loss: 33.08686| val_0_unsup_loss_numpy: 261.15325927734375|  0:00:04s\n",
      "epoch 40 | loss: 56.23543| val_0_unsup_loss_numpy: 73.16909790039062|  0:00:06s\n",
      "epoch 50 | loss: 17.6799 | val_0_unsup_loss_numpy: 213.44993591308594|  0:00:07s\n",
      "epoch 60 | loss: 63.28276| val_0_unsup_loss_numpy: 203.166259765625|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_unsup_loss_numpy = 21.714920043945312\n",
      "epoch 0  | loss: 0.65593 | valid_auc: 0.52988 |  0:00:00s\n",
      "epoch 10 | loss: 0.46077 | valid_auc: 0.48522 |  0:00:00s\n",
      "epoch 20 | loss: 0.44638 | valid_auc: 0.6017  |  0:00:01s\n",
      "epoch 30 | loss: 0.42583 | valid_auc: 0.68155 |  0:00:02s\n",
      "epoch 40 | loss: 0.41678 | valid_auc: 0.71286 |  0:00:03s\n",
      "epoch 50 | loss: 0.41089 | valid_auc: 0.73018 |  0:00:03s\n",
      "epoch 60 | loss: 0.40975 | valid_auc: 0.73012 |  0:00:04s\n",
      "epoch 70 | loss: 0.39049 | valid_auc: 0.74623 |  0:00:05s\n",
      "epoch 80 | loss: 0.38759 | valid_auc: 0.74778 |  0:00:06s\n",
      "epoch 90 | loss: 0.37898 | valid_auc: 0.73636 |  0:00:06s\n",
      "epoch 100| loss: 0.37382 | valid_auc: 0.74636 |  0:00:07s\n",
      "epoch 110| loss: 0.3718  | valid_auc: 0.74494 |  0:00:08s\n",
      "epoch 120| loss: 0.35987 | valid_auc: 0.75244 |  0:00:08s\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 102 and best_valid_auc = 0.75667\n",
      "epoch 0  | loss: 94370.03027| val_0_unsup_loss_numpy: 17508.9921875|  0:00:00s\n",
      "epoch 10 | loss: 50.14755| val_0_unsup_loss_numpy: 386726.625|  0:00:01s\n",
      "epoch 20 | loss: 26.16672| val_0_unsup_loss_numpy: 3018.63134765625|  0:00:03s\n",
      "epoch 30 | loss: 66.98974| val_0_unsup_loss_numpy: 114.78819274902344|  0:00:04s\n",
      "epoch 40 | loss: 21.48916| val_0_unsup_loss_numpy: 5724.16064453125|  0:00:06s\n",
      "epoch 50 | loss: 14.70267| val_0_unsup_loss_numpy: 458.03485107421875|  0:00:07s\n",
      "epoch 60 | loss: 5.83688 | val_0_unsup_loss_numpy: 18.403139114379883|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_unsup_loss_numpy = 4.336289882659912\n",
      "epoch 0  | loss: 0.61599 | valid_auc: 0.5856  |  0:00:00s\n",
      "epoch 10 | loss: 0.44422 | valid_auc: 0.68138 |  0:00:00s\n",
      "epoch 20 | loss: 0.43285 | valid_auc: 0.68199 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_valid_auc = 0.7296\n",
      "epoch 0  | loss: 172606.04688| val_0_unsup_loss_numpy: 443988.1875|  0:00:00s\n",
      "epoch 10 | loss: 302.13637| val_0_unsup_loss_numpy: 122103.421875|  0:00:01s\n",
      "epoch 20 | loss: 50.9832 | val_0_unsup_loss_numpy: 21448.150390625|  0:00:03s\n",
      "epoch 30 | loss: 101.98731| val_0_unsup_loss_numpy: 1743.913330078125|  0:00:04s\n",
      "epoch 40 | loss: 14.66358| val_0_unsup_loss_numpy: 7841.11767578125|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_unsup_loss_numpy = 1078.7030029296875\n",
      "epoch 0  | loss: 0.63946 | valid_auc: 0.49185 |  0:00:00s\n",
      "epoch 10 | loss: 0.45231 | valid_auc: 0.6158  |  0:00:00s\n",
      "epoch 20 | loss: 0.44445 | valid_auc: 0.66228 |  0:00:01s\n",
      "epoch 30 | loss: 0.42855 | valid_auc: 0.6914  |  0:00:02s\n",
      "epoch 40 | loss: 0.41515 | valid_auc: 0.71254 |  0:00:03s\n",
      "epoch 50 | loss: 0.40289 | valid_auc: 0.72965 |  0:00:03s\n",
      "epoch 60 | loss: 0.38184 | valid_auc: 0.7246  |  0:00:04s\n",
      "epoch 70 | loss: 0.3712  | valid_auc: 0.72264 |  0:00:05s\n",
      "epoch 80 | loss: 0.37229 | valid_auc: 0.69919 |  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 85 with best_epoch = 65 and best_valid_auc = 0.73217\n",
      "epoch 0  | loss: 94648.92773| val_0_unsup_loss_numpy: 26948.091796875|  0:00:00s\n",
      "epoch 10 | loss: 66.31772| val_0_unsup_loss_numpy: 398369.0|  0:00:01s\n",
      "epoch 20 | loss: 58.88721| val_0_unsup_loss_numpy: 15123.94921875|  0:00:03s\n",
      "epoch 30 | loss: 253.05495| val_0_unsup_loss_numpy: 10266.33984375|  0:00:04s\n",
      "epoch 40 | loss: 15.11711| val_0_unsup_loss_numpy: 19079.060546875|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_unsup_loss_numpy = 202.38461303710938\n",
      "epoch 0  | loss: 0.66325 | valid_auc: 0.59274 |  0:00:00s\n",
      "epoch 10 | loss: 0.44115 | valid_auc: 0.64124 |  0:00:00s\n",
      "epoch 20 | loss: 0.43025 | valid_auc: 0.5524  |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 14:27:34,868]\u001b[0m Trial 1 finished with value: 0.7656666666666667 and parameters: {'n_d': 63, 'n_a': 47, 'n_step': 5, 'gamma': 1.3921175181941505, 'mask_type': 'sparsemax'}. Best is trial 0 with value: 0.7656666666666667.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 | loss: 0.42158 | valid_auc: 0.6189  |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 12 and best_valid_auc = 0.67009\n",
      "epoch 0  | loss: 110376.18994| val_0_unsup_loss_numpy: 73749.171875|  0:00:00s\n",
      "epoch 10 | loss: 711.68524| val_0_unsup_loss_numpy: 1904.2930908203125|  0:00:01s\n",
      "epoch 20 | loss: 38.57199| val_0_unsup_loss_numpy: 564.6282958984375|  0:00:03s\n",
      "epoch 30 | loss: 33.08686| val_0_unsup_loss_numpy: 261.15325927734375|  0:00:04s\n",
      "epoch 40 | loss: 56.23543| val_0_unsup_loss_numpy: 73.16909790039062|  0:00:06s\n",
      "epoch 50 | loss: 17.6799 | val_0_unsup_loss_numpy: 213.44993591308594|  0:00:07s\n",
      "epoch 60 | loss: 63.28276| val_0_unsup_loss_numpy: 203.166259765625|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_unsup_loss_numpy = 21.714920043945312\n",
      "epoch 0  | loss: 0.65593 | valid_auc: 0.52988 |  0:00:00s\n",
      "epoch 10 | loss: 0.46077 | valid_auc: 0.48522 |  0:00:00s\n",
      "epoch 20 | loss: 0.44638 | valid_auc: 0.6017  |  0:00:01s\n",
      "epoch 30 | loss: 0.42583 | valid_auc: 0.68155 |  0:00:02s\n",
      "epoch 40 | loss: 0.41678 | valid_auc: 0.71286 |  0:00:03s\n",
      "epoch 50 | loss: 0.41089 | valid_auc: 0.73018 |  0:00:03s\n",
      "epoch 60 | loss: 0.40975 | valid_auc: 0.73012 |  0:00:04s\n",
      "epoch 70 | loss: 0.39049 | valid_auc: 0.74623 |  0:00:05s\n",
      "epoch 80 | loss: 0.38759 | valid_auc: 0.74778 |  0:00:05s\n",
      "epoch 90 | loss: 0.37898 | valid_auc: 0.73636 |  0:00:06s\n",
      "epoch 100| loss: 0.37382 | valid_auc: 0.74636 |  0:00:07s\n",
      "epoch 110| loss: 0.3718  | valid_auc: 0.74494 |  0:00:08s\n",
      "epoch 120| loss: 0.35987 | valid_auc: 0.75244 |  0:00:08s\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 102 and best_valid_auc = 0.75667\n",
      "epoch 0  | loss: 94370.03027| val_0_unsup_loss_numpy: 17508.9921875|  0:00:00s\n",
      "epoch 10 | loss: 50.14755| val_0_unsup_loss_numpy: 386726.625|  0:00:01s\n",
      "epoch 20 | loss: 26.16672| val_0_unsup_loss_numpy: 3018.63134765625|  0:00:03s\n",
      "epoch 30 | loss: 66.98974| val_0_unsup_loss_numpy: 114.78819274902344|  0:00:05s\n",
      "epoch 40 | loss: 21.48916| val_0_unsup_loss_numpy: 5724.16064453125|  0:00:07s\n",
      "epoch 50 | loss: 14.70267| val_0_unsup_loss_numpy: 458.03485107421875|  0:00:08s\n",
      "epoch 60 | loss: 5.83688 | val_0_unsup_loss_numpy: 18.403139114379883|  0:00:10s\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_unsup_loss_numpy = 4.336289882659912\n",
      "epoch 0  | loss: 0.61599 | valid_auc: 0.5856  |  0:00:00s\n",
      "epoch 10 | loss: 0.44422 | valid_auc: 0.68138 |  0:00:00s\n",
      "epoch 20 | loss: 0.43285 | valid_auc: 0.68199 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_valid_auc = 0.7296\n",
      "epoch 0  | loss: 172606.04688| val_0_unsup_loss_numpy: 443988.1875|  0:00:00s\n",
      "epoch 10 | loss: 302.13637| val_0_unsup_loss_numpy: 122103.421875|  0:00:01s\n",
      "epoch 20 | loss: 50.9832 | val_0_unsup_loss_numpy: 21448.150390625|  0:00:03s\n",
      "epoch 30 | loss: 101.98731| val_0_unsup_loss_numpy: 1743.913330078125|  0:00:04s\n",
      "epoch 40 | loss: 14.66358| val_0_unsup_loss_numpy: 7841.11767578125|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_unsup_loss_numpy = 1078.7030029296875\n",
      "epoch 0  | loss: 0.63946 | valid_auc: 0.49185 |  0:00:00s\n",
      "epoch 10 | loss: 0.45231 | valid_auc: 0.6158  |  0:00:00s\n",
      "epoch 20 | loss: 0.44445 | valid_auc: 0.66228 |  0:00:01s\n",
      "epoch 30 | loss: 0.42855 | valid_auc: 0.6914  |  0:00:02s\n",
      "epoch 40 | loss: 0.41515 | valid_auc: 0.71254 |  0:00:03s\n",
      "epoch 50 | loss: 0.40289 | valid_auc: 0.72965 |  0:00:03s\n",
      "epoch 60 | loss: 0.38184 | valid_auc: 0.7246  |  0:00:04s\n",
      "epoch 70 | loss: 0.3712  | valid_auc: 0.72264 |  0:00:05s\n",
      "epoch 80 | loss: 0.37229 | valid_auc: 0.69919 |  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 85 with best_epoch = 65 and best_valid_auc = 0.73217\n",
      "epoch 0  | loss: 94648.92773| val_0_unsup_loss_numpy: 26948.091796875|  0:00:00s\n",
      "epoch 10 | loss: 66.31772| val_0_unsup_loss_numpy: 398369.0|  0:00:01s\n",
      "epoch 20 | loss: 58.88721| val_0_unsup_loss_numpy: 15123.94921875|  0:00:03s\n",
      "epoch 30 | loss: 253.05495| val_0_unsup_loss_numpy: 10266.33984375|  0:00:04s\n",
      "epoch 40 | loss: 15.11711| val_0_unsup_loss_numpy: 19079.060546875|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_unsup_loss_numpy = 202.38461303710938\n",
      "epoch 0  | loss: 0.66325 | valid_auc: 0.59274 |  0:00:00s\n",
      "epoch 10 | loss: 0.44115 | valid_auc: 0.64124 |  0:00:00s\n",
      "epoch 20 | loss: 0.43025 | valid_auc: 0.5524  |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 14:28:29,472]\u001b[0m Trial 2 finished with value: 0.7656666666666667 and parameters: {'n_d': 32, 'n_a': 11, 'n_step': 4, 'gamma': 1.7379954057320357, 'mask_type': 'entmatx'}. Best is trial 0 with value: 0.7656666666666667.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 | loss: 0.42158 | valid_auc: 0.6189  |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 12 and best_valid_auc = 0.67009\n",
      "epoch 0  | loss: 110376.18994| val_0_unsup_loss_numpy: 73749.171875|  0:00:00s\n",
      "epoch 10 | loss: 711.68524| val_0_unsup_loss_numpy: 1904.2930908203125|  0:00:01s\n",
      "epoch 20 | loss: 38.57199| val_0_unsup_loss_numpy: 564.6282958984375|  0:00:03s\n",
      "epoch 30 | loss: 33.08686| val_0_unsup_loss_numpy: 261.15325927734375|  0:00:04s\n",
      "epoch 40 | loss: 56.23543| val_0_unsup_loss_numpy: 73.16909790039062|  0:00:06s\n",
      "epoch 50 | loss: 17.6799 | val_0_unsup_loss_numpy: 213.44993591308594|  0:00:07s\n",
      "epoch 60 | loss: 63.28276| val_0_unsup_loss_numpy: 203.166259765625|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_unsup_loss_numpy = 21.714920043945312\n",
      "epoch 0  | loss: 0.65593 | valid_auc: 0.52988 |  0:00:00s\n",
      "epoch 10 | loss: 0.46077 | valid_auc: 0.48522 |  0:00:00s\n",
      "epoch 20 | loss: 0.44638 | valid_auc: 0.6017  |  0:00:01s\n",
      "epoch 30 | loss: 0.42583 | valid_auc: 0.68155 |  0:00:02s\n",
      "epoch 40 | loss: 0.41678 | valid_auc: 0.71286 |  0:00:03s\n",
      "epoch 50 | loss: 0.41089 | valid_auc: 0.73018 |  0:00:03s\n",
      "epoch 60 | loss: 0.40975 | valid_auc: 0.73012 |  0:00:04s\n",
      "epoch 70 | loss: 0.39049 | valid_auc: 0.74623 |  0:00:05s\n",
      "epoch 80 | loss: 0.38759 | valid_auc: 0.74778 |  0:00:05s\n",
      "epoch 90 | loss: 0.37898 | valid_auc: 0.73636 |  0:00:06s\n",
      "epoch 100| loss: 0.37382 | valid_auc: 0.74636 |  0:00:07s\n",
      "epoch 110| loss: 0.3718  | valid_auc: 0.74494 |  0:00:08s\n",
      "epoch 120| loss: 0.35987 | valid_auc: 0.75244 |  0:00:08s\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 102 and best_valid_auc = 0.75667\n",
      "epoch 0  | loss: 94370.03027| val_0_unsup_loss_numpy: 17508.9921875|  0:00:00s\n",
      "epoch 10 | loss: 50.14755| val_0_unsup_loss_numpy: 386726.625|  0:00:01s\n",
      "epoch 20 | loss: 26.16672| val_0_unsup_loss_numpy: 3018.63134765625|  0:00:03s\n",
      "epoch 30 | loss: 66.98974| val_0_unsup_loss_numpy: 114.78819274902344|  0:00:04s\n",
      "epoch 40 | loss: 21.48916| val_0_unsup_loss_numpy: 5724.16064453125|  0:00:06s\n",
      "epoch 50 | loss: 14.70267| val_0_unsup_loss_numpy: 458.03485107421875|  0:00:07s\n",
      "epoch 60 | loss: 5.83688 | val_0_unsup_loss_numpy: 18.403139114379883|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_unsup_loss_numpy = 4.336289882659912\n",
      "epoch 0  | loss: 0.61599 | valid_auc: 0.5856  |  0:00:00s\n",
      "epoch 10 | loss: 0.44422 | valid_auc: 0.68138 |  0:00:00s\n",
      "epoch 20 | loss: 0.43285 | valid_auc: 0.68199 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_valid_auc = 0.7296\n",
      "epoch 0  | loss: 172606.04688| val_0_unsup_loss_numpy: 443988.1875|  0:00:00s\n",
      "epoch 10 | loss: 302.13637| val_0_unsup_loss_numpy: 122103.421875|  0:00:01s\n",
      "epoch 20 | loss: 50.9832 | val_0_unsup_loss_numpy: 21448.150390625|  0:00:03s\n",
      "epoch 30 | loss: 101.98731| val_0_unsup_loss_numpy: 1743.913330078125|  0:00:04s\n",
      "epoch 40 | loss: 14.66358| val_0_unsup_loss_numpy: 7841.11767578125|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_unsup_loss_numpy = 1078.7030029296875\n",
      "epoch 0  | loss: 0.63946 | valid_auc: 0.49185 |  0:00:00s\n",
      "epoch 10 | loss: 0.45231 | valid_auc: 0.6158  |  0:00:00s\n",
      "epoch 20 | loss: 0.44445 | valid_auc: 0.66228 |  0:00:01s\n",
      "epoch 30 | loss: 0.42855 | valid_auc: 0.6914  |  0:00:02s\n",
      "epoch 40 | loss: 0.41515 | valid_auc: 0.71254 |  0:00:03s\n",
      "epoch 50 | loss: 0.40289 | valid_auc: 0.72965 |  0:00:04s\n",
      "epoch 60 | loss: 0.38184 | valid_auc: 0.7246  |  0:00:04s\n",
      "epoch 70 | loss: 0.3712  | valid_auc: 0.72264 |  0:00:05s\n",
      "epoch 80 | loss: 0.37229 | valid_auc: 0.69919 |  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 85 with best_epoch = 65 and best_valid_auc = 0.73217\n",
      "epoch 0  | loss: 94648.92773| val_0_unsup_loss_numpy: 26948.091796875|  0:00:00s\n",
      "epoch 10 | loss: 66.31772| val_0_unsup_loss_numpy: 398369.0|  0:00:01s\n",
      "epoch 20 | loss: 58.88721| val_0_unsup_loss_numpy: 15123.94921875|  0:00:03s\n",
      "epoch 30 | loss: 253.05495| val_0_unsup_loss_numpy: 10266.33984375|  0:00:04s\n",
      "epoch 40 | loss: 15.11711| val_0_unsup_loss_numpy: 19079.060546875|  0:00:05s\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_unsup_loss_numpy = 202.38461303710938\n",
      "epoch 0  | loss: 0.66325 | valid_auc: 0.59274 |  0:00:00s\n",
      "epoch 10 | loss: 0.44115 | valid_auc: 0.64124 |  0:00:00s\n",
      "epoch 20 | loss: 0.43025 | valid_auc: 0.5524  |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 14:29:23,197]\u001b[0m Trial 3 finished with value: 0.7656666666666667 and parameters: {'n_d': 38, 'n_a': 38, 'n_step': 7, 'gamma': 1.8494317940777896, 'mask_type': 'entmatx'}. Best is trial 0 with value: 0.7656666666666667.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 | loss: 0.42158 | valid_auc: 0.6189  |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 12 and best_valid_auc = 0.67009\n",
      "epoch 0  | loss: 110376.18994| val_0_unsup_loss_numpy: 73749.171875|  0:00:00s\n",
      "epoch 10 | loss: 711.68524| val_0_unsup_loss_numpy: 1904.2930908203125|  0:00:01s\n",
      "epoch 20 | loss: 38.57199| val_0_unsup_loss_numpy: 564.6282958984375|  0:00:03s\n",
      "epoch 30 | loss: 33.08686| val_0_unsup_loss_numpy: 261.15325927734375|  0:00:04s\n",
      "epoch 40 | loss: 56.23543| val_0_unsup_loss_numpy: 73.16909790039062|  0:00:06s\n",
      "epoch 50 | loss: 17.6799 | val_0_unsup_loss_numpy: 213.44993591308594|  0:00:07s\n",
      "epoch 60 | loss: 63.28276| val_0_unsup_loss_numpy: 203.166259765625|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_unsup_loss_numpy = 21.714920043945312\n",
      "epoch 0  | loss: 0.65593 | valid_auc: 0.52988 |  0:00:00s\n",
      "epoch 10 | loss: 0.46077 | valid_auc: 0.48522 |  0:00:00s\n",
      "epoch 20 | loss: 0.44638 | valid_auc: 0.6017  |  0:00:01s\n",
      "epoch 30 | loss: 0.42583 | valid_auc: 0.68155 |  0:00:02s\n",
      "epoch 40 | loss: 0.41678 | valid_auc: 0.71286 |  0:00:03s\n",
      "epoch 50 | loss: 0.41089 | valid_auc: 0.73018 |  0:00:04s\n",
      "epoch 60 | loss: 0.40975 | valid_auc: 0.73012 |  0:00:04s\n",
      "epoch 70 | loss: 0.39049 | valid_auc: 0.74623 |  0:00:06s\n",
      "epoch 80 | loss: 0.38759 | valid_auc: 0.74778 |  0:00:07s\n",
      "epoch 90 | loss: 0.37898 | valid_auc: 0.73636 |  0:00:08s\n",
      "epoch 100| loss: 0.37382 | valid_auc: 0.74636 |  0:00:08s\n",
      "epoch 110| loss: 0.3718  | valid_auc: 0.74494 |  0:00:09s\n",
      "epoch 120| loss: 0.35987 | valid_auc: 0.75244 |  0:00:10s\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 102 and best_valid_auc = 0.75667\n",
      "epoch 0  | loss: 94370.03027| val_0_unsup_loss_numpy: 17508.9921875|  0:00:00s\n",
      "epoch 10 | loss: 50.14755| val_0_unsup_loss_numpy: 386726.625|  0:00:01s\n",
      "epoch 20 | loss: 26.16672| val_0_unsup_loss_numpy: 3018.63134765625|  0:00:03s\n",
      "epoch 30 | loss: 66.98974| val_0_unsup_loss_numpy: 114.78819274902344|  0:00:04s\n",
      "epoch 40 | loss: 21.48916| val_0_unsup_loss_numpy: 5724.16064453125|  0:00:06s\n",
      "epoch 50 | loss: 14.70267| val_0_unsup_loss_numpy: 458.03485107421875|  0:00:07s\n",
      "epoch 60 | loss: 5.83688 | val_0_unsup_loss_numpy: 18.403139114379883|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_unsup_loss_numpy = 4.336289882659912\n",
      "epoch 0  | loss: 0.61599 | valid_auc: 0.5856  |  0:00:00s\n",
      "epoch 10 | loss: 0.44422 | valid_auc: 0.68138 |  0:00:00s\n",
      "epoch 20 | loss: 0.43285 | valid_auc: 0.68199 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_valid_auc = 0.7296\n",
      "epoch 0  | loss: 172606.04688| val_0_unsup_loss_numpy: 443988.1875|  0:00:00s\n",
      "epoch 10 | loss: 302.13637| val_0_unsup_loss_numpy: 122103.421875|  0:00:01s\n",
      "epoch 20 | loss: 50.9832 | val_0_unsup_loss_numpy: 21448.150390625|  0:00:03s\n",
      "epoch 30 | loss: 101.98731| val_0_unsup_loss_numpy: 1743.913330078125|  0:00:04s\n",
      "epoch 40 | loss: 14.66358| val_0_unsup_loss_numpy: 7841.11767578125|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_unsup_loss_numpy = 1078.7030029296875\n",
      "epoch 0  | loss: 0.63946 | valid_auc: 0.49185 |  0:00:00s\n",
      "epoch 10 | loss: 0.45231 | valid_auc: 0.6158  |  0:00:00s\n",
      "epoch 20 | loss: 0.44445 | valid_auc: 0.66228 |  0:00:01s\n",
      "epoch 30 | loss: 0.42855 | valid_auc: 0.6914  |  0:00:02s\n",
      "epoch 40 | loss: 0.41515 | valid_auc: 0.71254 |  0:00:03s\n",
      "epoch 50 | loss: 0.40289 | valid_auc: 0.72965 |  0:00:04s\n",
      "epoch 60 | loss: 0.38184 | valid_auc: 0.7246  |  0:00:04s\n",
      "epoch 70 | loss: 0.3712  | valid_auc: 0.72264 |  0:00:05s\n",
      "epoch 80 | loss: 0.37229 | valid_auc: 0.69919 |  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 85 with best_epoch = 65 and best_valid_auc = 0.73217\n",
      "epoch 0  | loss: 94648.92773| val_0_unsup_loss_numpy: 26948.091796875|  0:00:00s\n",
      "epoch 10 | loss: 66.31772| val_0_unsup_loss_numpy: 398369.0|  0:00:01s\n",
      "epoch 20 | loss: 58.88721| val_0_unsup_loss_numpy: 15123.94921875|  0:00:05s\n",
      "epoch 30 | loss: 253.05495| val_0_unsup_loss_numpy: 10266.33984375|  0:00:06s\n",
      "epoch 40 | loss: 15.11711| val_0_unsup_loss_numpy: 19079.060546875|  0:00:08s\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_unsup_loss_numpy = 202.38461303710938\n",
      "epoch 0  | loss: 0.66325 | valid_auc: 0.59274 |  0:00:00s\n",
      "epoch 10 | loss: 0.44115 | valid_auc: 0.64124 |  0:00:00s\n",
      "epoch 20 | loss: 0.43025 | valid_auc: 0.5524  |  0:00:01s\n",
      "epoch 30 | loss: 0.42158 | valid_auc: 0.6189  |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 12 and best_valid_auc = 0.67009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 14:30:20,692]\u001b[0m Trial 4 finished with value: 0.7656666666666667 and parameters: {'n_d': 49, 'n_a': 26, 'n_step': 4, 'gamma': 1.2282632308789556, 'mask_type': 'sparsemax'}. Best is trial 0 with value: 0.7656666666666667.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 110376.18994| val_0_unsup_loss_numpy: 73749.171875|  0:00:00s\n",
      "epoch 10 | loss: 711.68524| val_0_unsup_loss_numpy: 1904.2930908203125|  0:00:01s\n",
      "epoch 20 | loss: 38.57199| val_0_unsup_loss_numpy: 564.6282958984375|  0:00:03s\n",
      "epoch 30 | loss: 33.08686| val_0_unsup_loss_numpy: 261.15325927734375|  0:00:04s\n",
      "epoch 40 | loss: 56.23543| val_0_unsup_loss_numpy: 73.16909790039062|  0:00:06s\n",
      "epoch 50 | loss: 17.6799 | val_0_unsup_loss_numpy: 213.44993591308594|  0:00:07s\n",
      "epoch 60 | loss: 63.28276| val_0_unsup_loss_numpy: 203.166259765625|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_unsup_loss_numpy = 21.714920043945312\n",
      "epoch 0  | loss: 0.65593 | valid_auc: 0.52988 |  0:00:00s\n",
      "epoch 10 | loss: 0.46077 | valid_auc: 0.48522 |  0:00:00s\n",
      "epoch 20 | loss: 0.44638 | valid_auc: 0.6017  |  0:00:01s\n",
      "epoch 30 | loss: 0.42583 | valid_auc: 0.68155 |  0:00:02s\n",
      "epoch 40 | loss: 0.41678 | valid_auc: 0.71286 |  0:00:03s\n",
      "epoch 50 | loss: 0.41089 | valid_auc: 0.73018 |  0:00:04s\n",
      "epoch 60 | loss: 0.40975 | valid_auc: 0.73012 |  0:00:04s\n",
      "epoch 70 | loss: 0.39049 | valid_auc: 0.74623 |  0:00:05s\n",
      "epoch 80 | loss: 0.38759 | valid_auc: 0.74778 |  0:00:06s\n",
      "epoch 90 | loss: 0.37898 | valid_auc: 0.73636 |  0:00:07s\n",
      "epoch 100| loss: 0.37382 | valid_auc: 0.74636 |  0:00:07s\n",
      "epoch 110| loss: 0.3718  | valid_auc: 0.74494 |  0:00:08s\n",
      "epoch 120| loss: 0.35987 | valid_auc: 0.75244 |  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 102 and best_valid_auc = 0.75667\n",
      "epoch 0  | loss: 94370.03027| val_0_unsup_loss_numpy: 17508.9921875|  0:00:00s\n",
      "epoch 10 | loss: 50.14755| val_0_unsup_loss_numpy: 386726.625|  0:00:01s\n",
      "epoch 20 | loss: 26.16672| val_0_unsup_loss_numpy: 3018.63134765625|  0:00:03s\n",
      "epoch 30 | loss: 66.98974| val_0_unsup_loss_numpy: 114.78819274902344|  0:00:04s\n",
      "epoch 40 | loss: 21.48916| val_0_unsup_loss_numpy: 5724.16064453125|  0:00:06s\n",
      "epoch 50 | loss: 14.70267| val_0_unsup_loss_numpy: 458.03485107421875|  0:00:07s\n",
      "epoch 60 | loss: 5.83688 | val_0_unsup_loss_numpy: 18.403139114379883|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_unsup_loss_numpy = 4.336289882659912\n",
      "epoch 0  | loss: 0.61599 | valid_auc: 0.5856  |  0:00:00s\n",
      "epoch 10 | loss: 0.44422 | valid_auc: 0.68138 |  0:00:00s\n",
      "epoch 20 | loss: 0.43285 | valid_auc: 0.68199 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_valid_auc = 0.7296\n",
      "epoch 0  | loss: 172606.04688| val_0_unsup_loss_numpy: 443988.1875|  0:00:00s\n",
      "epoch 10 | loss: 302.13637| val_0_unsup_loss_numpy: 122103.421875|  0:00:01s\n",
      "epoch 20 | loss: 50.9832 | val_0_unsup_loss_numpy: 21448.150390625|  0:00:03s\n",
      "epoch 30 | loss: 101.98731| val_0_unsup_loss_numpy: 1743.913330078125|  0:00:04s\n",
      "epoch 40 | loss: 14.66358| val_0_unsup_loss_numpy: 7841.11767578125|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_unsup_loss_numpy = 1078.7030029296875\n",
      "epoch 0  | loss: 0.63946 | valid_auc: 0.49185 |  0:00:00s\n",
      "epoch 10 | loss: 0.45231 | valid_auc: 0.6158  |  0:00:00s\n",
      "epoch 20 | loss: 0.44445 | valid_auc: 0.66228 |  0:00:01s\n",
      "epoch 30 | loss: 0.42855 | valid_auc: 0.6914  |  0:00:02s\n",
      "epoch 40 | loss: 0.41515 | valid_auc: 0.71254 |  0:00:03s\n",
      "epoch 50 | loss: 0.40289 | valid_auc: 0.72965 |  0:00:03s\n",
      "epoch 60 | loss: 0.38184 | valid_auc: 0.7246  |  0:00:04s\n",
      "epoch 70 | loss: 0.3712  | valid_auc: 0.72264 |  0:00:05s\n",
      "epoch 80 | loss: 0.37229 | valid_auc: 0.69919 |  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 85 with best_epoch = 65 and best_valid_auc = 0.73217\n",
      "epoch 0  | loss: 94648.92773| val_0_unsup_loss_numpy: 26948.091796875|  0:00:00s\n",
      "epoch 10 | loss: 66.31772| val_0_unsup_loss_numpy: 398369.0|  0:00:01s\n",
      "epoch 20 | loss: 58.88721| val_0_unsup_loss_numpy: 15123.94921875|  0:00:03s\n",
      "epoch 30 | loss: 253.05495| val_0_unsup_loss_numpy: 10266.33984375|  0:00:04s\n",
      "epoch 40 | loss: 15.11711| val_0_unsup_loss_numpy: 19079.060546875|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_unsup_loss_numpy = 202.38461303710938\n",
      "epoch 0  | loss: 0.66325 | valid_auc: 0.59274 |  0:00:00s\n",
      "epoch 10 | loss: 0.44115 | valid_auc: 0.64124 |  0:00:00s\n",
      "epoch 20 | loss: 0.43025 | valid_auc: 0.5524  |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 14:31:14,682]\u001b[0m Trial 5 finished with value: 0.7656666666666667 and parameters: {'n_d': 13, 'n_a': 32, 'n_step': 5, 'gamma': 1.4936850976503062, 'mask_type': 'entmatx'}. Best is trial 0 with value: 0.7656666666666667.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 | loss: 0.42158 | valid_auc: 0.6189  |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 12 and best_valid_auc = 0.67009\n",
      "epoch 0  | loss: 110376.18994| val_0_unsup_loss_numpy: 73749.171875|  0:00:00s\n",
      "epoch 10 | loss: 711.68524| val_0_unsup_loss_numpy: 1904.2930908203125|  0:00:01s\n",
      "epoch 20 | loss: 38.57199| val_0_unsup_loss_numpy: 564.6282958984375|  0:00:03s\n",
      "epoch 30 | loss: 33.08686| val_0_unsup_loss_numpy: 261.15325927734375|  0:00:04s\n",
      "epoch 40 | loss: 56.23543| val_0_unsup_loss_numpy: 73.16909790039062|  0:00:06s\n",
      "epoch 50 | loss: 17.6799 | val_0_unsup_loss_numpy: 213.44993591308594|  0:00:07s\n",
      "epoch 60 | loss: 63.28276| val_0_unsup_loss_numpy: 203.166259765625|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_unsup_loss_numpy = 21.714920043945312\n",
      "epoch 0  | loss: 0.65593 | valid_auc: 0.52988 |  0:00:00s\n",
      "epoch 10 | loss: 0.46077 | valid_auc: 0.48522 |  0:00:00s\n",
      "epoch 20 | loss: 0.44638 | valid_auc: 0.6017  |  0:00:01s\n",
      "epoch 30 | loss: 0.42583 | valid_auc: 0.68155 |  0:00:02s\n",
      "epoch 40 | loss: 0.41678 | valid_auc: 0.71286 |  0:00:03s\n",
      "epoch 50 | loss: 0.41089 | valid_auc: 0.73018 |  0:00:04s\n",
      "epoch 60 | loss: 0.40975 | valid_auc: 0.73012 |  0:00:04s\n",
      "epoch 70 | loss: 0.39049 | valid_auc: 0.74623 |  0:00:05s\n",
      "epoch 80 | loss: 0.38759 | valid_auc: 0.74778 |  0:00:06s\n",
      "epoch 90 | loss: 0.37898 | valid_auc: 0.73636 |  0:00:07s\n",
      "epoch 100| loss: 0.37382 | valid_auc: 0.74636 |  0:00:07s\n",
      "epoch 110| loss: 0.3718  | valid_auc: 0.74494 |  0:00:08s\n",
      "epoch 120| loss: 0.35987 | valid_auc: 0.75244 |  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 102 and best_valid_auc = 0.75667\n",
      "epoch 0  | loss: 94370.03027| val_0_unsup_loss_numpy: 17508.9921875|  0:00:00s\n",
      "epoch 10 | loss: 50.14755| val_0_unsup_loss_numpy: 386726.625|  0:00:01s\n",
      "epoch 20 | loss: 26.16672| val_0_unsup_loss_numpy: 3018.63134765625|  0:00:03s\n",
      "epoch 30 | loss: 66.98974| val_0_unsup_loss_numpy: 114.78819274902344|  0:00:04s\n",
      "epoch 40 | loss: 21.48916| val_0_unsup_loss_numpy: 5724.16064453125|  0:00:06s\n",
      "epoch 50 | loss: 14.70267| val_0_unsup_loss_numpy: 458.03485107421875|  0:00:08s\n",
      "epoch 60 | loss: 5.83688 | val_0_unsup_loss_numpy: 18.403139114379883|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_unsup_loss_numpy = 4.336289882659912\n",
      "epoch 0  | loss: 0.61599 | valid_auc: 0.5856  |  0:00:00s\n",
      "epoch 10 | loss: 0.44422 | valid_auc: 0.68138 |  0:00:00s\n",
      "epoch 20 | loss: 0.43285 | valid_auc: 0.68199 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_valid_auc = 0.7296\n",
      "epoch 0  | loss: 172606.04688| val_0_unsup_loss_numpy: 443988.1875|  0:00:00s\n",
      "epoch 10 | loss: 302.13637| val_0_unsup_loss_numpy: 122103.421875|  0:00:01s\n",
      "epoch 20 | loss: 50.9832 | val_0_unsup_loss_numpy: 21448.150390625|  0:00:03s\n",
      "epoch 30 | loss: 101.98731| val_0_unsup_loss_numpy: 1743.913330078125|  0:00:05s\n",
      "epoch 40 | loss: 14.66358| val_0_unsup_loss_numpy: 7841.11767578125|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_unsup_loss_numpy = 1078.7030029296875\n",
      "epoch 0  | loss: 0.63946 | valid_auc: 0.49185 |  0:00:00s\n",
      "epoch 10 | loss: 0.45231 | valid_auc: 0.6158  |  0:00:00s\n",
      "epoch 20 | loss: 0.44445 | valid_auc: 0.66228 |  0:00:01s\n",
      "epoch 30 | loss: 0.42855 | valid_auc: 0.6914  |  0:00:02s\n",
      "epoch 40 | loss: 0.41515 | valid_auc: 0.71254 |  0:00:03s\n",
      "epoch 50 | loss: 0.40289 | valid_auc: 0.72965 |  0:00:03s\n",
      "epoch 60 | loss: 0.38184 | valid_auc: 0.7246  |  0:00:04s\n",
      "epoch 70 | loss: 0.3712  | valid_auc: 0.72264 |  0:00:05s\n",
      "epoch 80 | loss: 0.37229 | valid_auc: 0.69919 |  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 85 with best_epoch = 65 and best_valid_auc = 0.73217\n",
      "epoch 0  | loss: 94648.92773| val_0_unsup_loss_numpy: 26948.091796875|  0:00:00s\n",
      "epoch 10 | loss: 66.31772| val_0_unsup_loss_numpy: 398369.0|  0:00:01s\n",
      "epoch 20 | loss: 58.88721| val_0_unsup_loss_numpy: 15123.94921875|  0:00:03s\n",
      "epoch 30 | loss: 253.05495| val_0_unsup_loss_numpy: 10266.33984375|  0:00:04s\n",
      "epoch 40 | loss: 15.11711| val_0_unsup_loss_numpy: 19079.060546875|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_unsup_loss_numpy = 202.38461303710938\n",
      "epoch 0  | loss: 0.66325 | valid_auc: 0.59274 |  0:00:00s\n",
      "epoch 10 | loss: 0.44115 | valid_auc: 0.64124 |  0:00:00s\n",
      "epoch 20 | loss: 0.43025 | valid_auc: 0.5524  |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 14:32:10,297]\u001b[0m Trial 6 finished with value: 0.7656666666666667 and parameters: {'n_d': 32, 'n_a': 58, 'n_step': 10, 'gamma': 1.5018366758843364, 'mask_type': 'entmatx'}. Best is trial 0 with value: 0.7656666666666667.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 | loss: 0.42158 | valid_auc: 0.6189  |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 12 and best_valid_auc = 0.67009\n",
      "epoch 0  | loss: 110376.18994| val_0_unsup_loss_numpy: 73749.171875|  0:00:00s\n",
      "epoch 10 | loss: 711.68524| val_0_unsup_loss_numpy: 1904.2930908203125|  0:00:01s\n",
      "epoch 20 | loss: 38.57199| val_0_unsup_loss_numpy: 564.6282958984375|  0:00:03s\n",
      "epoch 30 | loss: 33.08686| val_0_unsup_loss_numpy: 261.15325927734375|  0:00:04s\n",
      "epoch 40 | loss: 56.23543| val_0_unsup_loss_numpy: 73.16909790039062|  0:00:06s\n",
      "epoch 50 | loss: 17.6799 | val_0_unsup_loss_numpy: 213.44993591308594|  0:00:07s\n",
      "epoch 60 | loss: 63.28276| val_0_unsup_loss_numpy: 203.166259765625|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_unsup_loss_numpy = 21.714920043945312\n",
      "epoch 0  | loss: 0.65593 | valid_auc: 0.52988 |  0:00:00s\n",
      "epoch 10 | loss: 0.46077 | valid_auc: 0.48522 |  0:00:01s\n",
      "epoch 20 | loss: 0.44638 | valid_auc: 0.6017  |  0:00:01s\n",
      "epoch 30 | loss: 0.42583 | valid_auc: 0.68155 |  0:00:02s\n",
      "epoch 40 | loss: 0.41678 | valid_auc: 0.71286 |  0:00:03s\n",
      "epoch 50 | loss: 0.41089 | valid_auc: 0.73018 |  0:00:04s\n",
      "epoch 60 | loss: 0.40975 | valid_auc: 0.73012 |  0:00:05s\n",
      "epoch 70 | loss: 0.39049 | valid_auc: 0.74623 |  0:00:05s\n",
      "epoch 80 | loss: 0.38759 | valid_auc: 0.74778 |  0:00:06s\n",
      "epoch 90 | loss: 0.37898 | valid_auc: 0.73636 |  0:00:07s\n",
      "epoch 100| loss: 0.37382 | valid_auc: 0.74636 |  0:00:08s\n",
      "epoch 110| loss: 0.3718  | valid_auc: 0.74494 |  0:00:08s\n",
      "epoch 120| loss: 0.35987 | valid_auc: 0.75244 |  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 102 and best_valid_auc = 0.75667\n",
      "epoch 0  | loss: 94370.03027| val_0_unsup_loss_numpy: 17508.9921875|  0:00:00s\n",
      "epoch 10 | loss: 50.14755| val_0_unsup_loss_numpy: 386726.625|  0:00:01s\n",
      "epoch 20 | loss: 26.16672| val_0_unsup_loss_numpy: 3018.63134765625|  0:00:03s\n",
      "epoch 30 | loss: 66.98974| val_0_unsup_loss_numpy: 114.78819274902344|  0:00:04s\n",
      "epoch 40 | loss: 21.48916| val_0_unsup_loss_numpy: 5724.16064453125|  0:00:06s\n",
      "epoch 50 | loss: 14.70267| val_0_unsup_loss_numpy: 458.03485107421875|  0:00:07s\n",
      "epoch 60 | loss: 5.83688 | val_0_unsup_loss_numpy: 18.403139114379883|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_unsup_loss_numpy = 4.336289882659912\n",
      "epoch 0  | loss: 0.61599 | valid_auc: 0.5856  |  0:00:00s\n",
      "epoch 10 | loss: 0.44422 | valid_auc: 0.68138 |  0:00:00s\n",
      "epoch 20 | loss: 0.43285 | valid_auc: 0.68199 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_valid_auc = 0.7296\n",
      "epoch 0  | loss: 172606.04688| val_0_unsup_loss_numpy: 443988.1875|  0:00:00s\n",
      "epoch 10 | loss: 302.13637| val_0_unsup_loss_numpy: 122103.421875|  0:00:01s\n",
      "epoch 20 | loss: 50.9832 | val_0_unsup_loss_numpy: 21448.150390625|  0:00:03s\n",
      "epoch 30 | loss: 101.98731| val_0_unsup_loss_numpy: 1743.913330078125|  0:00:04s\n",
      "epoch 40 | loss: 14.66358| val_0_unsup_loss_numpy: 7841.11767578125|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_unsup_loss_numpy = 1078.7030029296875\n",
      "epoch 0  | loss: 0.63946 | valid_auc: 0.49185 |  0:00:00s\n",
      "epoch 10 | loss: 0.45231 | valid_auc: 0.6158  |  0:00:00s\n",
      "epoch 20 | loss: 0.44445 | valid_auc: 0.66228 |  0:00:01s\n",
      "epoch 30 | loss: 0.42855 | valid_auc: 0.6914  |  0:00:02s\n",
      "epoch 40 | loss: 0.41515 | valid_auc: 0.71254 |  0:00:03s\n",
      "epoch 50 | loss: 0.40289 | valid_auc: 0.72965 |  0:00:03s\n",
      "epoch 60 | loss: 0.38184 | valid_auc: 0.7246  |  0:00:04s\n",
      "epoch 70 | loss: 0.3712  | valid_auc: 0.72264 |  0:00:05s\n",
      "epoch 80 | loss: 0.37229 | valid_auc: 0.69919 |  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 85 with best_epoch = 65 and best_valid_auc = 0.73217\n",
      "epoch 0  | loss: 94648.92773| val_0_unsup_loss_numpy: 26948.091796875|  0:00:00s\n",
      "epoch 10 | loss: 66.31772| val_0_unsup_loss_numpy: 398369.0|  0:00:01s\n",
      "epoch 20 | loss: 58.88721| val_0_unsup_loss_numpy: 15123.94921875|  0:00:03s\n",
      "epoch 30 | loss: 253.05495| val_0_unsup_loss_numpy: 10266.33984375|  0:00:04s\n",
      "epoch 40 | loss: 15.11711| val_0_unsup_loss_numpy: 19079.060546875|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_unsup_loss_numpy = 202.38461303710938\n",
      "epoch 0  | loss: 0.66325 | valid_auc: 0.59274 |  0:00:00s\n",
      "epoch 10 | loss: 0.44115 | valid_auc: 0.64124 |  0:00:00s\n",
      "epoch 20 | loss: 0.43025 | valid_auc: 0.5524  |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 14:33:05,604]\u001b[0m Trial 7 finished with value: 0.7656666666666667 and parameters: {'n_d': 26, 'n_a': 31, 'n_step': 9, 'gamma': 1.2504553653965067, 'mask_type': 'sparsemax'}. Best is trial 0 with value: 0.7656666666666667.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 | loss: 0.42158 | valid_auc: 0.6189  |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 12 and best_valid_auc = 0.67009\n",
      "epoch 0  | loss: 110376.18994| val_0_unsup_loss_numpy: 73749.171875|  0:00:00s\n",
      "epoch 10 | loss: 711.68524| val_0_unsup_loss_numpy: 1904.2930908203125|  0:00:01s\n",
      "epoch 20 | loss: 38.57199| val_0_unsup_loss_numpy: 564.6282958984375|  0:00:03s\n",
      "epoch 30 | loss: 33.08686| val_0_unsup_loss_numpy: 261.15325927734375|  0:00:04s\n",
      "epoch 40 | loss: 56.23543| val_0_unsup_loss_numpy: 73.16909790039062|  0:00:06s\n",
      "epoch 50 | loss: 17.6799 | val_0_unsup_loss_numpy: 213.44993591308594|  0:00:08s\n",
      "epoch 60 | loss: 63.28276| val_0_unsup_loss_numpy: 203.166259765625|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_unsup_loss_numpy = 21.714920043945312\n",
      "epoch 0  | loss: 0.65593 | valid_auc: 0.52988 |  0:00:00s\n",
      "epoch 10 | loss: 0.46077 | valid_auc: 0.48522 |  0:00:00s\n",
      "epoch 20 | loss: 0.44638 | valid_auc: 0.6017  |  0:00:01s\n",
      "epoch 30 | loss: 0.42583 | valid_auc: 0.68155 |  0:00:02s\n",
      "epoch 40 | loss: 0.41678 | valid_auc: 0.71286 |  0:00:03s\n",
      "epoch 50 | loss: 0.41089 | valid_auc: 0.73018 |  0:00:03s\n",
      "epoch 60 | loss: 0.40975 | valid_auc: 0.73012 |  0:00:04s\n",
      "epoch 70 | loss: 0.39049 | valid_auc: 0.74623 |  0:00:05s\n",
      "epoch 80 | loss: 0.38759 | valid_auc: 0.74778 |  0:00:06s\n",
      "epoch 90 | loss: 0.37898 | valid_auc: 0.73636 |  0:00:07s\n",
      "epoch 100| loss: 0.37382 | valid_auc: 0.74636 |  0:00:08s\n",
      "epoch 110| loss: 0.3718  | valid_auc: 0.74494 |  0:00:08s\n",
      "epoch 120| loss: 0.35987 | valid_auc: 0.75244 |  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 102 and best_valid_auc = 0.75667\n",
      "epoch 0  | loss: 94370.03027| val_0_unsup_loss_numpy: 17508.9921875|  0:00:00s\n",
      "epoch 10 | loss: 50.14755| val_0_unsup_loss_numpy: 386726.625|  0:00:01s\n",
      "epoch 20 | loss: 26.16672| val_0_unsup_loss_numpy: 3018.63134765625|  0:00:03s\n",
      "epoch 30 | loss: 66.98974| val_0_unsup_loss_numpy: 114.78819274902344|  0:00:04s\n",
      "epoch 40 | loss: 21.48916| val_0_unsup_loss_numpy: 5724.16064453125|  0:00:06s\n",
      "epoch 50 | loss: 14.70267| val_0_unsup_loss_numpy: 458.03485107421875|  0:00:08s\n",
      "epoch 60 | loss: 5.83688 | val_0_unsup_loss_numpy: 18.403139114379883|  0:00:10s\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_unsup_loss_numpy = 4.336289882659912\n",
      "epoch 0  | loss: 0.61599 | valid_auc: 0.5856  |  0:00:00s\n",
      "epoch 10 | loss: 0.44422 | valid_auc: 0.68138 |  0:00:00s\n",
      "epoch 20 | loss: 0.43285 | valid_auc: 0.68199 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_valid_auc = 0.7296\n",
      "epoch 0  | loss: 172606.04688| val_0_unsup_loss_numpy: 443988.1875|  0:00:00s\n",
      "epoch 10 | loss: 302.13637| val_0_unsup_loss_numpy: 122103.421875|  0:00:01s\n",
      "epoch 20 | loss: 50.9832 | val_0_unsup_loss_numpy: 21448.150390625|  0:00:03s\n",
      "epoch 30 | loss: 101.98731| val_0_unsup_loss_numpy: 1743.913330078125|  0:00:04s\n",
      "epoch 40 | loss: 14.66358| val_0_unsup_loss_numpy: 7841.11767578125|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_unsup_loss_numpy = 1078.7030029296875\n",
      "epoch 0  | loss: 0.63946 | valid_auc: 0.49185 |  0:00:00s\n",
      "epoch 10 | loss: 0.45231 | valid_auc: 0.6158  |  0:00:00s\n",
      "epoch 20 | loss: 0.44445 | valid_auc: 0.66228 |  0:00:01s\n",
      "epoch 30 | loss: 0.42855 | valid_auc: 0.6914  |  0:00:02s\n",
      "epoch 40 | loss: 0.41515 | valid_auc: 0.71254 |  0:00:03s\n",
      "epoch 50 | loss: 0.40289 | valid_auc: 0.72965 |  0:00:04s\n",
      "epoch 60 | loss: 0.38184 | valid_auc: 0.7246  |  0:00:05s\n",
      "epoch 70 | loss: 0.3712  | valid_auc: 0.72264 |  0:00:06s\n",
      "epoch 80 | loss: 0.37229 | valid_auc: 0.69919 |  0:00:07s\n",
      "\n",
      "Early stopping occurred at epoch 85 with best_epoch = 65 and best_valid_auc = 0.73217\n",
      "epoch 0  | loss: 94648.92773| val_0_unsup_loss_numpy: 26948.091796875|  0:00:00s\n",
      "epoch 10 | loss: 66.31772| val_0_unsup_loss_numpy: 398369.0|  0:00:02s\n",
      "epoch 20 | loss: 58.88721| val_0_unsup_loss_numpy: 15123.94921875|  0:00:03s\n",
      "epoch 30 | loss: 253.05495| val_0_unsup_loss_numpy: 10266.33984375|  0:00:05s\n",
      "epoch 40 | loss: 15.11711| val_0_unsup_loss_numpy: 19079.060546875|  0:00:07s\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_unsup_loss_numpy = 202.38461303710938\n",
      "epoch 0  | loss: 0.66325 | valid_auc: 0.59274 |  0:00:00s\n",
      "epoch 10 | loss: 0.44115 | valid_auc: 0.64124 |  0:00:00s\n",
      "epoch 20 | loss: 0.43025 | valid_auc: 0.5524  |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 14:34:03,700]\u001b[0m Trial 8 finished with value: 0.7656666666666667 and parameters: {'n_d': 37, 'n_a': 42, 'n_step': 2, 'gamma': 1.8263408005068333, 'mask_type': 'entmatx'}. Best is trial 0 with value: 0.7656666666666667.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 | loss: 0.42158 | valid_auc: 0.6189  |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 12 and best_valid_auc = 0.67009\n",
      "epoch 0  | loss: 110376.18994| val_0_unsup_loss_numpy: 73749.171875|  0:00:00s\n",
      "epoch 10 | loss: 711.68524| val_0_unsup_loss_numpy: 1904.2930908203125|  0:00:01s\n",
      "epoch 20 | loss: 38.57199| val_0_unsup_loss_numpy: 564.6282958984375|  0:00:03s\n",
      "epoch 30 | loss: 33.08686| val_0_unsup_loss_numpy: 261.15325927734375|  0:00:05s\n",
      "epoch 40 | loss: 56.23543| val_0_unsup_loss_numpy: 73.16909790039062|  0:00:06s\n",
      "epoch 50 | loss: 17.6799 | val_0_unsup_loss_numpy: 213.44993591308594|  0:00:08s\n",
      "epoch 60 | loss: 63.28276| val_0_unsup_loss_numpy: 203.166259765625|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_unsup_loss_numpy = 21.714920043945312\n",
      "epoch 0  | loss: 0.65593 | valid_auc: 0.52988 |  0:00:00s\n",
      "epoch 10 | loss: 0.46077 | valid_auc: 0.48522 |  0:00:00s\n",
      "epoch 20 | loss: 0.44638 | valid_auc: 0.6017  |  0:00:01s\n",
      "epoch 30 | loss: 0.42583 | valid_auc: 0.68155 |  0:00:02s\n",
      "epoch 40 | loss: 0.41678 | valid_auc: 0.71286 |  0:00:03s\n",
      "epoch 50 | loss: 0.41089 | valid_auc: 0.73018 |  0:00:04s\n",
      "epoch 60 | loss: 0.40975 | valid_auc: 0.73012 |  0:00:05s\n",
      "epoch 70 | loss: 0.39049 | valid_auc: 0.74623 |  0:00:06s\n",
      "epoch 80 | loss: 0.38759 | valid_auc: 0.74778 |  0:00:07s\n",
      "epoch 90 | loss: 0.37898 | valid_auc: 0.73636 |  0:00:08s\n",
      "epoch 100| loss: 0.37382 | valid_auc: 0.74636 |  0:00:08s\n",
      "epoch 110| loss: 0.3718  | valid_auc: 0.74494 |  0:00:09s\n",
      "epoch 120| loss: 0.35987 | valid_auc: 0.75244 |  0:00:10s\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 102 and best_valid_auc = 0.75667\n",
      "epoch 0  | loss: 94370.03027| val_0_unsup_loss_numpy: 17508.9921875|  0:00:00s\n",
      "epoch 10 | loss: 50.14755| val_0_unsup_loss_numpy: 386726.625|  0:00:01s\n",
      "epoch 20 | loss: 26.16672| val_0_unsup_loss_numpy: 3018.63134765625|  0:00:03s\n",
      "epoch 30 | loss: 66.98974| val_0_unsup_loss_numpy: 114.78819274902344|  0:00:04s\n",
      "epoch 40 | loss: 21.48916| val_0_unsup_loss_numpy: 5724.16064453125|  0:00:06s\n",
      "epoch 50 | loss: 14.70267| val_0_unsup_loss_numpy: 458.03485107421875|  0:00:07s\n",
      "epoch 60 | loss: 5.83688 | val_0_unsup_loss_numpy: 18.403139114379883|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_unsup_loss_numpy = 4.336289882659912\n",
      "epoch 0  | loss: 0.61599 | valid_auc: 0.5856  |  0:00:00s\n",
      "epoch 10 | loss: 0.44422 | valid_auc: 0.68138 |  0:00:00s\n",
      "epoch 20 | loss: 0.43285 | valid_auc: 0.68199 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_valid_auc = 0.7296\n",
      "epoch 0  | loss: 172606.04688| val_0_unsup_loss_numpy: 443988.1875|  0:00:00s\n",
      "epoch 10 | loss: 302.13637| val_0_unsup_loss_numpy: 122103.421875|  0:00:01s\n",
      "epoch 20 | loss: 50.9832 | val_0_unsup_loss_numpy: 21448.150390625|  0:00:03s\n",
      "epoch 30 | loss: 101.98731| val_0_unsup_loss_numpy: 1743.913330078125|  0:00:04s\n",
      "epoch 40 | loss: 14.66358| val_0_unsup_loss_numpy: 7841.11767578125|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_unsup_loss_numpy = 1078.7030029296875\n",
      "epoch 0  | loss: 0.63946 | valid_auc: 0.49185 |  0:00:00s\n",
      "epoch 10 | loss: 0.45231 | valid_auc: 0.6158  |  0:00:00s\n",
      "epoch 20 | loss: 0.44445 | valid_auc: 0.66228 |  0:00:01s\n",
      "epoch 30 | loss: 0.42855 | valid_auc: 0.6914  |  0:00:02s\n",
      "epoch 40 | loss: 0.41515 | valid_auc: 0.71254 |  0:00:03s\n",
      "epoch 50 | loss: 0.40289 | valid_auc: 0.72965 |  0:00:04s\n",
      "epoch 60 | loss: 0.38184 | valid_auc: 0.7246  |  0:00:04s\n",
      "epoch 70 | loss: 0.3712  | valid_auc: 0.72264 |  0:00:05s\n",
      "epoch 80 | loss: 0.37229 | valid_auc: 0.69919 |  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 85 with best_epoch = 65 and best_valid_auc = 0.73217\n",
      "epoch 0  | loss: 94648.92773| val_0_unsup_loss_numpy: 26948.091796875|  0:00:00s\n",
      "epoch 10 | loss: 66.31772| val_0_unsup_loss_numpy: 398369.0|  0:00:01s\n",
      "epoch 20 | loss: 58.88721| val_0_unsup_loss_numpy: 15123.94921875|  0:00:03s\n",
      "epoch 30 | loss: 253.05495| val_0_unsup_loss_numpy: 10266.33984375|  0:00:04s\n",
      "epoch 40 | loss: 15.11711| val_0_unsup_loss_numpy: 19079.060546875|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_unsup_loss_numpy = 202.38461303710938\n",
      "epoch 0  | loss: 0.66325 | valid_auc: 0.59274 |  0:00:00s\n",
      "epoch 10 | loss: 0.44115 | valid_auc: 0.64124 |  0:00:00s\n",
      "epoch 20 | loss: 0.43025 | valid_auc: 0.5524  |  0:00:01s\n",
      "epoch 30 | loss: 0.42158 | valid_auc: 0.6189  |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 12 and best_valid_auc = 0.67009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 14:35:00,323]\u001b[0m Trial 9 finished with value: 0.7656666666666667 and parameters: {'n_d': 27, 'n_a': 25, 'n_step': 5, 'gamma': 1.6813007657927965, 'mask_type': 'entmatx'}. Best is trial 0 with value: 0.7656666666666667.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 110376.18994| val_0_unsup_loss_numpy: 73749.171875|  0:00:00s\n",
      "epoch 10 | loss: 711.68524| val_0_unsup_loss_numpy: 1904.2930908203125|  0:00:01s\n",
      "epoch 20 | loss: 38.57199| val_0_unsup_loss_numpy: 564.6282958984375|  0:00:03s\n",
      "epoch 30 | loss: 33.08686| val_0_unsup_loss_numpy: 261.15325927734375|  0:00:04s\n",
      "epoch 40 | loss: 56.23543| val_0_unsup_loss_numpy: 73.16909790039062|  0:00:06s\n",
      "epoch 50 | loss: 17.6799 | val_0_unsup_loss_numpy: 213.44993591308594|  0:00:07s\n",
      "epoch 60 | loss: 63.28276| val_0_unsup_loss_numpy: 203.166259765625|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_unsup_loss_numpy = 21.714920043945312\n",
      "epoch 0  | loss: 0.65593 | valid_auc: 0.52988 |  0:00:00s\n",
      "epoch 10 | loss: 0.46077 | valid_auc: 0.48522 |  0:00:00s\n",
      "epoch 20 | loss: 0.44638 | valid_auc: 0.6017  |  0:00:01s\n",
      "epoch 30 | loss: 0.42583 | valid_auc: 0.68155 |  0:00:02s\n",
      "epoch 40 | loss: 0.41678 | valid_auc: 0.71286 |  0:00:03s\n",
      "epoch 50 | loss: 0.41089 | valid_auc: 0.73018 |  0:00:04s\n",
      "epoch 60 | loss: 0.40975 | valid_auc: 0.73012 |  0:00:05s\n",
      "epoch 70 | loss: 0.39049 | valid_auc: 0.74623 |  0:00:05s\n",
      "epoch 80 | loss: 0.38759 | valid_auc: 0.74778 |  0:00:06s\n",
      "epoch 90 | loss: 0.37898 | valid_auc: 0.73636 |  0:00:07s\n",
      "epoch 100| loss: 0.37382 | valid_auc: 0.74636 |  0:00:08s\n",
      "epoch 110| loss: 0.3718  | valid_auc: 0.74494 |  0:00:09s\n",
      "epoch 120| loss: 0.35987 | valid_auc: 0.75244 |  0:00:10s\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 102 and best_valid_auc = 0.75667\n",
      "epoch 0  | loss: 94370.03027| val_0_unsup_loss_numpy: 17508.9921875|  0:00:00s\n",
      "epoch 10 | loss: 50.14755| val_0_unsup_loss_numpy: 386726.625|  0:00:01s\n",
      "epoch 20 | loss: 26.16672| val_0_unsup_loss_numpy: 3018.63134765625|  0:00:02s\n",
      "epoch 30 | loss: 66.98974| val_0_unsup_loss_numpy: 114.78819274902344|  0:00:04s\n",
      "epoch 40 | loss: 21.48916| val_0_unsup_loss_numpy: 5724.16064453125|  0:00:05s\n",
      "epoch 50 | loss: 14.70267| val_0_unsup_loss_numpy: 458.03485107421875|  0:00:07s\n",
      "epoch 60 | loss: 5.83688 | val_0_unsup_loss_numpy: 18.403139114379883|  0:00:08s\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_unsup_loss_numpy = 4.336289882659912\n",
      "epoch 0  | loss: 0.61599 | valid_auc: 0.5856  |  0:00:00s\n",
      "epoch 10 | loss: 0.44422 | valid_auc: 0.68138 |  0:00:00s\n",
      "epoch 20 | loss: 0.43285 | valid_auc: 0.68199 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_valid_auc = 0.7296\n",
      "epoch 0  | loss: 172606.04688| val_0_unsup_loss_numpy: 443988.1875|  0:00:00s\n",
      "epoch 10 | loss: 302.13637| val_0_unsup_loss_numpy: 122103.421875|  0:00:01s\n",
      "epoch 20 | loss: 50.9832 | val_0_unsup_loss_numpy: 21448.150390625|  0:00:03s\n",
      "epoch 30 | loss: 101.98731| val_0_unsup_loss_numpy: 1743.913330078125|  0:00:04s\n",
      "epoch 40 | loss: 14.66358| val_0_unsup_loss_numpy: 7841.11767578125|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_unsup_loss_numpy = 1078.7030029296875\n",
      "epoch 0  | loss: 0.63946 | valid_auc: 0.49185 |  0:00:00s\n",
      "epoch 10 | loss: 0.45231 | valid_auc: 0.6158  |  0:00:00s\n",
      "epoch 20 | loss: 0.44445 | valid_auc: 0.66228 |  0:00:01s\n",
      "epoch 30 | loss: 0.42855 | valid_auc: 0.6914  |  0:00:02s\n",
      "epoch 40 | loss: 0.41515 | valid_auc: 0.71254 |  0:00:03s\n",
      "epoch 50 | loss: 0.40289 | valid_auc: 0.72965 |  0:00:04s\n",
      "epoch 60 | loss: 0.38184 | valid_auc: 0.7246  |  0:00:05s\n",
      "epoch 70 | loss: 0.3712  | valid_auc: 0.72264 |  0:00:05s\n",
      "epoch 80 | loss: 0.37229 | valid_auc: 0.69919 |  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 85 with best_epoch = 65 and best_valid_auc = 0.73217\n",
      "epoch 0  | loss: 94648.92773| val_0_unsup_loss_numpy: 26948.091796875|  0:00:00s\n",
      "epoch 10 | loss: 66.31772| val_0_unsup_loss_numpy: 398369.0|  0:00:01s\n",
      "epoch 20 | loss: 58.88721| val_0_unsup_loss_numpy: 15123.94921875|  0:00:03s\n",
      "epoch 30 | loss: 253.05495| val_0_unsup_loss_numpy: 10266.33984375|  0:00:04s\n",
      "epoch 40 | loss: 15.11711| val_0_unsup_loss_numpy: 19079.060546875|  0:00:05s\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_unsup_loss_numpy = 202.38461303710938\n",
      "epoch 0  | loss: 0.66325 | valid_auc: 0.59274 |  0:00:00s\n",
      "epoch 10 | loss: 0.44115 | valid_auc: 0.64124 |  0:00:00s\n",
      "epoch 20 | loss: 0.43025 | valid_auc: 0.5524  |  0:00:01s\n",
      "epoch 30 | loss: 0.42158 | valid_auc: 0.6189  |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 12 and best_valid_auc = 0.67009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 14:35:54,823]\u001b[0m Trial 10 finished with value: 0.7656666666666667 and parameters: {'n_d': 56, 'n_a': 8, 'n_step': 1, 'gamma': 1.0080611434040203, 'mask_type': 'sparsemax'}. Best is trial 0 with value: 0.7656666666666667.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 110376.18994| val_0_unsup_loss_numpy: 73749.171875|  0:00:00s\n",
      "epoch 10 | loss: 711.68524| val_0_unsup_loss_numpy: 1904.2930908203125|  0:00:01s\n",
      "epoch 20 | loss: 38.57199| val_0_unsup_loss_numpy: 564.6282958984375|  0:00:03s\n",
      "epoch 30 | loss: 33.08686| val_0_unsup_loss_numpy: 261.15325927734375|  0:00:04s\n",
      "epoch 40 | loss: 56.23543| val_0_unsup_loss_numpy: 73.16909790039062|  0:00:06s\n",
      "epoch 50 | loss: 17.6799 | val_0_unsup_loss_numpy: 213.44993591308594|  0:00:07s\n",
      "epoch 60 | loss: 63.28276| val_0_unsup_loss_numpy: 203.166259765625|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_unsup_loss_numpy = 21.714920043945312\n",
      "epoch 0  | loss: 0.65593 | valid_auc: 0.52988 |  0:00:00s\n",
      "epoch 10 | loss: 0.46077 | valid_auc: 0.48522 |  0:00:00s\n",
      "epoch 20 | loss: 0.44638 | valid_auc: 0.6017  |  0:00:01s\n",
      "epoch 30 | loss: 0.42583 | valid_auc: 0.68155 |  0:00:02s\n",
      "epoch 40 | loss: 0.41678 | valid_auc: 0.71286 |  0:00:03s\n",
      "epoch 50 | loss: 0.41089 | valid_auc: 0.73018 |  0:00:04s\n",
      "epoch 60 | loss: 0.40975 | valid_auc: 0.73012 |  0:00:05s\n",
      "epoch 70 | loss: 0.39049 | valid_auc: 0.74623 |  0:00:05s\n",
      "epoch 80 | loss: 0.38759 | valid_auc: 0.74778 |  0:00:06s\n",
      "epoch 90 | loss: 0.37898 | valid_auc: 0.73636 |  0:00:07s\n",
      "epoch 100| loss: 0.37382 | valid_auc: 0.74636 |  0:00:08s\n",
      "epoch 110| loss: 0.3718  | valid_auc: 0.74494 |  0:00:08s\n",
      "epoch 120| loss: 0.35987 | valid_auc: 0.75244 |  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 102 and best_valid_auc = 0.75667\n",
      "epoch 0  | loss: 94370.03027| val_0_unsup_loss_numpy: 17508.9921875|  0:00:00s\n",
      "epoch 10 | loss: 50.14755| val_0_unsup_loss_numpy: 386726.625|  0:00:01s\n",
      "epoch 20 | loss: 26.16672| val_0_unsup_loss_numpy: 3018.63134765625|  0:00:03s\n",
      "epoch 30 | loss: 66.98974| val_0_unsup_loss_numpy: 114.78819274902344|  0:00:04s\n",
      "epoch 40 | loss: 21.48916| val_0_unsup_loss_numpy: 5724.16064453125|  0:00:06s\n",
      "epoch 50 | loss: 14.70267| val_0_unsup_loss_numpy: 458.03485107421875|  0:00:07s\n",
      "epoch 60 | loss: 5.83688 | val_0_unsup_loss_numpy: 18.403139114379883|  0:00:08s\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_unsup_loss_numpy = 4.336289882659912\n",
      "epoch 0  | loss: 0.61599 | valid_auc: 0.5856  |  0:00:00s\n",
      "epoch 10 | loss: 0.44422 | valid_auc: 0.68138 |  0:00:00s\n",
      "epoch 20 | loss: 0.43285 | valid_auc: 0.68199 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_valid_auc = 0.7296\n",
      "epoch 0  | loss: 172606.04688| val_0_unsup_loss_numpy: 443988.1875|  0:00:00s\n",
      "epoch 10 | loss: 302.13637| val_0_unsup_loss_numpy: 122103.421875|  0:00:01s\n",
      "epoch 20 | loss: 50.9832 | val_0_unsup_loss_numpy: 21448.150390625|  0:00:03s\n",
      "epoch 30 | loss: 101.98731| val_0_unsup_loss_numpy: 1743.913330078125|  0:00:04s\n",
      "epoch 40 | loss: 14.66358| val_0_unsup_loss_numpy: 7841.11767578125|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_unsup_loss_numpy = 1078.7030029296875\n",
      "epoch 0  | loss: 0.63946 | valid_auc: 0.49185 |  0:00:00s\n",
      "epoch 10 | loss: 0.45231 | valid_auc: 0.6158  |  0:00:01s\n",
      "epoch 20 | loss: 0.44445 | valid_auc: 0.66228 |  0:00:01s\n",
      "epoch 30 | loss: 0.42855 | valid_auc: 0.6914  |  0:00:02s\n",
      "epoch 40 | loss: 0.41515 | valid_auc: 0.71254 |  0:00:03s\n",
      "epoch 50 | loss: 0.40289 | valid_auc: 0.72965 |  0:00:04s\n",
      "epoch 60 | loss: 0.38184 | valid_auc: 0.7246  |  0:00:05s\n",
      "epoch 70 | loss: 0.3712  | valid_auc: 0.72264 |  0:00:06s\n",
      "epoch 80 | loss: 0.37229 | valid_auc: 0.69919 |  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 85 with best_epoch = 65 and best_valid_auc = 0.73217\n",
      "epoch 0  | loss: 94648.92773| val_0_unsup_loss_numpy: 26948.091796875|  0:00:00s\n",
      "epoch 10 | loss: 66.31772| val_0_unsup_loss_numpy: 398369.0|  0:00:01s\n",
      "epoch 20 | loss: 58.88721| val_0_unsup_loss_numpy: 15123.94921875|  0:00:03s\n",
      "epoch 30 | loss: 253.05495| val_0_unsup_loss_numpy: 10266.33984375|  0:00:04s\n",
      "epoch 40 | loss: 15.11711| val_0_unsup_loss_numpy: 19079.060546875|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_unsup_loss_numpy = 202.38461303710938\n",
      "epoch 0  | loss: 0.66325 | valid_auc: 0.59274 |  0:00:00s\n",
      "epoch 10 | loss: 0.44115 | valid_auc: 0.64124 |  0:00:00s\n",
      "epoch 20 | loss: 0.43025 | valid_auc: 0.5524  |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 14:36:49,120]\u001b[0m Trial 11 finished with value: 0.7656666666666667 and parameters: {'n_d': 63, 'n_a': 51, 'n_step': 3, 'gamma': 1.9862890564016848, 'mask_type': 'sparsemax'}. Best is trial 0 with value: 0.7656666666666667.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 | loss: 0.42158 | valid_auc: 0.6189  |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 12 and best_valid_auc = 0.67009\n",
      "epoch 0  | loss: 110376.18994| val_0_unsup_loss_numpy: 73749.171875|  0:00:00s\n",
      "epoch 10 | loss: 711.68524| val_0_unsup_loss_numpy: 1904.2930908203125|  0:00:01s\n",
      "epoch 20 | loss: 38.57199| val_0_unsup_loss_numpy: 564.6282958984375|  0:00:03s\n",
      "epoch 30 | loss: 33.08686| val_0_unsup_loss_numpy: 261.15325927734375|  0:00:04s\n",
      "epoch 40 | loss: 56.23543| val_0_unsup_loss_numpy: 73.16909790039062|  0:00:06s\n",
      "epoch 50 | loss: 17.6799 | val_0_unsup_loss_numpy: 213.44993591308594|  0:00:07s\n",
      "epoch 60 | loss: 63.28276| val_0_unsup_loss_numpy: 203.166259765625|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_unsup_loss_numpy = 21.714920043945312\n",
      "epoch 0  | loss: 0.65593 | valid_auc: 0.52988 |  0:00:00s\n",
      "epoch 10 | loss: 0.46077 | valid_auc: 0.48522 |  0:00:00s\n",
      "epoch 20 | loss: 0.44638 | valid_auc: 0.6017  |  0:00:01s\n",
      "epoch 30 | loss: 0.42583 | valid_auc: 0.68155 |  0:00:02s\n",
      "epoch 40 | loss: 0.41678 | valid_auc: 0.71286 |  0:00:03s\n",
      "epoch 50 | loss: 0.41089 | valid_auc: 0.73018 |  0:00:04s\n",
      "epoch 60 | loss: 0.40975 | valid_auc: 0.73012 |  0:00:04s\n",
      "epoch 70 | loss: 0.39049 | valid_auc: 0.74623 |  0:00:05s\n",
      "epoch 80 | loss: 0.38759 | valid_auc: 0.74778 |  0:00:06s\n",
      "epoch 90 | loss: 0.37898 | valid_auc: 0.73636 |  0:00:07s\n",
      "epoch 100| loss: 0.37382 | valid_auc: 0.74636 |  0:00:08s\n",
      "epoch 110| loss: 0.3718  | valid_auc: 0.74494 |  0:00:09s\n",
      "epoch 120| loss: 0.35987 | valid_auc: 0.75244 |  0:00:10s\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 102 and best_valid_auc = 0.75667\n",
      "epoch 0  | loss: 94370.03027| val_0_unsup_loss_numpy: 17508.9921875|  0:00:00s\n",
      "epoch 10 | loss: 50.14755| val_0_unsup_loss_numpy: 386726.625|  0:00:01s\n",
      "epoch 20 | loss: 26.16672| val_0_unsup_loss_numpy: 3018.63134765625|  0:00:03s\n",
      "epoch 30 | loss: 66.98974| val_0_unsup_loss_numpy: 114.78819274902344|  0:00:04s\n",
      "epoch 40 | loss: 21.48916| val_0_unsup_loss_numpy: 5724.16064453125|  0:00:06s\n",
      "epoch 50 | loss: 14.70267| val_0_unsup_loss_numpy: 458.03485107421875|  0:00:07s\n",
      "epoch 60 | loss: 5.83688 | val_0_unsup_loss_numpy: 18.403139114379883|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_unsup_loss_numpy = 4.336289882659912\n",
      "epoch 0  | loss: 0.61599 | valid_auc: 0.5856  |  0:00:00s\n",
      "epoch 10 | loss: 0.44422 | valid_auc: 0.68138 |  0:00:00s\n",
      "epoch 20 | loss: 0.43285 | valid_auc: 0.68199 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_valid_auc = 0.7296\n",
      "epoch 0  | loss: 172606.04688| val_0_unsup_loss_numpy: 443988.1875|  0:00:00s\n",
      "epoch 10 | loss: 302.13637| val_0_unsup_loss_numpy: 122103.421875|  0:00:01s\n",
      "epoch 20 | loss: 50.9832 | val_0_unsup_loss_numpy: 21448.150390625|  0:00:03s\n",
      "epoch 30 | loss: 101.98731| val_0_unsup_loss_numpy: 1743.913330078125|  0:00:04s\n",
      "epoch 40 | loss: 14.66358| val_0_unsup_loss_numpy: 7841.11767578125|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_unsup_loss_numpy = 1078.7030029296875\n",
      "epoch 0  | loss: 0.63946 | valid_auc: 0.49185 |  0:00:00s\n",
      "epoch 10 | loss: 0.45231 | valid_auc: 0.6158  |  0:00:00s\n",
      "epoch 20 | loss: 0.44445 | valid_auc: 0.66228 |  0:00:01s\n",
      "epoch 30 | loss: 0.42855 | valid_auc: 0.6914  |  0:00:02s\n",
      "epoch 40 | loss: 0.41515 | valid_auc: 0.71254 |  0:00:03s\n",
      "epoch 50 | loss: 0.40289 | valid_auc: 0.72965 |  0:00:04s\n",
      "epoch 60 | loss: 0.38184 | valid_auc: 0.7246  |  0:00:05s\n",
      "epoch 70 | loss: 0.3712  | valid_auc: 0.72264 |  0:00:05s\n",
      "epoch 80 | loss: 0.37229 | valid_auc: 0.69919 |  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 85 with best_epoch = 65 and best_valid_auc = 0.73217\n",
      "epoch 0  | loss: 94648.92773| val_0_unsup_loss_numpy: 26948.091796875|  0:00:00s\n",
      "epoch 10 | loss: 66.31772| val_0_unsup_loss_numpy: 398369.0|  0:00:01s\n",
      "epoch 20 | loss: 58.88721| val_0_unsup_loss_numpy: 15123.94921875|  0:00:03s\n",
      "epoch 30 | loss: 253.05495| val_0_unsup_loss_numpy: 10266.33984375|  0:00:04s\n",
      "epoch 40 | loss: 15.11711| val_0_unsup_loss_numpy: 19079.060546875|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_unsup_loss_numpy = 202.38461303710938\n",
      "epoch 0  | loss: 0.66325 | valid_auc: 0.59274 |  0:00:00s\n",
      "epoch 10 | loss: 0.44115 | valid_auc: 0.64124 |  0:00:00s\n",
      "epoch 20 | loss: 0.43025 | valid_auc: 0.5524  |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 14:37:44,848]\u001b[0m Trial 12 finished with value: 0.7656666666666667 and parameters: {'n_d': 48, 'n_a': 46, 'n_step': 7, 'gamma': 1.555399472872045, 'mask_type': 'sparsemax'}. Best is trial 0 with value: 0.7656666666666667.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 | loss: 0.42158 | valid_auc: 0.6189  |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 12 and best_valid_auc = 0.67009\n",
      "epoch 0  | loss: 110376.18994| val_0_unsup_loss_numpy: 73749.171875|  0:00:00s\n",
      "epoch 10 | loss: 711.68524| val_0_unsup_loss_numpy: 1904.2930908203125|  0:00:01s\n",
      "epoch 20 | loss: 38.57199| val_0_unsup_loss_numpy: 564.6282958984375|  0:00:03s\n",
      "epoch 30 | loss: 33.08686| val_0_unsup_loss_numpy: 261.15325927734375|  0:00:05s\n",
      "epoch 40 | loss: 56.23543| val_0_unsup_loss_numpy: 73.16909790039062|  0:00:06s\n",
      "epoch 50 | loss: 17.6799 | val_0_unsup_loss_numpy: 213.44993591308594|  0:00:08s\n",
      "epoch 60 | loss: 63.28276| val_0_unsup_loss_numpy: 203.166259765625|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_unsup_loss_numpy = 21.714920043945312\n",
      "epoch 0  | loss: 0.65593 | valid_auc: 0.52988 |  0:00:00s\n",
      "epoch 10 | loss: 0.46077 | valid_auc: 0.48522 |  0:00:00s\n",
      "epoch 20 | loss: 0.44638 | valid_auc: 0.6017  |  0:00:01s\n",
      "epoch 30 | loss: 0.42583 | valid_auc: 0.68155 |  0:00:02s\n",
      "epoch 40 | loss: 0.41678 | valid_auc: 0.71286 |  0:00:03s\n",
      "epoch 50 | loss: 0.41089 | valid_auc: 0.73018 |  0:00:04s\n",
      "epoch 60 | loss: 0.40975 | valid_auc: 0.73012 |  0:00:05s\n",
      "epoch 70 | loss: 0.39049 | valid_auc: 0.74623 |  0:00:05s\n",
      "epoch 80 | loss: 0.38759 | valid_auc: 0.74778 |  0:00:06s\n",
      "epoch 90 | loss: 0.37898 | valid_auc: 0.73636 |  0:00:07s\n",
      "epoch 100| loss: 0.37382 | valid_auc: 0.74636 |  0:00:08s\n",
      "epoch 110| loss: 0.3718  | valid_auc: 0.74494 |  0:00:09s\n",
      "epoch 120| loss: 0.35987 | valid_auc: 0.75244 |  0:00:10s\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 102 and best_valid_auc = 0.75667\n",
      "epoch 0  | loss: 94370.03027| val_0_unsup_loss_numpy: 17508.9921875|  0:00:00s\n",
      "epoch 10 | loss: 50.14755| val_0_unsup_loss_numpy: 386726.625|  0:00:01s\n",
      "epoch 20 | loss: 26.16672| val_0_unsup_loss_numpy: 3018.63134765625|  0:00:03s\n",
      "epoch 30 | loss: 66.98974| val_0_unsup_loss_numpy: 114.78819274902344|  0:00:04s\n",
      "epoch 40 | loss: 21.48916| val_0_unsup_loss_numpy: 5724.16064453125|  0:00:06s\n",
      "epoch 50 | loss: 14.70267| val_0_unsup_loss_numpy: 458.03485107421875|  0:00:07s\n",
      "epoch 60 | loss: 5.83688 | val_0_unsup_loss_numpy: 18.403139114379883|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_unsup_loss_numpy = 4.336289882659912\n",
      "epoch 0  | loss: 0.61599 | valid_auc: 0.5856  |  0:00:00s\n",
      "epoch 10 | loss: 0.44422 | valid_auc: 0.68138 |  0:00:00s\n",
      "epoch 20 | loss: 0.43285 | valid_auc: 0.68199 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_valid_auc = 0.7296\n",
      "epoch 0  | loss: 172606.04688| val_0_unsup_loss_numpy: 443988.1875|  0:00:00s\n",
      "epoch 10 | loss: 302.13637| val_0_unsup_loss_numpy: 122103.421875|  0:00:01s\n",
      "epoch 20 | loss: 50.9832 | val_0_unsup_loss_numpy: 21448.150390625|  0:00:03s\n",
      "epoch 30 | loss: 101.98731| val_0_unsup_loss_numpy: 1743.913330078125|  0:00:04s\n",
      "epoch 40 | loss: 14.66358| val_0_unsup_loss_numpy: 7841.11767578125|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_unsup_loss_numpy = 1078.7030029296875\n",
      "epoch 0  | loss: 0.63946 | valid_auc: 0.49185 |  0:00:00s\n",
      "epoch 10 | loss: 0.45231 | valid_auc: 0.6158  |  0:00:00s\n",
      "epoch 20 | loss: 0.44445 | valid_auc: 0.66228 |  0:00:01s\n",
      "epoch 30 | loss: 0.42855 | valid_auc: 0.6914  |  0:00:02s\n",
      "epoch 40 | loss: 0.41515 | valid_auc: 0.71254 |  0:00:03s\n",
      "epoch 50 | loss: 0.40289 | valid_auc: 0.72965 |  0:00:04s\n",
      "epoch 60 | loss: 0.38184 | valid_auc: 0.7246  |  0:00:04s\n",
      "epoch 70 | loss: 0.3712  | valid_auc: 0.72264 |  0:00:05s\n",
      "epoch 80 | loss: 0.37229 | valid_auc: 0.69919 |  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 85 with best_epoch = 65 and best_valid_auc = 0.73217\n",
      "epoch 0  | loss: 94648.92773| val_0_unsup_loss_numpy: 26948.091796875|  0:00:00s\n",
      "epoch 10 | loss: 66.31772| val_0_unsup_loss_numpy: 398369.0|  0:00:01s\n",
      "epoch 20 | loss: 58.88721| val_0_unsup_loss_numpy: 15123.94921875|  0:00:03s\n",
      "epoch 30 | loss: 253.05495| val_0_unsup_loss_numpy: 10266.33984375|  0:00:04s\n",
      "epoch 40 | loss: 15.11711| val_0_unsup_loss_numpy: 19079.060546875|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_unsup_loss_numpy = 202.38461303710938\n",
      "epoch 0  | loss: 0.66325 | valid_auc: 0.59274 |  0:00:00s\n",
      "epoch 10 | loss: 0.44115 | valid_auc: 0.64124 |  0:00:00s\n",
      "epoch 20 | loss: 0.43025 | valid_auc: 0.5524  |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 14:38:40,678]\u001b[0m Trial 13 finished with value: 0.7656666666666667 and parameters: {'n_d': 64, 'n_a': 19, 'n_step': 7, 'gamma': 1.3631357691972947, 'mask_type': 'sparsemax'}. Best is trial 0 with value: 0.7656666666666667.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 | loss: 0.42158 | valid_auc: 0.6189  |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 12 and best_valid_auc = 0.67009\n",
      "epoch 0  | loss: 110376.18994| val_0_unsup_loss_numpy: 73749.171875|  0:00:00s\n",
      "epoch 10 | loss: 711.68524| val_0_unsup_loss_numpy: 1904.2930908203125|  0:00:01s\n",
      "epoch 20 | loss: 38.57199| val_0_unsup_loss_numpy: 564.6282958984375|  0:00:03s\n",
      "epoch 30 | loss: 33.08686| val_0_unsup_loss_numpy: 261.15325927734375|  0:00:04s\n",
      "epoch 40 | loss: 56.23543| val_0_unsup_loss_numpy: 73.16909790039062|  0:00:06s\n",
      "epoch 50 | loss: 17.6799 | val_0_unsup_loss_numpy: 213.44993591308594|  0:00:07s\n",
      "epoch 60 | loss: 63.28276| val_0_unsup_loss_numpy: 203.166259765625|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_unsup_loss_numpy = 21.714920043945312\n",
      "epoch 0  | loss: 0.65593 | valid_auc: 0.52988 |  0:00:00s\n",
      "epoch 10 | loss: 0.46077 | valid_auc: 0.48522 |  0:00:00s\n",
      "epoch 20 | loss: 0.44638 | valid_auc: 0.6017  |  0:00:01s\n",
      "epoch 30 | loss: 0.42583 | valid_auc: 0.68155 |  0:00:02s\n",
      "epoch 40 | loss: 0.41678 | valid_auc: 0.71286 |  0:00:03s\n",
      "epoch 50 | loss: 0.41089 | valid_auc: 0.73018 |  0:00:04s\n",
      "epoch 60 | loss: 0.40975 | valid_auc: 0.73012 |  0:00:04s\n",
      "epoch 70 | loss: 0.39049 | valid_auc: 0.74623 |  0:00:05s\n",
      "epoch 80 | loss: 0.38759 | valid_auc: 0.74778 |  0:00:06s\n",
      "epoch 90 | loss: 0.37898 | valid_auc: 0.73636 |  0:00:07s\n",
      "epoch 100| loss: 0.37382 | valid_auc: 0.74636 |  0:00:08s\n",
      "epoch 110| loss: 0.3718  | valid_auc: 0.74494 |  0:00:08s\n",
      "epoch 120| loss: 0.35987 | valid_auc: 0.75244 |  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 102 and best_valid_auc = 0.75667\n",
      "epoch 0  | loss: 94370.03027| val_0_unsup_loss_numpy: 17508.9921875|  0:00:00s\n",
      "epoch 10 | loss: 50.14755| val_0_unsup_loss_numpy: 386726.625|  0:00:01s\n",
      "epoch 20 | loss: 26.16672| val_0_unsup_loss_numpy: 3018.63134765625|  0:00:03s\n",
      "epoch 30 | loss: 66.98974| val_0_unsup_loss_numpy: 114.78819274902344|  0:00:04s\n",
      "epoch 40 | loss: 21.48916| val_0_unsup_loss_numpy: 5724.16064453125|  0:00:06s\n",
      "epoch 50 | loss: 14.70267| val_0_unsup_loss_numpy: 458.03485107421875|  0:00:07s\n",
      "epoch 60 | loss: 5.83688 | val_0_unsup_loss_numpy: 18.403139114379883|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_unsup_loss_numpy = 4.336289882659912\n",
      "epoch 0  | loss: 0.61599 | valid_auc: 0.5856  |  0:00:00s\n",
      "epoch 10 | loss: 0.44422 | valid_auc: 0.68138 |  0:00:00s\n",
      "epoch 20 | loss: 0.43285 | valid_auc: 0.68199 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_valid_auc = 0.7296\n",
      "epoch 0  | loss: 172606.04688| val_0_unsup_loss_numpy: 443988.1875|  0:00:00s\n",
      "epoch 10 | loss: 302.13637| val_0_unsup_loss_numpy: 122103.421875|  0:00:01s\n",
      "epoch 20 | loss: 50.9832 | val_0_unsup_loss_numpy: 21448.150390625|  0:00:03s\n",
      "epoch 30 | loss: 101.98731| val_0_unsup_loss_numpy: 1743.913330078125|  0:00:04s\n",
      "epoch 40 | loss: 14.66358| val_0_unsup_loss_numpy: 7841.11767578125|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_unsup_loss_numpy = 1078.7030029296875\n",
      "epoch 0  | loss: 0.63946 | valid_auc: 0.49185 |  0:00:00s\n",
      "epoch 10 | loss: 0.45231 | valid_auc: 0.6158  |  0:00:00s\n",
      "epoch 20 | loss: 0.44445 | valid_auc: 0.66228 |  0:00:01s\n",
      "epoch 30 | loss: 0.42855 | valid_auc: 0.6914  |  0:00:02s\n",
      "epoch 40 | loss: 0.41515 | valid_auc: 0.71254 |  0:00:03s\n",
      "epoch 50 | loss: 0.40289 | valid_auc: 0.72965 |  0:00:04s\n",
      "epoch 60 | loss: 0.38184 | valid_auc: 0.7246  |  0:00:04s\n",
      "epoch 70 | loss: 0.3712  | valid_auc: 0.72264 |  0:00:05s\n",
      "epoch 80 | loss: 0.37229 | valid_auc: 0.69919 |  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 85 with best_epoch = 65 and best_valid_auc = 0.73217\n",
      "epoch 0  | loss: 94648.92773| val_0_unsup_loss_numpy: 26948.091796875|  0:00:00s\n",
      "epoch 10 | loss: 66.31772| val_0_unsup_loss_numpy: 398369.0|  0:00:01s\n",
      "epoch 20 | loss: 58.88721| val_0_unsup_loss_numpy: 15123.94921875|  0:00:03s\n",
      "epoch 30 | loss: 253.05495| val_0_unsup_loss_numpy: 10266.33984375|  0:00:04s\n",
      "epoch 40 | loss: 15.11711| val_0_unsup_loss_numpy: 19079.060546875|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_unsup_loss_numpy = 202.38461303710938\n",
      "epoch 0  | loss: 0.66325 | valid_auc: 0.59274 |  0:00:00s\n",
      "epoch 10 | loss: 0.44115 | valid_auc: 0.64124 |  0:00:00s\n",
      "epoch 20 | loss: 0.43025 | valid_auc: 0.5524  |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 14:39:35,045]\u001b[0m Trial 14 finished with value: 0.7656666666666667 and parameters: {'n_d': 49, 'n_a': 59, 'n_step': 3, 'gamma': 1.6071903264160659, 'mask_type': 'entmatx'}. Best is trial 0 with value: 0.7656666666666667.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 | loss: 0.42158 | valid_auc: 0.6189  |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 12 and best_valid_auc = 0.67009\n",
      "epoch 0  | loss: 110376.18994| val_0_unsup_loss_numpy: 73749.171875|  0:00:00s\n",
      "epoch 10 | loss: 711.68524| val_0_unsup_loss_numpy: 1904.2930908203125|  0:00:01s\n",
      "epoch 20 | loss: 38.57199| val_0_unsup_loss_numpy: 564.6282958984375|  0:00:03s\n",
      "epoch 30 | loss: 33.08686| val_0_unsup_loss_numpy: 261.15325927734375|  0:00:04s\n",
      "epoch 40 | loss: 56.23543| val_0_unsup_loss_numpy: 73.16909790039062|  0:00:06s\n",
      "epoch 50 | loss: 17.6799 | val_0_unsup_loss_numpy: 213.44993591308594|  0:00:07s\n",
      "epoch 60 | loss: 63.28276| val_0_unsup_loss_numpy: 203.166259765625|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_unsup_loss_numpy = 21.714920043945312\n",
      "epoch 0  | loss: 0.65593 | valid_auc: 0.52988 |  0:00:00s\n",
      "epoch 10 | loss: 0.46077 | valid_auc: 0.48522 |  0:00:00s\n",
      "epoch 20 | loss: 0.44638 | valid_auc: 0.6017  |  0:00:01s\n",
      "epoch 30 | loss: 0.42583 | valid_auc: 0.68155 |  0:00:02s\n",
      "epoch 40 | loss: 0.41678 | valid_auc: 0.71286 |  0:00:03s\n",
      "epoch 50 | loss: 0.41089 | valid_auc: 0.73018 |  0:00:04s\n",
      "epoch 60 | loss: 0.40975 | valid_auc: 0.73012 |  0:00:04s\n",
      "epoch 70 | loss: 0.39049 | valid_auc: 0.74623 |  0:00:05s\n",
      "epoch 80 | loss: 0.38759 | valid_auc: 0.74778 |  0:00:06s\n",
      "epoch 90 | loss: 0.37898 | valid_auc: 0.73636 |  0:00:07s\n",
      "epoch 100| loss: 0.37382 | valid_auc: 0.74636 |  0:00:08s\n",
      "epoch 110| loss: 0.3718  | valid_auc: 0.74494 |  0:00:08s\n",
      "epoch 120| loss: 0.35987 | valid_auc: 0.75244 |  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 102 and best_valid_auc = 0.75667\n",
      "epoch 0  | loss: 94370.03027| val_0_unsup_loss_numpy: 17508.9921875|  0:00:00s\n",
      "epoch 10 | loss: 50.14755| val_0_unsup_loss_numpy: 386726.625|  0:00:01s\n",
      "epoch 20 | loss: 26.16672| val_0_unsup_loss_numpy: 3018.63134765625|  0:00:03s\n",
      "epoch 30 | loss: 66.98974| val_0_unsup_loss_numpy: 114.78819274902344|  0:00:04s\n",
      "epoch 40 | loss: 21.48916| val_0_unsup_loss_numpy: 5724.16064453125|  0:00:06s\n",
      "epoch 50 | loss: 14.70267| val_0_unsup_loss_numpy: 458.03485107421875|  0:00:07s\n",
      "epoch 60 | loss: 5.83688 | val_0_unsup_loss_numpy: 18.403139114379883|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_unsup_loss_numpy = 4.336289882659912\n",
      "epoch 0  | loss: 0.61599 | valid_auc: 0.5856  |  0:00:00s\n",
      "epoch 10 | loss: 0.44422 | valid_auc: 0.68138 |  0:00:00s\n",
      "epoch 20 | loss: 0.43285 | valid_auc: 0.68199 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_valid_auc = 0.7296\n",
      "epoch 0  | loss: 172606.04688| val_0_unsup_loss_numpy: 443988.1875|  0:00:00s\n",
      "epoch 10 | loss: 302.13637| val_0_unsup_loss_numpy: 122103.421875|  0:00:01s\n",
      "epoch 20 | loss: 50.9832 | val_0_unsup_loss_numpy: 21448.150390625|  0:00:03s\n",
      "epoch 30 | loss: 101.98731| val_0_unsup_loss_numpy: 1743.913330078125|  0:00:04s\n",
      "epoch 40 | loss: 14.66358| val_0_unsup_loss_numpy: 7841.11767578125|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_unsup_loss_numpy = 1078.7030029296875\n",
      "epoch 0  | loss: 0.63946 | valid_auc: 0.49185 |  0:00:00s\n",
      "epoch 10 | loss: 0.45231 | valid_auc: 0.6158  |  0:00:00s\n",
      "epoch 20 | loss: 0.44445 | valid_auc: 0.66228 |  0:00:01s\n",
      "epoch 30 | loss: 0.42855 | valid_auc: 0.6914  |  0:00:02s\n",
      "epoch 40 | loss: 0.41515 | valid_auc: 0.71254 |  0:00:03s\n",
      "epoch 50 | loss: 0.40289 | valid_auc: 0.72965 |  0:00:04s\n",
      "epoch 60 | loss: 0.38184 | valid_auc: 0.7246  |  0:00:04s\n",
      "epoch 70 | loss: 0.3712  | valid_auc: 0.72264 |  0:00:05s\n",
      "epoch 80 | loss: 0.37229 | valid_auc: 0.69919 |  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 85 with best_epoch = 65 and best_valid_auc = 0.73217\n",
      "epoch 0  | loss: 94648.92773| val_0_unsup_loss_numpy: 26948.091796875|  0:00:00s\n",
      "epoch 10 | loss: 66.31772| val_0_unsup_loss_numpy: 398369.0|  0:00:01s\n",
      "epoch 20 | loss: 58.88721| val_0_unsup_loss_numpy: 15123.94921875|  0:00:03s\n",
      "epoch 30 | loss: 253.05495| val_0_unsup_loss_numpy: 10266.33984375|  0:00:05s\n",
      "epoch 40 | loss: 15.11711| val_0_unsup_loss_numpy: 19079.060546875|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_unsup_loss_numpy = 202.38461303710938\n",
      "epoch 0  | loss: 0.66325 | valid_auc: 0.59274 |  0:00:00s\n",
      "epoch 10 | loss: 0.44115 | valid_auc: 0.64124 |  0:00:01s\n",
      "epoch 20 | loss: 0.43025 | valid_auc: 0.5524  |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 14:40:30,787]\u001b[0m Trial 15 finished with value: 0.7656666666666667 and parameters: {'n_d': 56, 'n_a': 19, 'n_step': 1, 'gamma': 1.4511497032783554, 'mask_type': 'sparsemax'}. Best is trial 0 with value: 0.7656666666666667.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 | loss: 0.42158 | valid_auc: 0.6189  |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 12 and best_valid_auc = 0.67009\n",
      "epoch 0  | loss: 110376.18994| val_0_unsup_loss_numpy: 73749.171875|  0:00:00s\n",
      "epoch 10 | loss: 711.68524| val_0_unsup_loss_numpy: 1904.2930908203125|  0:00:01s\n",
      "epoch 20 | loss: 38.57199| val_0_unsup_loss_numpy: 564.6282958984375|  0:00:03s\n",
      "epoch 30 | loss: 33.08686| val_0_unsup_loss_numpy: 261.15325927734375|  0:00:04s\n",
      "epoch 40 | loss: 56.23543| val_0_unsup_loss_numpy: 73.16909790039062|  0:00:06s\n",
      "epoch 50 | loss: 17.6799 | val_0_unsup_loss_numpy: 213.44993591308594|  0:00:07s\n",
      "epoch 60 | loss: 63.28276| val_0_unsup_loss_numpy: 203.166259765625|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_unsup_loss_numpy = 21.714920043945312\n",
      "epoch 0  | loss: 0.65593 | valid_auc: 0.52988 |  0:00:00s\n",
      "epoch 10 | loss: 0.46077 | valid_auc: 0.48522 |  0:00:00s\n",
      "epoch 20 | loss: 0.44638 | valid_auc: 0.6017  |  0:00:01s\n",
      "epoch 30 | loss: 0.42583 | valid_auc: 0.68155 |  0:00:02s\n",
      "epoch 40 | loss: 0.41678 | valid_auc: 0.71286 |  0:00:03s\n",
      "epoch 50 | loss: 0.41089 | valid_auc: 0.73018 |  0:00:04s\n",
      "epoch 60 | loss: 0.40975 | valid_auc: 0.73012 |  0:00:05s\n",
      "epoch 70 | loss: 0.39049 | valid_auc: 0.74623 |  0:00:05s\n",
      "epoch 80 | loss: 0.38759 | valid_auc: 0.74778 |  0:00:06s\n",
      "epoch 90 | loss: 0.37898 | valid_auc: 0.73636 |  0:00:07s\n",
      "epoch 100| loss: 0.37382 | valid_auc: 0.74636 |  0:00:08s\n",
      "epoch 110| loss: 0.3718  | valid_auc: 0.74494 |  0:00:09s\n",
      "epoch 120| loss: 0.35987 | valid_auc: 0.75244 |  0:00:10s\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 102 and best_valid_auc = 0.75667\n",
      "epoch 0  | loss: 94370.03027| val_0_unsup_loss_numpy: 17508.9921875|  0:00:00s\n",
      "epoch 10 | loss: 50.14755| val_0_unsup_loss_numpy: 386726.625|  0:00:01s\n",
      "epoch 20 | loss: 26.16672| val_0_unsup_loss_numpy: 3018.63134765625|  0:00:02s\n",
      "epoch 30 | loss: 66.98974| val_0_unsup_loss_numpy: 114.78819274902344|  0:00:04s\n",
      "epoch 40 | loss: 21.48916| val_0_unsup_loss_numpy: 5724.16064453125|  0:00:06s\n",
      "epoch 50 | loss: 14.70267| val_0_unsup_loss_numpy: 458.03485107421875|  0:00:07s\n",
      "epoch 60 | loss: 5.83688 | val_0_unsup_loss_numpy: 18.403139114379883|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_unsup_loss_numpy = 4.336289882659912\n",
      "epoch 0  | loss: 0.61599 | valid_auc: 0.5856  |  0:00:00s\n",
      "epoch 10 | loss: 0.44422 | valid_auc: 0.68138 |  0:00:00s\n",
      "epoch 20 | loss: 0.43285 | valid_auc: 0.68199 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_valid_auc = 0.7296\n",
      "epoch 0  | loss: 172606.04688| val_0_unsup_loss_numpy: 443988.1875|  0:00:00s\n",
      "epoch 10 | loss: 302.13637| val_0_unsup_loss_numpy: 122103.421875|  0:00:01s\n",
      "epoch 20 | loss: 50.9832 | val_0_unsup_loss_numpy: 21448.150390625|  0:00:03s\n",
      "epoch 30 | loss: 101.98731| val_0_unsup_loss_numpy: 1743.913330078125|  0:00:04s\n",
      "epoch 40 | loss: 14.66358| val_0_unsup_loss_numpy: 7841.11767578125|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_unsup_loss_numpy = 1078.7030029296875\n",
      "epoch 0  | loss: 0.63946 | valid_auc: 0.49185 |  0:00:00s\n",
      "epoch 10 | loss: 0.45231 | valid_auc: 0.6158  |  0:00:00s\n",
      "epoch 20 | loss: 0.44445 | valid_auc: 0.66228 |  0:00:01s\n",
      "epoch 30 | loss: 0.42855 | valid_auc: 0.6914  |  0:00:02s\n",
      "epoch 40 | loss: 0.41515 | valid_auc: 0.71254 |  0:00:03s\n",
      "epoch 50 | loss: 0.40289 | valid_auc: 0.72965 |  0:00:04s\n",
      "epoch 60 | loss: 0.38184 | valid_auc: 0.7246  |  0:00:05s\n",
      "epoch 70 | loss: 0.3712  | valid_auc: 0.72264 |  0:00:05s\n",
      "epoch 80 | loss: 0.37229 | valid_auc: 0.69919 |  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 85 with best_epoch = 65 and best_valid_auc = 0.73217\n",
      "epoch 0  | loss: 94648.92773| val_0_unsup_loss_numpy: 26948.091796875|  0:00:00s\n",
      "epoch 10 | loss: 66.31772| val_0_unsup_loss_numpy: 398369.0|  0:00:01s\n",
      "epoch 20 | loss: 58.88721| val_0_unsup_loss_numpy: 15123.94921875|  0:00:03s\n",
      "epoch 30 | loss: 253.05495| val_0_unsup_loss_numpy: 10266.33984375|  0:00:04s\n",
      "epoch 40 | loss: 15.11711| val_0_unsup_loss_numpy: 19079.060546875|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_unsup_loss_numpy = 202.38461303710938\n",
      "epoch 0  | loss: 0.66325 | valid_auc: 0.59274 |  0:00:00s\n",
      "epoch 10 | loss: 0.44115 | valid_auc: 0.64124 |  0:00:00s\n",
      "epoch 20 | loss: 0.43025 | valid_auc: 0.5524  |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 14:41:27,051]\u001b[0m Trial 16 finished with value: 0.7656666666666667 and parameters: {'n_d': 44, 'n_a': 49, 'n_step': 6, 'gamma': 1.3844748717398336, 'mask_type': 'entmatx'}. Best is trial 0 with value: 0.7656666666666667.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 | loss: 0.42158 | valid_auc: 0.6189  |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 12 and best_valid_auc = 0.67009\n",
      "epoch 0  | loss: 110376.18994| val_0_unsup_loss_numpy: 73749.171875|  0:00:00s\n",
      "epoch 10 | loss: 711.68524| val_0_unsup_loss_numpy: 1904.2930908203125|  0:00:01s\n",
      "epoch 20 | loss: 38.57199| val_0_unsup_loss_numpy: 564.6282958984375|  0:00:03s\n",
      "epoch 30 | loss: 33.08686| val_0_unsup_loss_numpy: 261.15325927734375|  0:00:04s\n",
      "epoch 40 | loss: 56.23543| val_0_unsup_loss_numpy: 73.16909790039062|  0:00:06s\n",
      "epoch 50 | loss: 17.6799 | val_0_unsup_loss_numpy: 213.44993591308594|  0:00:07s\n",
      "epoch 60 | loss: 63.28276| val_0_unsup_loss_numpy: 203.166259765625|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_unsup_loss_numpy = 21.714920043945312\n",
      "epoch 0  | loss: 0.65593 | valid_auc: 0.52988 |  0:00:00s\n",
      "epoch 10 | loss: 0.46077 | valid_auc: 0.48522 |  0:00:00s\n",
      "epoch 20 | loss: 0.44638 | valid_auc: 0.6017  |  0:00:01s\n",
      "epoch 30 | loss: 0.42583 | valid_auc: 0.68155 |  0:00:02s\n",
      "epoch 40 | loss: 0.41678 | valid_auc: 0.71286 |  0:00:03s\n",
      "epoch 50 | loss: 0.41089 | valid_auc: 0.73018 |  0:00:04s\n",
      "epoch 60 | loss: 0.40975 | valid_auc: 0.73012 |  0:00:05s\n",
      "epoch 70 | loss: 0.39049 | valid_auc: 0.74623 |  0:00:05s\n",
      "epoch 80 | loss: 0.38759 | valid_auc: 0.74778 |  0:00:06s\n",
      "epoch 90 | loss: 0.37898 | valid_auc: 0.73636 |  0:00:07s\n",
      "epoch 100| loss: 0.37382 | valid_auc: 0.74636 |  0:00:08s\n",
      "epoch 110| loss: 0.3718  | valid_auc: 0.74494 |  0:00:08s\n",
      "epoch 120| loss: 0.35987 | valid_auc: 0.75244 |  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 102 and best_valid_auc = 0.75667\n",
      "epoch 0  | loss: 94370.03027| val_0_unsup_loss_numpy: 17508.9921875|  0:00:00s\n",
      "epoch 10 | loss: 50.14755| val_0_unsup_loss_numpy: 386726.625|  0:00:01s\n",
      "epoch 20 | loss: 26.16672| val_0_unsup_loss_numpy: 3018.63134765625|  0:00:03s\n",
      "epoch 30 | loss: 66.98974| val_0_unsup_loss_numpy: 114.78819274902344|  0:00:04s\n",
      "epoch 40 | loss: 21.48916| val_0_unsup_loss_numpy: 5724.16064453125|  0:00:06s\n",
      "epoch 50 | loss: 14.70267| val_0_unsup_loss_numpy: 458.03485107421875|  0:00:07s\n",
      "epoch 60 | loss: 5.83688 | val_0_unsup_loss_numpy: 18.403139114379883|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_unsup_loss_numpy = 4.336289882659912\n",
      "epoch 0  | loss: 0.61599 | valid_auc: 0.5856  |  0:00:00s\n",
      "epoch 10 | loss: 0.44422 | valid_auc: 0.68138 |  0:00:00s\n",
      "epoch 20 | loss: 0.43285 | valid_auc: 0.68199 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_valid_auc = 0.7296\n",
      "epoch 0  | loss: 172606.04688| val_0_unsup_loss_numpy: 443988.1875|  0:00:00s\n",
      "epoch 10 | loss: 302.13637| val_0_unsup_loss_numpy: 122103.421875|  0:00:01s\n",
      "epoch 20 | loss: 50.9832 | val_0_unsup_loss_numpy: 21448.150390625|  0:00:03s\n",
      "epoch 30 | loss: 101.98731| val_0_unsup_loss_numpy: 1743.913330078125|  0:00:04s\n",
      "epoch 40 | loss: 14.66358| val_0_unsup_loss_numpy: 7841.11767578125|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_unsup_loss_numpy = 1078.7030029296875\n",
      "epoch 0  | loss: 0.63946 | valid_auc: 0.49185 |  0:00:00s\n",
      "epoch 10 | loss: 0.45231 | valid_auc: 0.6158  |  0:00:01s\n",
      "epoch 20 | loss: 0.44445 | valid_auc: 0.66228 |  0:00:01s\n",
      "epoch 30 | loss: 0.42855 | valid_auc: 0.6914  |  0:00:02s\n",
      "epoch 40 | loss: 0.41515 | valid_auc: 0.71254 |  0:00:03s\n",
      "epoch 50 | loss: 0.40289 | valid_auc: 0.72965 |  0:00:04s\n",
      "epoch 60 | loss: 0.38184 | valid_auc: 0.7246  |  0:00:05s\n",
      "epoch 70 | loss: 0.3712  | valid_auc: 0.72264 |  0:00:05s\n",
      "epoch 80 | loss: 0.37229 | valid_auc: 0.69919 |  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 85 with best_epoch = 65 and best_valid_auc = 0.73217\n",
      "epoch 0  | loss: 94648.92773| val_0_unsup_loss_numpy: 26948.091796875|  0:00:00s\n",
      "epoch 10 | loss: 66.31772| val_0_unsup_loss_numpy: 398369.0|  0:00:01s\n",
      "epoch 20 | loss: 58.88721| val_0_unsup_loss_numpy: 15123.94921875|  0:00:03s\n",
      "epoch 30 | loss: 253.05495| val_0_unsup_loss_numpy: 10266.33984375|  0:00:04s\n",
      "epoch 40 | loss: 15.11711| val_0_unsup_loss_numpy: 19079.060546875|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_unsup_loss_numpy = 202.38461303710938\n",
      "epoch 0  | loss: 0.66325 | valid_auc: 0.59274 |  0:00:00s\n",
      "epoch 10 | loss: 0.44115 | valid_auc: 0.64124 |  0:00:00s\n",
      "epoch 20 | loss: 0.43025 | valid_auc: 0.5524  |  0:00:01s\n",
      "epoch 30 | loss: 0.42158 | valid_auc: 0.6189  |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 12 and best_valid_auc = 0.67009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 14:42:21,674]\u001b[0m Trial 17 finished with value: 0.7656666666666667 and parameters: {'n_d': 57, 'n_a': 38, 'n_step': 3, 'gamma': 1.6228321701158495, 'mask_type': 'sparsemax'}. Best is trial 0 with value: 0.7656666666666667.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 110376.18994| val_0_unsup_loss_numpy: 73749.171875|  0:00:00s\n",
      "epoch 10 | loss: 711.68524| val_0_unsup_loss_numpy: 1904.2930908203125|  0:00:01s\n",
      "epoch 20 | loss: 38.57199| val_0_unsup_loss_numpy: 564.6282958984375|  0:00:03s\n",
      "epoch 30 | loss: 33.08686| val_0_unsup_loss_numpy: 261.15325927734375|  0:00:05s\n",
      "epoch 40 | loss: 56.23543| val_0_unsup_loss_numpy: 73.16909790039062|  0:00:06s\n",
      "epoch 50 | loss: 17.6799 | val_0_unsup_loss_numpy: 213.44993591308594|  0:00:08s\n",
      "epoch 60 | loss: 63.28276| val_0_unsup_loss_numpy: 203.166259765625|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_unsup_loss_numpy = 21.714920043945312\n",
      "epoch 0  | loss: 0.65593 | valid_auc: 0.52988 |  0:00:00s\n",
      "epoch 10 | loss: 0.46077 | valid_auc: 0.48522 |  0:00:00s\n",
      "epoch 20 | loss: 0.44638 | valid_auc: 0.6017  |  0:00:01s\n",
      "epoch 30 | loss: 0.42583 | valid_auc: 0.68155 |  0:00:02s\n",
      "epoch 40 | loss: 0.41678 | valid_auc: 0.71286 |  0:00:03s\n",
      "epoch 50 | loss: 0.41089 | valid_auc: 0.73018 |  0:00:04s\n",
      "epoch 60 | loss: 0.40975 | valid_auc: 0.73012 |  0:00:05s\n",
      "epoch 70 | loss: 0.39049 | valid_auc: 0.74623 |  0:00:05s\n",
      "epoch 80 | loss: 0.38759 | valid_auc: 0.74778 |  0:00:06s\n",
      "epoch 90 | loss: 0.37898 | valid_auc: 0.73636 |  0:00:07s\n",
      "epoch 100| loss: 0.37382 | valid_auc: 0.74636 |  0:00:08s\n",
      "epoch 110| loss: 0.3718  | valid_auc: 0.74494 |  0:00:08s\n",
      "epoch 120| loss: 0.35987 | valid_auc: 0.75244 |  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 102 and best_valid_auc = 0.75667\n",
      "epoch 0  | loss: 94370.03027| val_0_unsup_loss_numpy: 17508.9921875|  0:00:00s\n",
      "epoch 10 | loss: 50.14755| val_0_unsup_loss_numpy: 386726.625|  0:00:01s\n",
      "epoch 20 | loss: 26.16672| val_0_unsup_loss_numpy: 3018.63134765625|  0:00:03s\n",
      "epoch 30 | loss: 66.98974| val_0_unsup_loss_numpy: 114.78819274902344|  0:00:04s\n",
      "epoch 40 | loss: 21.48916| val_0_unsup_loss_numpy: 5724.16064453125|  0:00:06s\n",
      "epoch 50 | loss: 14.70267| val_0_unsup_loss_numpy: 458.03485107421875|  0:00:07s\n",
      "epoch 60 | loss: 5.83688 | val_0_unsup_loss_numpy: 18.403139114379883|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_unsup_loss_numpy = 4.336289882659912\n",
      "epoch 0  | loss: 0.61599 | valid_auc: 0.5856  |  0:00:00s\n",
      "epoch 10 | loss: 0.44422 | valid_auc: 0.68138 |  0:00:00s\n",
      "epoch 20 | loss: 0.43285 | valid_auc: 0.68199 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_valid_auc = 0.7296\n",
      "epoch 0  | loss: 172606.04688| val_0_unsup_loss_numpy: 443988.1875|  0:00:00s\n",
      "epoch 10 | loss: 302.13637| val_0_unsup_loss_numpy: 122103.421875|  0:00:01s\n",
      "epoch 20 | loss: 50.9832 | val_0_unsup_loss_numpy: 21448.150390625|  0:00:03s\n",
      "epoch 30 | loss: 101.98731| val_0_unsup_loss_numpy: 1743.913330078125|  0:00:04s\n",
      "epoch 40 | loss: 14.66358| val_0_unsup_loss_numpy: 7841.11767578125|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_unsup_loss_numpy = 1078.7030029296875\n",
      "epoch 0  | loss: 0.63946 | valid_auc: 0.49185 |  0:00:00s\n",
      "epoch 10 | loss: 0.45231 | valid_auc: 0.6158  |  0:00:00s\n",
      "epoch 20 | loss: 0.44445 | valid_auc: 0.66228 |  0:00:01s\n",
      "epoch 30 | loss: 0.42855 | valid_auc: 0.6914  |  0:00:02s\n",
      "epoch 40 | loss: 0.41515 | valid_auc: 0.71254 |  0:00:03s\n",
      "epoch 50 | loss: 0.40289 | valid_auc: 0.72965 |  0:00:04s\n",
      "epoch 60 | loss: 0.38184 | valid_auc: 0.7246  |  0:00:05s\n",
      "epoch 70 | loss: 0.3712  | valid_auc: 0.72264 |  0:00:06s\n",
      "epoch 80 | loss: 0.37229 | valid_auc: 0.69919 |  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 85 with best_epoch = 65 and best_valid_auc = 0.73217\n",
      "epoch 0  | loss: 94648.92773| val_0_unsup_loss_numpy: 26948.091796875|  0:00:00s\n",
      "epoch 10 | loss: 66.31772| val_0_unsup_loss_numpy: 398369.0|  0:00:01s\n",
      "epoch 20 | loss: 58.88721| val_0_unsup_loss_numpy: 15123.94921875|  0:00:03s\n",
      "epoch 30 | loss: 253.05495| val_0_unsup_loss_numpy: 10266.33984375|  0:00:04s\n",
      "epoch 40 | loss: 15.11711| val_0_unsup_loss_numpy: 19079.060546875|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_unsup_loss_numpy = 202.38461303710938\n",
      "epoch 0  | loss: 0.66325 | valid_auc: 0.59274 |  0:00:00s\n",
      "epoch 10 | loss: 0.44115 | valid_auc: 0.64124 |  0:00:00s\n",
      "epoch 20 | loss: 0.43025 | valid_auc: 0.5524  |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 14:43:17,846]\u001b[0m Trial 18 finished with value: 0.7656666666666667 and parameters: {'n_d': 13, 'n_a': 54, 'n_step': 6, 'gamma': 1.3222958430513911, 'mask_type': 'entmatx'}. Best is trial 0 with value: 0.7656666666666667.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 | loss: 0.42158 | valid_auc: 0.6189  |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 12 and best_valid_auc = 0.67009\n",
      "epoch 0  | loss: 110376.18994| val_0_unsup_loss_numpy: 73749.171875|  0:00:00s\n",
      "epoch 10 | loss: 711.68524| val_0_unsup_loss_numpy: 1904.2930908203125|  0:00:01s\n",
      "epoch 20 | loss: 38.57199| val_0_unsup_loss_numpy: 564.6282958984375|  0:00:03s\n",
      "epoch 30 | loss: 33.08686| val_0_unsup_loss_numpy: 261.15325927734375|  0:00:04s\n",
      "epoch 40 | loss: 56.23543| val_0_unsup_loss_numpy: 73.16909790039062|  0:00:06s\n",
      "epoch 50 | loss: 17.6799 | val_0_unsup_loss_numpy: 213.44993591308594|  0:00:07s\n",
      "epoch 60 | loss: 63.28276| val_0_unsup_loss_numpy: 203.166259765625|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_unsup_loss_numpy = 21.714920043945312\n",
      "epoch 0  | loss: 0.65593 | valid_auc: 0.52988 |  0:00:00s\n",
      "epoch 10 | loss: 0.46077 | valid_auc: 0.48522 |  0:00:00s\n",
      "epoch 20 | loss: 0.44638 | valid_auc: 0.6017  |  0:00:01s\n",
      "epoch 30 | loss: 0.42583 | valid_auc: 0.68155 |  0:00:02s\n",
      "epoch 40 | loss: 0.41678 | valid_auc: 0.71286 |  0:00:03s\n",
      "epoch 50 | loss: 0.41089 | valid_auc: 0.73018 |  0:00:04s\n",
      "epoch 60 | loss: 0.40975 | valid_auc: 0.73012 |  0:00:05s\n",
      "epoch 70 | loss: 0.39049 | valid_auc: 0.74623 |  0:00:05s\n",
      "epoch 80 | loss: 0.38759 | valid_auc: 0.74778 |  0:00:06s\n",
      "epoch 90 | loss: 0.37898 | valid_auc: 0.73636 |  0:00:07s\n",
      "epoch 100| loss: 0.37382 | valid_auc: 0.74636 |  0:00:08s\n",
      "epoch 110| loss: 0.3718  | valid_auc: 0.74494 |  0:00:08s\n",
      "epoch 120| loss: 0.35987 | valid_auc: 0.75244 |  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 102 and best_valid_auc = 0.75667\n",
      "epoch 0  | loss: 94370.03027| val_0_unsup_loss_numpy: 17508.9921875|  0:00:00s\n",
      "epoch 10 | loss: 50.14755| val_0_unsup_loss_numpy: 386726.625|  0:00:01s\n",
      "epoch 20 | loss: 26.16672| val_0_unsup_loss_numpy: 3018.63134765625|  0:00:03s\n",
      "epoch 30 | loss: 66.98974| val_0_unsup_loss_numpy: 114.78819274902344|  0:00:04s\n",
      "epoch 40 | loss: 21.48916| val_0_unsup_loss_numpy: 5724.16064453125|  0:00:06s\n",
      "epoch 50 | loss: 14.70267| val_0_unsup_loss_numpy: 458.03485107421875|  0:00:07s\n",
      "epoch 60 | loss: 5.83688 | val_0_unsup_loss_numpy: 18.403139114379883|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_unsup_loss_numpy = 4.336289882659912\n",
      "epoch 0  | loss: 0.61599 | valid_auc: 0.5856  |  0:00:00s\n",
      "epoch 10 | loss: 0.44422 | valid_auc: 0.68138 |  0:00:00s\n",
      "epoch 20 | loss: 0.43285 | valid_auc: 0.68199 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_valid_auc = 0.7296\n",
      "epoch 0  | loss: 172606.04688| val_0_unsup_loss_numpy: 443988.1875|  0:00:00s\n",
      "epoch 10 | loss: 302.13637| val_0_unsup_loss_numpy: 122103.421875|  0:00:01s\n",
      "epoch 20 | loss: 50.9832 | val_0_unsup_loss_numpy: 21448.150390625|  0:00:03s\n",
      "epoch 30 | loss: 101.98731| val_0_unsup_loss_numpy: 1743.913330078125|  0:00:04s\n",
      "epoch 40 | loss: 14.66358| val_0_unsup_loss_numpy: 7841.11767578125|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_unsup_loss_numpy = 1078.7030029296875\n",
      "epoch 0  | loss: 0.63946 | valid_auc: 0.49185 |  0:00:00s\n",
      "epoch 10 | loss: 0.45231 | valid_auc: 0.6158  |  0:00:00s\n",
      "epoch 20 | loss: 0.44445 | valid_auc: 0.66228 |  0:00:01s\n",
      "epoch 30 | loss: 0.42855 | valid_auc: 0.6914  |  0:00:02s\n",
      "epoch 40 | loss: 0.41515 | valid_auc: 0.71254 |  0:00:03s\n",
      "epoch 50 | loss: 0.40289 | valid_auc: 0.72965 |  0:00:04s\n",
      "epoch 60 | loss: 0.38184 | valid_auc: 0.7246  |  0:00:04s\n",
      "epoch 70 | loss: 0.3712  | valid_auc: 0.72264 |  0:00:05s\n",
      "epoch 80 | loss: 0.37229 | valid_auc: 0.69919 |  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 85 with best_epoch = 65 and best_valid_auc = 0.73217\n",
      "epoch 0  | loss: 94648.92773| val_0_unsup_loss_numpy: 26948.091796875|  0:00:00s\n",
      "epoch 10 | loss: 66.31772| val_0_unsup_loss_numpy: 398369.0|  0:00:01s\n",
      "epoch 20 | loss: 58.88721| val_0_unsup_loss_numpy: 15123.94921875|  0:00:03s\n",
      "epoch 30 | loss: 253.05495| val_0_unsup_loss_numpy: 10266.33984375|  0:00:04s\n",
      "epoch 40 | loss: 15.11711| val_0_unsup_loss_numpy: 19079.060546875|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_unsup_loss_numpy = 202.38461303710938\n",
      "epoch 0  | loss: 0.66325 | valid_auc: 0.59274 |  0:00:00s\n",
      "epoch 10 | loss: 0.44115 | valid_auc: 0.64124 |  0:00:00s\n",
      "epoch 20 | loss: 0.43025 | valid_auc: 0.5524  |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 14:44:12,759]\u001b[0m Trial 19 finished with value: 0.7656666666666667 and parameters: {'n_d': 43, 'n_a': 62, 'n_step': 8, 'gamma': 1.5520330411371615, 'mask_type': 'sparsemax'}. Best is trial 0 with value: 0.7656666666666667.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 | loss: 0.42158 | valid_auc: 0.6189  |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 12 and best_valid_auc = 0.67009\n",
      "epoch 0  | loss: 110376.18994| val_0_unsup_loss_numpy: 73749.171875|  0:00:00s\n",
      "epoch 10 | loss: 711.68524| val_0_unsup_loss_numpy: 1904.2930908203125|  0:00:01s\n",
      "epoch 20 | loss: 38.57199| val_0_unsup_loss_numpy: 564.6282958984375|  0:00:03s\n",
      "epoch 30 | loss: 33.08686| val_0_unsup_loss_numpy: 261.15325927734375|  0:00:04s\n",
      "epoch 40 | loss: 56.23543| val_0_unsup_loss_numpy: 73.16909790039062|  0:00:06s\n",
      "epoch 50 | loss: 17.6799 | val_0_unsup_loss_numpy: 213.44993591308594|  0:00:07s\n",
      "epoch 60 | loss: 63.28276| val_0_unsup_loss_numpy: 203.166259765625|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_unsup_loss_numpy = 21.714920043945312\n",
      "epoch 0  | loss: 0.65593 | valid_auc: 0.52988 |  0:00:00s\n",
      "epoch 10 | loss: 0.46077 | valid_auc: 0.48522 |  0:00:00s\n",
      "epoch 20 | loss: 0.44638 | valid_auc: 0.6017  |  0:00:01s\n",
      "epoch 30 | loss: 0.42583 | valid_auc: 0.68155 |  0:00:02s\n",
      "epoch 40 | loss: 0.41678 | valid_auc: 0.71286 |  0:00:03s\n",
      "epoch 50 | loss: 0.41089 | valid_auc: 0.73018 |  0:00:04s\n",
      "epoch 60 | loss: 0.40975 | valid_auc: 0.73012 |  0:00:05s\n",
      "epoch 70 | loss: 0.39049 | valid_auc: 0.74623 |  0:00:05s\n",
      "epoch 80 | loss: 0.38759 | valid_auc: 0.74778 |  0:00:06s\n",
      "epoch 90 | loss: 0.37898 | valid_auc: 0.73636 |  0:00:07s\n",
      "epoch 100| loss: 0.37382 | valid_auc: 0.74636 |  0:00:08s\n",
      "epoch 110| loss: 0.3718  | valid_auc: 0.74494 |  0:00:09s\n",
      "epoch 120| loss: 0.35987 | valid_auc: 0.75244 |  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 102 and best_valid_auc = 0.75667\n",
      "epoch 0  | loss: 94370.03027| val_0_unsup_loss_numpy: 17508.9921875|  0:00:00s\n",
      "epoch 10 | loss: 50.14755| val_0_unsup_loss_numpy: 386726.625|  0:00:01s\n",
      "epoch 20 | loss: 26.16672| val_0_unsup_loss_numpy: 3018.63134765625|  0:00:03s\n",
      "epoch 30 | loss: 66.98974| val_0_unsup_loss_numpy: 114.78819274902344|  0:00:04s\n",
      "epoch 40 | loss: 21.48916| val_0_unsup_loss_numpy: 5724.16064453125|  0:00:06s\n",
      "epoch 50 | loss: 14.70267| val_0_unsup_loss_numpy: 458.03485107421875|  0:00:07s\n",
      "epoch 60 | loss: 5.83688 | val_0_unsup_loss_numpy: 18.403139114379883|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_unsup_loss_numpy = 4.336289882659912\n",
      "epoch 0  | loss: 0.61599 | valid_auc: 0.5856  |  0:00:00s\n",
      "epoch 10 | loss: 0.44422 | valid_auc: 0.68138 |  0:00:00s\n",
      "epoch 20 | loss: 0.43285 | valid_auc: 0.68199 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_valid_auc = 0.7296\n",
      "epoch 0  | loss: 172606.04688| val_0_unsup_loss_numpy: 443988.1875|  0:00:00s\n",
      "epoch 10 | loss: 302.13637| val_0_unsup_loss_numpy: 122103.421875|  0:00:01s\n",
      "epoch 20 | loss: 50.9832 | val_0_unsup_loss_numpy: 21448.150390625|  0:00:03s\n",
      "epoch 30 | loss: 101.98731| val_0_unsup_loss_numpy: 1743.913330078125|  0:00:04s\n",
      "epoch 40 | loss: 14.66358| val_0_unsup_loss_numpy: 7841.11767578125|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_unsup_loss_numpy = 1078.7030029296875\n",
      "epoch 0  | loss: 0.63946 | valid_auc: 0.49185 |  0:00:00s\n",
      "epoch 10 | loss: 0.45231 | valid_auc: 0.6158  |  0:00:00s\n",
      "epoch 20 | loss: 0.44445 | valid_auc: 0.66228 |  0:00:01s\n",
      "epoch 30 | loss: 0.42855 | valid_auc: 0.6914  |  0:00:02s\n",
      "epoch 40 | loss: 0.41515 | valid_auc: 0.71254 |  0:00:03s\n",
      "epoch 50 | loss: 0.40289 | valid_auc: 0.72965 |  0:00:04s\n",
      "epoch 60 | loss: 0.38184 | valid_auc: 0.7246  |  0:00:04s\n",
      "epoch 70 | loss: 0.3712  | valid_auc: 0.72264 |  0:00:05s\n",
      "epoch 80 | loss: 0.37229 | valid_auc: 0.69919 |  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 85 with best_epoch = 65 and best_valid_auc = 0.73217\n",
      "epoch 0  | loss: 94648.92773| val_0_unsup_loss_numpy: 26948.091796875|  0:00:00s\n",
      "epoch 10 | loss: 66.31772| val_0_unsup_loss_numpy: 398369.0|  0:00:01s\n",
      "epoch 20 | loss: 58.88721| val_0_unsup_loss_numpy: 15123.94921875|  0:00:03s\n",
      "epoch 30 | loss: 253.05495| val_0_unsup_loss_numpy: 10266.33984375|  0:00:04s\n",
      "epoch 40 | loss: 15.11711| val_0_unsup_loss_numpy: 19079.060546875|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_unsup_loss_numpy = 202.38461303710938\n",
      "epoch 0  | loss: 0.66325 | valid_auc: 0.59274 |  0:00:00s\n",
      "epoch 10 | loss: 0.44115 | valid_auc: 0.64124 |  0:00:00s\n",
      "epoch 20 | loss: 0.43025 | valid_auc: 0.5524  |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 14:45:08,441]\u001b[0m Trial 20 finished with value: 0.7656666666666667 and parameters: {'n_d': 59, 'n_a': 15, 'n_step': 4, 'gamma': 1.4356061202518677, 'mask_type': 'sparsemax'}. Best is trial 0 with value: 0.7656666666666667.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 | loss: 0.42158 | valid_auc: 0.6189  |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 12 and best_valid_auc = 0.67009\n",
      "epoch 0  | loss: 110376.18994| val_0_unsup_loss_numpy: 73749.171875|  0:00:00s\n",
      "epoch 10 | loss: 711.68524| val_0_unsup_loss_numpy: 1904.2930908203125|  0:00:01s\n",
      "epoch 20 | loss: 38.57199| val_0_unsup_loss_numpy: 564.6282958984375|  0:00:03s\n",
      "epoch 30 | loss: 33.08686| val_0_unsup_loss_numpy: 261.15325927734375|  0:00:04s\n",
      "epoch 40 | loss: 56.23543| val_0_unsup_loss_numpy: 73.16909790039062|  0:00:06s\n",
      "epoch 50 | loss: 17.6799 | val_0_unsup_loss_numpy: 213.44993591308594|  0:00:07s\n",
      "epoch 60 | loss: 63.28276| val_0_unsup_loss_numpy: 203.166259765625|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_unsup_loss_numpy = 21.714920043945312\n",
      "epoch 0  | loss: 0.65593 | valid_auc: 0.52988 |  0:00:00s\n",
      "epoch 10 | loss: 0.46077 | valid_auc: 0.48522 |  0:00:00s\n",
      "epoch 20 | loss: 0.44638 | valid_auc: 0.6017  |  0:00:01s\n",
      "epoch 30 | loss: 0.42583 | valid_auc: 0.68155 |  0:00:02s\n",
      "epoch 40 | loss: 0.41678 | valid_auc: 0.71286 |  0:00:03s\n",
      "epoch 50 | loss: 0.41089 | valid_auc: 0.73018 |  0:00:04s\n",
      "epoch 60 | loss: 0.40975 | valid_auc: 0.73012 |  0:00:05s\n",
      "epoch 70 | loss: 0.39049 | valid_auc: 0.74623 |  0:00:05s\n",
      "epoch 80 | loss: 0.38759 | valid_auc: 0.74778 |  0:00:06s\n",
      "epoch 90 | loss: 0.37898 | valid_auc: 0.73636 |  0:00:07s\n",
      "epoch 100| loss: 0.37382 | valid_auc: 0.74636 |  0:00:08s\n",
      "epoch 110| loss: 0.3718  | valid_auc: 0.74494 |  0:00:08s\n",
      "epoch 120| loss: 0.35987 | valid_auc: 0.75244 |  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 102 and best_valid_auc = 0.75667\n",
      "epoch 0  | loss: 94370.03027| val_0_unsup_loss_numpy: 17508.9921875|  0:00:00s\n",
      "epoch 10 | loss: 50.14755| val_0_unsup_loss_numpy: 386726.625|  0:00:01s\n",
      "epoch 20 | loss: 26.16672| val_0_unsup_loss_numpy: 3018.63134765625|  0:00:03s\n",
      "epoch 30 | loss: 66.98974| val_0_unsup_loss_numpy: 114.78819274902344|  0:00:04s\n",
      "epoch 40 | loss: 21.48916| val_0_unsup_loss_numpy: 5724.16064453125|  0:00:06s\n",
      "epoch 50 | loss: 14.70267| val_0_unsup_loss_numpy: 458.03485107421875|  0:00:07s\n",
      "epoch 60 | loss: 5.83688 | val_0_unsup_loss_numpy: 18.403139114379883|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_unsup_loss_numpy = 4.336289882659912\n",
      "epoch 0  | loss: 0.61599 | valid_auc: 0.5856  |  0:00:00s\n",
      "epoch 10 | loss: 0.44422 | valid_auc: 0.68138 |  0:00:00s\n",
      "epoch 20 | loss: 0.43285 | valid_auc: 0.68199 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_valid_auc = 0.7296\n",
      "epoch 0  | loss: 172606.04688| val_0_unsup_loss_numpy: 443988.1875|  0:00:00s\n",
      "epoch 10 | loss: 302.13637| val_0_unsup_loss_numpy: 122103.421875|  0:00:01s\n",
      "epoch 20 | loss: 50.9832 | val_0_unsup_loss_numpy: 21448.150390625|  0:00:03s\n",
      "epoch 30 | loss: 101.98731| val_0_unsup_loss_numpy: 1743.913330078125|  0:00:04s\n",
      "epoch 40 | loss: 14.66358| val_0_unsup_loss_numpy: 7841.11767578125|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_unsup_loss_numpy = 1078.7030029296875\n",
      "epoch 0  | loss: 0.63946 | valid_auc: 0.49185 |  0:00:00s\n",
      "epoch 10 | loss: 0.45231 | valid_auc: 0.6158  |  0:00:00s\n",
      "epoch 20 | loss: 0.44445 | valid_auc: 0.66228 |  0:00:01s\n",
      "epoch 30 | loss: 0.42855 | valid_auc: 0.6914  |  0:00:02s\n",
      "epoch 40 | loss: 0.41515 | valid_auc: 0.71254 |  0:00:03s\n",
      "epoch 50 | loss: 0.40289 | valid_auc: 0.72965 |  0:00:04s\n",
      "epoch 60 | loss: 0.38184 | valid_auc: 0.7246  |  0:00:05s\n",
      "epoch 70 | loss: 0.3712  | valid_auc: 0.72264 |  0:00:06s\n",
      "epoch 80 | loss: 0.37229 | valid_auc: 0.69919 |  0:00:07s\n",
      "\n",
      "Early stopping occurred at epoch 85 with best_epoch = 65 and best_valid_auc = 0.73217\n",
      "epoch 0  | loss: 94648.92773| val_0_unsup_loss_numpy: 26948.091796875|  0:00:00s\n",
      "epoch 10 | loss: 66.31772| val_0_unsup_loss_numpy: 398369.0|  0:00:01s\n",
      "epoch 20 | loss: 58.88721| val_0_unsup_loss_numpy: 15123.94921875|  0:00:03s\n",
      "epoch 30 | loss: 253.05495| val_0_unsup_loss_numpy: 10266.33984375|  0:00:04s\n",
      "epoch 40 | loss: 15.11711| val_0_unsup_loss_numpy: 19079.060546875|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_unsup_loss_numpy = 202.38461303710938\n",
      "epoch 0  | loss: 0.66325 | valid_auc: 0.59274 |  0:00:00s\n",
      "epoch 10 | loss: 0.44115 | valid_auc: 0.64124 |  0:00:00s\n",
      "epoch 20 | loss: 0.43025 | valid_auc: 0.5524  |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 14:46:05,297]\u001b[0m Trial 21 finished with value: 0.7656666666666667 and parameters: {'n_d': 21, 'n_a': 8, 'n_step': 4, 'gamma': 1.7069340307634664, 'mask_type': 'entmatx'}. Best is trial 0 with value: 0.7656666666666667.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 | loss: 0.42158 | valid_auc: 0.6189  |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 12 and best_valid_auc = 0.67009\n",
      "epoch 0  | loss: 110376.18994| val_0_unsup_loss_numpy: 73749.171875|  0:00:00s\n",
      "epoch 10 | loss: 711.68524| val_0_unsup_loss_numpy: 1904.2930908203125|  0:00:01s\n",
      "epoch 20 | loss: 38.57199| val_0_unsup_loss_numpy: 564.6282958984375|  0:00:03s\n",
      "epoch 30 | loss: 33.08686| val_0_unsup_loss_numpy: 261.15325927734375|  0:00:04s\n",
      "epoch 40 | loss: 56.23543| val_0_unsup_loss_numpy: 73.16909790039062|  0:00:06s\n",
      "epoch 50 | loss: 17.6799 | val_0_unsup_loss_numpy: 213.44993591308594|  0:00:08s\n",
      "epoch 60 | loss: 63.28276| val_0_unsup_loss_numpy: 203.166259765625|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_unsup_loss_numpy = 21.714920043945312\n",
      "epoch 0  | loss: 0.65593 | valid_auc: 0.52988 |  0:00:00s\n",
      "epoch 10 | loss: 0.46077 | valid_auc: 0.48522 |  0:00:01s\n",
      "epoch 20 | loss: 0.44638 | valid_auc: 0.6017  |  0:00:01s\n",
      "epoch 30 | loss: 0.42583 | valid_auc: 0.68155 |  0:00:02s\n",
      "epoch 40 | loss: 0.41678 | valid_auc: 0.71286 |  0:00:03s\n",
      "epoch 50 | loss: 0.41089 | valid_auc: 0.73018 |  0:00:04s\n",
      "epoch 60 | loss: 0.40975 | valid_auc: 0.73012 |  0:00:05s\n",
      "epoch 70 | loss: 0.39049 | valid_auc: 0.74623 |  0:00:05s\n",
      "epoch 80 | loss: 0.38759 | valid_auc: 0.74778 |  0:00:06s\n",
      "epoch 90 | loss: 0.37898 | valid_auc: 0.73636 |  0:00:07s\n",
      "epoch 100| loss: 0.37382 | valid_auc: 0.74636 |  0:00:08s\n",
      "epoch 110| loss: 0.3718  | valid_auc: 0.74494 |  0:00:09s\n",
      "epoch 120| loss: 0.35987 | valid_auc: 0.75244 |  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 102 and best_valid_auc = 0.75667\n",
      "epoch 0  | loss: 94370.03027| val_0_unsup_loss_numpy: 17508.9921875|  0:00:00s\n",
      "epoch 10 | loss: 50.14755| val_0_unsup_loss_numpy: 386726.625|  0:00:01s\n",
      "epoch 20 | loss: 26.16672| val_0_unsup_loss_numpy: 3018.63134765625|  0:00:03s\n",
      "epoch 30 | loss: 66.98974| val_0_unsup_loss_numpy: 114.78819274902344|  0:00:04s\n",
      "epoch 40 | loss: 21.48916| val_0_unsup_loss_numpy: 5724.16064453125|  0:00:05s\n",
      "epoch 50 | loss: 14.70267| val_0_unsup_loss_numpy: 458.03485107421875|  0:00:07s\n",
      "epoch 60 | loss: 5.83688 | val_0_unsup_loss_numpy: 18.403139114379883|  0:00:08s\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_unsup_loss_numpy = 4.336289882659912\n",
      "epoch 0  | loss: 0.61599 | valid_auc: 0.5856  |  0:00:00s\n",
      "epoch 10 | loss: 0.44422 | valid_auc: 0.68138 |  0:00:00s\n",
      "epoch 20 | loss: 0.43285 | valid_auc: 0.68199 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_valid_auc = 0.7296\n",
      "epoch 0  | loss: 172606.04688| val_0_unsup_loss_numpy: 443988.1875|  0:00:00s\n",
      "epoch 10 | loss: 302.13637| val_0_unsup_loss_numpy: 122103.421875|  0:00:01s\n",
      "epoch 20 | loss: 50.9832 | val_0_unsup_loss_numpy: 21448.150390625|  0:00:03s\n",
      "epoch 30 | loss: 101.98731| val_0_unsup_loss_numpy: 1743.913330078125|  0:00:04s\n",
      "epoch 40 | loss: 14.66358| val_0_unsup_loss_numpy: 7841.11767578125|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_unsup_loss_numpy = 1078.7030029296875\n",
      "epoch 0  | loss: 0.63946 | valid_auc: 0.49185 |  0:00:00s\n",
      "epoch 10 | loss: 0.45231 | valid_auc: 0.6158  |  0:00:00s\n",
      "epoch 20 | loss: 0.44445 | valid_auc: 0.66228 |  0:00:01s\n",
      "epoch 30 | loss: 0.42855 | valid_auc: 0.6914  |  0:00:02s\n",
      "epoch 40 | loss: 0.41515 | valid_auc: 0.71254 |  0:00:03s\n",
      "epoch 50 | loss: 0.40289 | valid_auc: 0.72965 |  0:00:04s\n",
      "epoch 60 | loss: 0.38184 | valid_auc: 0.7246  |  0:00:05s\n",
      "epoch 70 | loss: 0.3712  | valid_auc: 0.72264 |  0:00:05s\n",
      "epoch 80 | loss: 0.37229 | valid_auc: 0.69919 |  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 85 with best_epoch = 65 and best_valid_auc = 0.73217\n",
      "epoch 0  | loss: 94648.92773| val_0_unsup_loss_numpy: 26948.091796875|  0:00:00s\n",
      "epoch 10 | loss: 66.31772| val_0_unsup_loss_numpy: 398369.0|  0:00:01s\n",
      "epoch 20 | loss: 58.88721| val_0_unsup_loss_numpy: 15123.94921875|  0:00:03s\n",
      "epoch 30 | loss: 253.05495| val_0_unsup_loss_numpy: 10266.33984375|  0:00:04s\n",
      "epoch 40 | loss: 15.11711| val_0_unsup_loss_numpy: 19079.060546875|  0:00:05s\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_unsup_loss_numpy = 202.38461303710938\n",
      "epoch 0  | loss: 0.66325 | valid_auc: 0.59274 |  0:00:00s\n",
      "epoch 10 | loss: 0.44115 | valid_auc: 0.64124 |  0:00:00s\n",
      "epoch 20 | loss: 0.43025 | valid_auc: 0.5524  |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 14:46:59,923]\u001b[0m Trial 22 finished with value: 0.7656666666666667 and parameters: {'n_d': 29, 'n_a': 14, 'n_step': 2, 'gamma': 1.7163415570713012, 'mask_type': 'entmatx'}. Best is trial 0 with value: 0.7656666666666667.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 | loss: 0.42158 | valid_auc: 0.6189  |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 12 and best_valid_auc = 0.67009\n",
      "epoch 0  | loss: 110376.18994| val_0_unsup_loss_numpy: 73749.171875|  0:00:00s\n",
      "epoch 10 | loss: 711.68524| val_0_unsup_loss_numpy: 1904.2930908203125|  0:00:01s\n",
      "epoch 20 | loss: 38.57199| val_0_unsup_loss_numpy: 564.6282958984375|  0:00:03s\n",
      "epoch 30 | loss: 33.08686| val_0_unsup_loss_numpy: 261.15325927734375|  0:00:04s\n",
      "epoch 40 | loss: 56.23543| val_0_unsup_loss_numpy: 73.16909790039062|  0:00:06s\n",
      "epoch 50 | loss: 17.6799 | val_0_unsup_loss_numpy: 213.44993591308594|  0:00:07s\n",
      "epoch 60 | loss: 63.28276| val_0_unsup_loss_numpy: 203.166259765625|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_unsup_loss_numpy = 21.714920043945312\n",
      "epoch 0  | loss: 0.65593 | valid_auc: 0.52988 |  0:00:00s\n",
      "epoch 10 | loss: 0.46077 | valid_auc: 0.48522 |  0:00:00s\n",
      "epoch 20 | loss: 0.44638 | valid_auc: 0.6017  |  0:00:01s\n",
      "epoch 30 | loss: 0.42583 | valid_auc: 0.68155 |  0:00:02s\n",
      "epoch 40 | loss: 0.41678 | valid_auc: 0.71286 |  0:00:03s\n",
      "epoch 50 | loss: 0.41089 | valid_auc: 0.73018 |  0:00:04s\n",
      "epoch 60 | loss: 0.40975 | valid_auc: 0.73012 |  0:00:05s\n",
      "epoch 70 | loss: 0.39049 | valid_auc: 0.74623 |  0:00:05s\n",
      "epoch 80 | loss: 0.38759 | valid_auc: 0.74778 |  0:00:06s\n",
      "epoch 90 | loss: 0.37898 | valid_auc: 0.73636 |  0:00:07s\n",
      "epoch 100| loss: 0.37382 | valid_auc: 0.74636 |  0:00:08s\n",
      "epoch 110| loss: 0.3718  | valid_auc: 0.74494 |  0:00:09s\n",
      "epoch 120| loss: 0.35987 | valid_auc: 0.75244 |  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 102 and best_valid_auc = 0.75667\n",
      "epoch 0  | loss: 94370.03027| val_0_unsup_loss_numpy: 17508.9921875|  0:00:00s\n",
      "epoch 10 | loss: 50.14755| val_0_unsup_loss_numpy: 386726.625|  0:00:01s\n",
      "epoch 20 | loss: 26.16672| val_0_unsup_loss_numpy: 3018.63134765625|  0:00:03s\n",
      "epoch 30 | loss: 66.98974| val_0_unsup_loss_numpy: 114.78819274902344|  0:00:04s\n",
      "epoch 40 | loss: 21.48916| val_0_unsup_loss_numpy: 5724.16064453125|  0:00:06s\n",
      "epoch 50 | loss: 14.70267| val_0_unsup_loss_numpy: 458.03485107421875|  0:00:07s\n",
      "epoch 60 | loss: 5.83688 | val_0_unsup_loss_numpy: 18.403139114379883|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_unsup_loss_numpy = 4.336289882659912\n",
      "epoch 0  | loss: 0.61599 | valid_auc: 0.5856  |  0:00:00s\n",
      "epoch 10 | loss: 0.44422 | valid_auc: 0.68138 |  0:00:00s\n",
      "epoch 20 | loss: 0.43285 | valid_auc: 0.68199 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_valid_auc = 0.7296\n",
      "epoch 0  | loss: 172606.04688| val_0_unsup_loss_numpy: 443988.1875|  0:00:00s\n",
      "epoch 10 | loss: 302.13637| val_0_unsup_loss_numpy: 122103.421875|  0:00:01s\n",
      "epoch 20 | loss: 50.9832 | val_0_unsup_loss_numpy: 21448.150390625|  0:00:03s\n",
      "epoch 30 | loss: 101.98731| val_0_unsup_loss_numpy: 1743.913330078125|  0:00:04s\n",
      "epoch 40 | loss: 14.66358| val_0_unsup_loss_numpy: 7841.11767578125|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_unsup_loss_numpy = 1078.7030029296875\n",
      "epoch 0  | loss: 0.63946 | valid_auc: 0.49185 |  0:00:00s\n",
      "epoch 10 | loss: 0.45231 | valid_auc: 0.6158  |  0:00:00s\n",
      "epoch 20 | loss: 0.44445 | valid_auc: 0.66228 |  0:00:01s\n",
      "epoch 30 | loss: 0.42855 | valid_auc: 0.6914  |  0:00:02s\n",
      "epoch 40 | loss: 0.41515 | valid_auc: 0.71254 |  0:00:03s\n",
      "epoch 50 | loss: 0.40289 | valid_auc: 0.72965 |  0:00:04s\n",
      "epoch 60 | loss: 0.38184 | valid_auc: 0.7246  |  0:00:04s\n",
      "epoch 70 | loss: 0.3712  | valid_auc: 0.72264 |  0:00:05s\n",
      "epoch 80 | loss: 0.37229 | valid_auc: 0.69919 |  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 85 with best_epoch = 65 and best_valid_auc = 0.73217\n",
      "epoch 0  | loss: 94648.92773| val_0_unsup_loss_numpy: 26948.091796875|  0:00:00s\n",
      "epoch 10 | loss: 66.31772| val_0_unsup_loss_numpy: 398369.0|  0:00:01s\n",
      "epoch 20 | loss: 58.88721| val_0_unsup_loss_numpy: 15123.94921875|  0:00:03s\n",
      "epoch 30 | loss: 253.05495| val_0_unsup_loss_numpy: 10266.33984375|  0:00:04s\n",
      "epoch 40 | loss: 15.11711| val_0_unsup_loss_numpy: 19079.060546875|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_unsup_loss_numpy = 202.38461303710938\n",
      "epoch 0  | loss: 0.66325 | valid_auc: 0.59274 |  0:00:00s\n",
      "epoch 10 | loss: 0.44115 | valid_auc: 0.64124 |  0:00:00s\n",
      "epoch 20 | loss: 0.43025 | valid_auc: 0.5524  |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 14:47:54,900]\u001b[0m Trial 23 finished with value: 0.7656666666666667 and parameters: {'n_d': 20, 'n_a': 25, 'n_step': 5, 'gamma': 1.6119687787895545, 'mask_type': 'entmatx'}. Best is trial 0 with value: 0.7656666666666667.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 | loss: 0.42158 | valid_auc: 0.6189  |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 12 and best_valid_auc = 0.67009\n",
      "epoch 0  | loss: 110376.18994| val_0_unsup_loss_numpy: 73749.171875|  0:00:00s\n",
      "epoch 10 | loss: 711.68524| val_0_unsup_loss_numpy: 1904.2930908203125|  0:00:01s\n",
      "epoch 20 | loss: 38.57199| val_0_unsup_loss_numpy: 564.6282958984375|  0:00:03s\n",
      "epoch 30 | loss: 33.08686| val_0_unsup_loss_numpy: 261.15325927734375|  0:00:04s\n",
      "epoch 40 | loss: 56.23543| val_0_unsup_loss_numpy: 73.16909790039062|  0:00:06s\n",
      "epoch 50 | loss: 17.6799 | val_0_unsup_loss_numpy: 213.44993591308594|  0:00:07s\n",
      "epoch 60 | loss: 63.28276| val_0_unsup_loss_numpy: 203.166259765625|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_unsup_loss_numpy = 21.714920043945312\n",
      "epoch 0  | loss: 0.65593 | valid_auc: 0.52988 |  0:00:00s\n",
      "epoch 10 | loss: 0.46077 | valid_auc: 0.48522 |  0:00:00s\n",
      "epoch 20 | loss: 0.44638 | valid_auc: 0.6017  |  0:00:01s\n",
      "epoch 30 | loss: 0.42583 | valid_auc: 0.68155 |  0:00:02s\n",
      "epoch 40 | loss: 0.41678 | valid_auc: 0.71286 |  0:00:03s\n",
      "epoch 50 | loss: 0.41089 | valid_auc: 0.73018 |  0:00:04s\n",
      "epoch 60 | loss: 0.40975 | valid_auc: 0.73012 |  0:00:05s\n",
      "epoch 70 | loss: 0.39049 | valid_auc: 0.74623 |  0:00:05s\n",
      "epoch 80 | loss: 0.38759 | valid_auc: 0.74778 |  0:00:06s\n",
      "epoch 90 | loss: 0.37898 | valid_auc: 0.73636 |  0:00:07s\n",
      "epoch 100| loss: 0.37382 | valid_auc: 0.74636 |  0:00:08s\n",
      "epoch 110| loss: 0.3718  | valid_auc: 0.74494 |  0:00:08s\n",
      "epoch 120| loss: 0.35987 | valid_auc: 0.75244 |  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 102 and best_valid_auc = 0.75667\n",
      "epoch 0  | loss: 94370.03027| val_0_unsup_loss_numpy: 17508.9921875|  0:00:00s\n",
      "epoch 10 | loss: 50.14755| val_0_unsup_loss_numpy: 386726.625|  0:00:01s\n",
      "epoch 20 | loss: 26.16672| val_0_unsup_loss_numpy: 3018.63134765625|  0:00:03s\n",
      "epoch 30 | loss: 66.98974| val_0_unsup_loss_numpy: 114.78819274902344|  0:00:04s\n",
      "epoch 40 | loss: 21.48916| val_0_unsup_loss_numpy: 5724.16064453125|  0:00:06s\n",
      "epoch 50 | loss: 14.70267| val_0_unsup_loss_numpy: 458.03485107421875|  0:00:07s\n",
      "epoch 60 | loss: 5.83688 | val_0_unsup_loss_numpy: 18.403139114379883|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_unsup_loss_numpy = 4.336289882659912\n",
      "epoch 0  | loss: 0.61599 | valid_auc: 0.5856  |  0:00:00s\n",
      "epoch 10 | loss: 0.44422 | valid_auc: 0.68138 |  0:00:01s\n",
      "epoch 20 | loss: 0.43285 | valid_auc: 0.68199 |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_valid_auc = 0.7296\n",
      "epoch 0  | loss: 172606.04688| val_0_unsup_loss_numpy: 443988.1875|  0:00:00s\n",
      "epoch 10 | loss: 302.13637| val_0_unsup_loss_numpy: 122103.421875|  0:00:01s\n",
      "epoch 20 | loss: 50.9832 | val_0_unsup_loss_numpy: 21448.150390625|  0:00:03s\n",
      "epoch 30 | loss: 101.98731| val_0_unsup_loss_numpy: 1743.913330078125|  0:00:04s\n",
      "epoch 40 | loss: 14.66358| val_0_unsup_loss_numpy: 7841.11767578125|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_unsup_loss_numpy = 1078.7030029296875\n",
      "epoch 0  | loss: 0.63946 | valid_auc: 0.49185 |  0:00:00s\n",
      "epoch 10 | loss: 0.45231 | valid_auc: 0.6158  |  0:00:00s\n",
      "epoch 20 | loss: 0.44445 | valid_auc: 0.66228 |  0:00:01s\n",
      "epoch 30 | loss: 0.42855 | valid_auc: 0.6914  |  0:00:02s\n",
      "epoch 40 | loss: 0.41515 | valid_auc: 0.71254 |  0:00:03s\n",
      "epoch 50 | loss: 0.40289 | valid_auc: 0.72965 |  0:00:04s\n",
      "epoch 60 | loss: 0.38184 | valid_auc: 0.7246  |  0:00:04s\n",
      "epoch 70 | loss: 0.3712  | valid_auc: 0.72264 |  0:00:05s\n",
      "epoch 80 | loss: 0.37229 | valid_auc: 0.69919 |  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 85 with best_epoch = 65 and best_valid_auc = 0.73217\n",
      "epoch 0  | loss: 94648.92773| val_0_unsup_loss_numpy: 26948.091796875|  0:00:00s\n",
      "epoch 10 | loss: 66.31772| val_0_unsup_loss_numpy: 398369.0|  0:00:01s\n",
      "epoch 20 | loss: 58.88721| val_0_unsup_loss_numpy: 15123.94921875|  0:00:03s\n",
      "epoch 30 | loss: 253.05495| val_0_unsup_loss_numpy: 10266.33984375|  0:00:04s\n",
      "epoch 40 | loss: 15.11711| val_0_unsup_loss_numpy: 19079.060546875|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_unsup_loss_numpy = 202.38461303710938\n",
      "epoch 0  | loss: 0.66325 | valid_auc: 0.59274 |  0:00:00s\n",
      "epoch 10 | loss: 0.44115 | valid_auc: 0.64124 |  0:00:00s\n",
      "epoch 20 | loss: 0.43025 | valid_auc: 0.5524  |  0:00:01s\n",
      "epoch 30 | loss: 0.42158 | valid_auc: 0.6189  |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 12 and best_valid_auc = 0.67009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 14:48:50,379]\u001b[0m Trial 24 finished with value: 0.7656666666666667 and parameters: {'n_d': 41, 'n_a': 31, 'n_step': 4, 'gamma': 1.7913032879236999, 'mask_type': 'entmatx'}. Best is trial 0 with value: 0.7656666666666667.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 110376.18994| val_0_unsup_loss_numpy: 73749.171875|  0:00:00s\n",
      "epoch 10 | loss: 711.68524| val_0_unsup_loss_numpy: 1904.2930908203125|  0:00:01s\n",
      "epoch 20 | loss: 38.57199| val_0_unsup_loss_numpy: 564.6282958984375|  0:00:03s\n",
      "epoch 30 | loss: 33.08686| val_0_unsup_loss_numpy: 261.15325927734375|  0:00:04s\n",
      "epoch 40 | loss: 56.23543| val_0_unsup_loss_numpy: 73.16909790039062|  0:00:06s\n",
      "epoch 50 | loss: 17.6799 | val_0_unsup_loss_numpy: 213.44993591308594|  0:00:08s\n",
      "epoch 60 | loss: 63.28276| val_0_unsup_loss_numpy: 203.166259765625|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_unsup_loss_numpy = 21.714920043945312\n",
      "epoch 0  | loss: 0.65593 | valid_auc: 0.52988 |  0:00:00s\n",
      "epoch 10 | loss: 0.46077 | valid_auc: 0.48522 |  0:00:00s\n",
      "epoch 20 | loss: 0.44638 | valid_auc: 0.6017  |  0:00:01s\n",
      "epoch 30 | loss: 0.42583 | valid_auc: 0.68155 |  0:00:02s\n",
      "epoch 40 | loss: 0.41678 | valid_auc: 0.71286 |  0:00:03s\n",
      "epoch 50 | loss: 0.41089 | valid_auc: 0.73018 |  0:00:04s\n",
      "epoch 60 | loss: 0.40975 | valid_auc: 0.73012 |  0:00:05s\n",
      "epoch 70 | loss: 0.39049 | valid_auc: 0.74623 |  0:00:06s\n",
      "epoch 80 | loss: 0.38759 | valid_auc: 0.74778 |  0:00:06s\n",
      "epoch 90 | loss: 0.37898 | valid_auc: 0.73636 |  0:00:07s\n",
      "epoch 100| loss: 0.37382 | valid_auc: 0.74636 |  0:00:08s\n",
      "epoch 110| loss: 0.3718  | valid_auc: 0.74494 |  0:00:09s\n",
      "epoch 120| loss: 0.35987 | valid_auc: 0.75244 |  0:00:10s\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 102 and best_valid_auc = 0.75667\n",
      "epoch 0  | loss: 94370.03027| val_0_unsup_loss_numpy: 17508.9921875|  0:00:00s\n",
      "epoch 10 | loss: 50.14755| val_0_unsup_loss_numpy: 386726.625|  0:00:01s\n",
      "epoch 20 | loss: 26.16672| val_0_unsup_loss_numpy: 3018.63134765625|  0:00:03s\n",
      "epoch 30 | loss: 66.98974| val_0_unsup_loss_numpy: 114.78819274902344|  0:00:04s\n",
      "epoch 40 | loss: 21.48916| val_0_unsup_loss_numpy: 5724.16064453125|  0:00:06s\n",
      "epoch 50 | loss: 14.70267| val_0_unsup_loss_numpy: 458.03485107421875|  0:00:07s\n",
      "epoch 60 | loss: 5.83688 | val_0_unsup_loss_numpy: 18.403139114379883|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_unsup_loss_numpy = 4.336289882659912\n",
      "epoch 0  | loss: 0.61599 | valid_auc: 0.5856  |  0:00:00s\n",
      "epoch 10 | loss: 0.44422 | valid_auc: 0.68138 |  0:00:00s\n",
      "epoch 20 | loss: 0.43285 | valid_auc: 0.68199 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_valid_auc = 0.7296\n",
      "epoch 0  | loss: 172606.04688| val_0_unsup_loss_numpy: 443988.1875|  0:00:00s\n",
      "epoch 10 | loss: 302.13637| val_0_unsup_loss_numpy: 122103.421875|  0:00:01s\n",
      "epoch 20 | loss: 50.9832 | val_0_unsup_loss_numpy: 21448.150390625|  0:00:03s\n",
      "epoch 30 | loss: 101.98731| val_0_unsup_loss_numpy: 1743.913330078125|  0:00:05s\n",
      "epoch 40 | loss: 14.66358| val_0_unsup_loss_numpy: 7841.11767578125|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_unsup_loss_numpy = 1078.7030029296875\n",
      "epoch 0  | loss: 0.63946 | valid_auc: 0.49185 |  0:00:00s\n",
      "epoch 10 | loss: 0.45231 | valid_auc: 0.6158  |  0:00:00s\n",
      "epoch 20 | loss: 0.44445 | valid_auc: 0.66228 |  0:00:01s\n",
      "epoch 30 | loss: 0.42855 | valid_auc: 0.6914  |  0:00:02s\n",
      "epoch 40 | loss: 0.41515 | valid_auc: 0.71254 |  0:00:03s\n",
      "epoch 50 | loss: 0.40289 | valid_auc: 0.72965 |  0:00:04s\n",
      "epoch 60 | loss: 0.38184 | valid_auc: 0.7246  |  0:00:05s\n",
      "epoch 70 | loss: 0.3712  | valid_auc: 0.72264 |  0:00:06s\n",
      "epoch 80 | loss: 0.37229 | valid_auc: 0.69919 |  0:00:07s\n",
      "\n",
      "Early stopping occurred at epoch 85 with best_epoch = 65 and best_valid_auc = 0.73217\n",
      "epoch 0  | loss: 94648.92773| val_0_unsup_loss_numpy: 26948.091796875|  0:00:00s\n",
      "epoch 10 | loss: 66.31772| val_0_unsup_loss_numpy: 398369.0|  0:00:01s\n",
      "epoch 20 | loss: 58.88721| val_0_unsup_loss_numpy: 15123.94921875|  0:00:03s\n",
      "epoch 30 | loss: 253.05495| val_0_unsup_loss_numpy: 10266.33984375|  0:00:04s\n",
      "epoch 40 | loss: 15.11711| val_0_unsup_loss_numpy: 19079.060546875|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_unsup_loss_numpy = 202.38461303710938\n",
      "epoch 0  | loss: 0.66325 | valid_auc: 0.59274 |  0:00:00s\n",
      "epoch 10 | loss: 0.44115 | valid_auc: 0.64124 |  0:00:00s\n",
      "epoch 20 | loss: 0.43025 | valid_auc: 0.5524  |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 14:49:47,427]\u001b[0m Trial 25 finished with value: 0.7656666666666667 and parameters: {'n_d': 33, 'n_a': 14, 'n_step': 2, 'gamma': 1.494434901957632, 'mask_type': 'entmatx'}. Best is trial 0 with value: 0.7656666666666667.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 | loss: 0.42158 | valid_auc: 0.6189  |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 12 and best_valid_auc = 0.67009\n",
      "epoch 0  | loss: 110376.18994| val_0_unsup_loss_numpy: 73749.171875|  0:00:00s\n",
      "epoch 10 | loss: 711.68524| val_0_unsup_loss_numpy: 1904.2930908203125|  0:00:02s\n",
      "epoch 20 | loss: 38.57199| val_0_unsup_loss_numpy: 564.6282958984375|  0:00:04s\n",
      "epoch 30 | loss: 33.08686| val_0_unsup_loss_numpy: 261.15325927734375|  0:00:06s\n",
      "epoch 40 | loss: 56.23543| val_0_unsup_loss_numpy: 73.16909790039062|  0:00:07s\n",
      "epoch 50 | loss: 17.6799 | val_0_unsup_loss_numpy: 213.44993591308594|  0:00:09s\n",
      "epoch 60 | loss: 63.28276| val_0_unsup_loss_numpy: 203.166259765625|  0:00:10s\n",
      "\n",
      "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_unsup_loss_numpy = 21.714920043945312\n",
      "epoch 0  | loss: 0.65593 | valid_auc: 0.52988 |  0:00:00s\n",
      "epoch 10 | loss: 0.46077 | valid_auc: 0.48522 |  0:00:00s\n",
      "epoch 20 | loss: 0.44638 | valid_auc: 0.6017  |  0:00:01s\n",
      "epoch 30 | loss: 0.42583 | valid_auc: 0.68155 |  0:00:02s\n",
      "epoch 40 | loss: 0.41678 | valid_auc: 0.71286 |  0:00:03s\n",
      "epoch 50 | loss: 0.41089 | valid_auc: 0.73018 |  0:00:04s\n",
      "epoch 60 | loss: 0.40975 | valid_auc: 0.73012 |  0:00:05s\n",
      "epoch 70 | loss: 0.39049 | valid_auc: 0.74623 |  0:00:05s\n",
      "epoch 80 | loss: 0.38759 | valid_auc: 0.74778 |  0:00:06s\n",
      "epoch 90 | loss: 0.37898 | valid_auc: 0.73636 |  0:00:07s\n",
      "epoch 100| loss: 0.37382 | valid_auc: 0.74636 |  0:00:08s\n",
      "epoch 110| loss: 0.3718  | valid_auc: 0.74494 |  0:00:09s\n",
      "epoch 120| loss: 0.35987 | valid_auc: 0.75244 |  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 102 and best_valid_auc = 0.75667\n",
      "epoch 0  | loss: 94370.03027| val_0_unsup_loss_numpy: 17508.9921875|  0:00:00s\n",
      "epoch 10 | loss: 50.14755| val_0_unsup_loss_numpy: 386726.625|  0:00:01s\n",
      "epoch 20 | loss: 26.16672| val_0_unsup_loss_numpy: 3018.63134765625|  0:00:03s\n",
      "epoch 30 | loss: 66.98974| val_0_unsup_loss_numpy: 114.78819274902344|  0:00:04s\n",
      "epoch 40 | loss: 21.48916| val_0_unsup_loss_numpy: 5724.16064453125|  0:00:06s\n",
      "epoch 50 | loss: 14.70267| val_0_unsup_loss_numpy: 458.03485107421875|  0:00:07s\n",
      "epoch 60 | loss: 5.83688 | val_0_unsup_loss_numpy: 18.403139114379883|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_unsup_loss_numpy = 4.336289882659912\n",
      "epoch 0  | loss: 0.61599 | valid_auc: 0.5856  |  0:00:00s\n",
      "epoch 10 | loss: 0.44422 | valid_auc: 0.68138 |  0:00:00s\n",
      "epoch 20 | loss: 0.43285 | valid_auc: 0.68199 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_valid_auc = 0.7296\n",
      "epoch 0  | loss: 172606.04688| val_0_unsup_loss_numpy: 443988.1875|  0:00:00s\n",
      "epoch 10 | loss: 302.13637| val_0_unsup_loss_numpy: 122103.421875|  0:00:01s\n",
      "epoch 20 | loss: 50.9832 | val_0_unsup_loss_numpy: 21448.150390625|  0:00:03s\n",
      "epoch 30 | loss: 101.98731| val_0_unsup_loss_numpy: 1743.913330078125|  0:00:04s\n",
      "epoch 40 | loss: 14.66358| val_0_unsup_loss_numpy: 7841.11767578125|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_unsup_loss_numpy = 1078.7030029296875\n",
      "epoch 0  | loss: 0.63946 | valid_auc: 0.49185 |  0:00:00s\n",
      "epoch 10 | loss: 0.45231 | valid_auc: 0.6158  |  0:00:00s\n",
      "epoch 20 | loss: 0.44445 | valid_auc: 0.66228 |  0:00:01s\n",
      "epoch 30 | loss: 0.42855 | valid_auc: 0.6914  |  0:00:02s\n",
      "epoch 40 | loss: 0.41515 | valid_auc: 0.71254 |  0:00:03s\n",
      "epoch 50 | loss: 0.40289 | valid_auc: 0.72965 |  0:00:04s\n",
      "epoch 60 | loss: 0.38184 | valid_auc: 0.7246  |  0:00:04s\n",
      "epoch 70 | loss: 0.3712  | valid_auc: 0.72264 |  0:00:05s\n",
      "epoch 80 | loss: 0.37229 | valid_auc: 0.69919 |  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 85 with best_epoch = 65 and best_valid_auc = 0.73217\n",
      "epoch 0  | loss: 94648.92773| val_0_unsup_loss_numpy: 26948.091796875|  0:00:00s\n",
      "epoch 10 | loss: 66.31772| val_0_unsup_loss_numpy: 398369.0|  0:00:01s\n",
      "epoch 20 | loss: 58.88721| val_0_unsup_loss_numpy: 15123.94921875|  0:00:03s\n",
      "epoch 30 | loss: 253.05495| val_0_unsup_loss_numpy: 10266.33984375|  0:00:04s\n",
      "epoch 40 | loss: 15.11711| val_0_unsup_loss_numpy: 19079.060546875|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_unsup_loss_numpy = 202.38461303710938\n",
      "epoch 0  | loss: 0.66325 | valid_auc: 0.59274 |  0:00:00s\n",
      "epoch 10 | loss: 0.44115 | valid_auc: 0.64124 |  0:00:00s\n",
      "epoch 20 | loss: 0.43025 | valid_auc: 0.5524  |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 14:50:44,496]\u001b[0m Trial 26 finished with value: 0.7656666666666667 and parameters: {'n_d': 52, 'n_a': 19, 'n_step': 3, 'gamma': 1.7716585404282987, 'mask_type': 'entmatx'}. Best is trial 0 with value: 0.7656666666666667.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 | loss: 0.42158 | valid_auc: 0.6189  |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 12 and best_valid_auc = 0.67009\n",
      "epoch 0  | loss: 110376.18994| val_0_unsup_loss_numpy: 73749.171875|  0:00:00s\n",
      "epoch 10 | loss: 711.68524| val_0_unsup_loss_numpy: 1904.2930908203125|  0:00:01s\n",
      "epoch 20 | loss: 38.57199| val_0_unsup_loss_numpy: 564.6282958984375|  0:00:03s\n",
      "epoch 30 | loss: 33.08686| val_0_unsup_loss_numpy: 261.15325927734375|  0:00:04s\n",
      "epoch 40 | loss: 56.23543| val_0_unsup_loss_numpy: 73.16909790039062|  0:00:06s\n",
      "epoch 50 | loss: 17.6799 | val_0_unsup_loss_numpy: 213.44993591308594|  0:00:07s\n",
      "epoch 60 | loss: 63.28276| val_0_unsup_loss_numpy: 203.166259765625|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_unsup_loss_numpy = 21.714920043945312\n",
      "epoch 0  | loss: 0.65593 | valid_auc: 0.52988 |  0:00:00s\n",
      "epoch 10 | loss: 0.46077 | valid_auc: 0.48522 |  0:00:00s\n",
      "epoch 20 | loss: 0.44638 | valid_auc: 0.6017  |  0:00:01s\n",
      "epoch 30 | loss: 0.42583 | valid_auc: 0.68155 |  0:00:02s\n",
      "epoch 40 | loss: 0.41678 | valid_auc: 0.71286 |  0:00:03s\n",
      "epoch 50 | loss: 0.41089 | valid_auc: 0.73018 |  0:00:04s\n",
      "epoch 60 | loss: 0.40975 | valid_auc: 0.73012 |  0:00:04s\n",
      "epoch 70 | loss: 0.39049 | valid_auc: 0.74623 |  0:00:05s\n",
      "epoch 80 | loss: 0.38759 | valid_auc: 0.74778 |  0:00:06s\n",
      "epoch 90 | loss: 0.37898 | valid_auc: 0.73636 |  0:00:07s\n",
      "epoch 100| loss: 0.37382 | valid_auc: 0.74636 |  0:00:08s\n",
      "epoch 110| loss: 0.3718  | valid_auc: 0.74494 |  0:00:08s\n",
      "epoch 120| loss: 0.35987 | valid_auc: 0.75244 |  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 102 and best_valid_auc = 0.75667\n",
      "epoch 0  | loss: 94370.03027| val_0_unsup_loss_numpy: 17508.9921875|  0:00:00s\n",
      "epoch 10 | loss: 50.14755| val_0_unsup_loss_numpy: 386726.625|  0:00:01s\n",
      "epoch 20 | loss: 26.16672| val_0_unsup_loss_numpy: 3018.63134765625|  0:00:03s\n",
      "epoch 30 | loss: 66.98974| val_0_unsup_loss_numpy: 114.78819274902344|  0:00:04s\n",
      "epoch 40 | loss: 21.48916| val_0_unsup_loss_numpy: 5724.16064453125|  0:00:06s\n",
      "epoch 50 | loss: 14.70267| val_0_unsup_loss_numpy: 458.03485107421875|  0:00:07s\n",
      "epoch 60 | loss: 5.83688 | val_0_unsup_loss_numpy: 18.403139114379883|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_unsup_loss_numpy = 4.336289882659912\n",
      "epoch 0  | loss: 0.61599 | valid_auc: 0.5856  |  0:00:00s\n",
      "epoch 10 | loss: 0.44422 | valid_auc: 0.68138 |  0:00:00s\n",
      "epoch 20 | loss: 0.43285 | valid_auc: 0.68199 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_valid_auc = 0.7296\n",
      "epoch 0  | loss: 172606.04688| val_0_unsup_loss_numpy: 443988.1875|  0:00:00s\n",
      "epoch 10 | loss: 302.13637| val_0_unsup_loss_numpy: 122103.421875|  0:00:01s\n",
      "epoch 20 | loss: 50.9832 | val_0_unsup_loss_numpy: 21448.150390625|  0:00:03s\n",
      "epoch 30 | loss: 101.98731| val_0_unsup_loss_numpy: 1743.913330078125|  0:00:04s\n",
      "epoch 40 | loss: 14.66358| val_0_unsup_loss_numpy: 7841.11767578125|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_unsup_loss_numpy = 1078.7030029296875\n",
      "epoch 0  | loss: 0.63946 | valid_auc: 0.49185 |  0:00:00s\n",
      "epoch 10 | loss: 0.45231 | valid_auc: 0.6158  |  0:00:00s\n",
      "epoch 20 | loss: 0.44445 | valid_auc: 0.66228 |  0:00:01s\n",
      "epoch 30 | loss: 0.42855 | valid_auc: 0.6914  |  0:00:02s\n",
      "epoch 40 | loss: 0.41515 | valid_auc: 0.71254 |  0:00:03s\n",
      "epoch 50 | loss: 0.40289 | valid_auc: 0.72965 |  0:00:04s\n",
      "epoch 60 | loss: 0.38184 | valid_auc: 0.7246  |  0:00:04s\n",
      "epoch 70 | loss: 0.3712  | valid_auc: 0.72264 |  0:00:05s\n",
      "epoch 80 | loss: 0.37229 | valid_auc: 0.69919 |  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 85 with best_epoch = 65 and best_valid_auc = 0.73217\n",
      "epoch 0  | loss: 94648.92773| val_0_unsup_loss_numpy: 26948.091796875|  0:00:00s\n",
      "epoch 10 | loss: 66.31772| val_0_unsup_loss_numpy: 398369.0|  0:00:01s\n",
      "epoch 20 | loss: 58.88721| val_0_unsup_loss_numpy: 15123.94921875|  0:00:03s\n",
      "epoch 30 | loss: 253.05495| val_0_unsup_loss_numpy: 10266.33984375|  0:00:04s\n",
      "epoch 40 | loss: 15.11711| val_0_unsup_loss_numpy: 19079.060546875|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_unsup_loss_numpy = 202.38461303710938\n",
      "epoch 0  | loss: 0.66325 | valid_auc: 0.59274 |  0:00:00s\n",
      "epoch 10 | loss: 0.44115 | valid_auc: 0.64124 |  0:00:00s\n",
      "epoch 20 | loss: 0.43025 | valid_auc: 0.5524  |  0:00:01s\n",
      "epoch 30 | loss: 0.42158 | valid_auc: 0.6189  |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 12 and best_valid_auc = 0.67009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 14:51:39,812]\u001b[0m Trial 27 finished with value: 0.7656666666666667 and parameters: {'n_d': 22, 'n_a': 42, 'n_step': 6, 'gamma': 1.6495702502566643, 'mask_type': 'entmatx'}. Best is trial 0 with value: 0.7656666666666667.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 110376.18994| val_0_unsup_loss_numpy: 73749.171875|  0:00:00s\n",
      "epoch 10 | loss: 711.68524| val_0_unsup_loss_numpy: 1904.2930908203125|  0:00:01s\n",
      "epoch 20 | loss: 38.57199| val_0_unsup_loss_numpy: 564.6282958984375|  0:00:03s\n",
      "epoch 30 | loss: 33.08686| val_0_unsup_loss_numpy: 261.15325927734375|  0:00:04s\n",
      "epoch 40 | loss: 56.23543| val_0_unsup_loss_numpy: 73.16909790039062|  0:00:06s\n",
      "epoch 50 | loss: 17.6799 | val_0_unsup_loss_numpy: 213.44993591308594|  0:00:07s\n",
      "epoch 60 | loss: 63.28276| val_0_unsup_loss_numpy: 203.166259765625|  0:00:10s\n",
      "\n",
      "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_unsup_loss_numpy = 21.714920043945312\n",
      "epoch 0  | loss: 0.65593 | valid_auc: 0.52988 |  0:00:00s\n",
      "epoch 10 | loss: 0.46077 | valid_auc: 0.48522 |  0:00:00s\n",
      "epoch 20 | loss: 0.44638 | valid_auc: 0.6017  |  0:00:01s\n",
      "epoch 30 | loss: 0.42583 | valid_auc: 0.68155 |  0:00:02s\n",
      "epoch 40 | loss: 0.41678 | valid_auc: 0.71286 |  0:00:03s\n",
      "epoch 50 | loss: 0.41089 | valid_auc: 0.73018 |  0:00:04s\n",
      "epoch 60 | loss: 0.40975 | valid_auc: 0.73012 |  0:00:04s\n",
      "epoch 70 | loss: 0.39049 | valid_auc: 0.74623 |  0:00:05s\n",
      "epoch 80 | loss: 0.38759 | valid_auc: 0.74778 |  0:00:06s\n",
      "epoch 90 | loss: 0.37898 | valid_auc: 0.73636 |  0:00:07s\n",
      "epoch 100| loss: 0.37382 | valid_auc: 0.74636 |  0:00:08s\n",
      "epoch 110| loss: 0.3718  | valid_auc: 0.74494 |  0:00:08s\n",
      "epoch 120| loss: 0.35987 | valid_auc: 0.75244 |  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 102 and best_valid_auc = 0.75667\n",
      "epoch 0  | loss: 94370.03027| val_0_unsup_loss_numpy: 17508.9921875|  0:00:00s\n",
      "epoch 10 | loss: 50.14755| val_0_unsup_loss_numpy: 386726.625|  0:00:01s\n",
      "epoch 20 | loss: 26.16672| val_0_unsup_loss_numpy: 3018.63134765625|  0:00:03s\n",
      "epoch 30 | loss: 66.98974| val_0_unsup_loss_numpy: 114.78819274902344|  0:00:04s\n",
      "epoch 40 | loss: 21.48916| val_0_unsup_loss_numpy: 5724.16064453125|  0:00:06s\n",
      "epoch 50 | loss: 14.70267| val_0_unsup_loss_numpy: 458.03485107421875|  0:00:07s\n",
      "epoch 60 | loss: 5.83688 | val_0_unsup_loss_numpy: 18.403139114379883|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_unsup_loss_numpy = 4.336289882659912\n",
      "epoch 0  | loss: 0.61599 | valid_auc: 0.5856  |  0:00:00s\n",
      "epoch 10 | loss: 0.44422 | valid_auc: 0.68138 |  0:00:01s\n",
      "epoch 20 | loss: 0.43285 | valid_auc: 0.68199 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_valid_auc = 0.7296\n",
      "epoch 0  | loss: 172606.04688| val_0_unsup_loss_numpy: 443988.1875|  0:00:00s\n",
      "epoch 10 | loss: 302.13637| val_0_unsup_loss_numpy: 122103.421875|  0:00:01s\n",
      "epoch 20 | loss: 50.9832 | val_0_unsup_loss_numpy: 21448.150390625|  0:00:03s\n",
      "epoch 30 | loss: 101.98731| val_0_unsup_loss_numpy: 1743.913330078125|  0:00:04s\n",
      "epoch 40 | loss: 14.66358| val_0_unsup_loss_numpy: 7841.11767578125|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_unsup_loss_numpy = 1078.7030029296875\n",
      "epoch 0  | loss: 0.63946 | valid_auc: 0.49185 |  0:00:00s\n",
      "epoch 10 | loss: 0.45231 | valid_auc: 0.6158  |  0:00:00s\n",
      "epoch 20 | loss: 0.44445 | valid_auc: 0.66228 |  0:00:01s\n",
      "epoch 30 | loss: 0.42855 | valid_auc: 0.6914  |  0:00:02s\n",
      "epoch 40 | loss: 0.41515 | valid_auc: 0.71254 |  0:00:03s\n",
      "epoch 50 | loss: 0.40289 | valid_auc: 0.72965 |  0:00:04s\n",
      "epoch 60 | loss: 0.38184 | valid_auc: 0.7246  |  0:00:05s\n",
      "epoch 70 | loss: 0.3712  | valid_auc: 0.72264 |  0:00:05s\n",
      "epoch 80 | loss: 0.37229 | valid_auc: 0.69919 |  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 85 with best_epoch = 65 and best_valid_auc = 0.73217\n",
      "epoch 0  | loss: 94648.92773| val_0_unsup_loss_numpy: 26948.091796875|  0:00:00s\n",
      "epoch 10 | loss: 66.31772| val_0_unsup_loss_numpy: 398369.0|  0:00:01s\n",
      "epoch 20 | loss: 58.88721| val_0_unsup_loss_numpy: 15123.94921875|  0:00:03s\n",
      "epoch 30 | loss: 253.05495| val_0_unsup_loss_numpy: 10266.33984375|  0:00:04s\n",
      "epoch 40 | loss: 15.11711| val_0_unsup_loss_numpy: 19079.060546875|  0:00:05s\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_unsup_loss_numpy = 202.38461303710938\n",
      "epoch 0  | loss: 0.66325 | valid_auc: 0.59274 |  0:00:00s\n",
      "epoch 10 | loss: 0.44115 | valid_auc: 0.64124 |  0:00:00s\n",
      "epoch 20 | loss: 0.43025 | valid_auc: 0.5524  |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 14:52:36,215]\u001b[0m Trial 28 finished with value: 0.7656666666666667 and parameters: {'n_d': 53, 'n_a': 44, 'n_step': 5, 'gamma': 1.570458959494639, 'mask_type': 'sparsemax'}. Best is trial 0 with value: 0.7656666666666667.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 | loss: 0.42158 | valid_auc: 0.6189  |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 12 and best_valid_auc = 0.67009\n",
      "epoch 0  | loss: 110376.18994| val_0_unsup_loss_numpy: 73749.171875|  0:00:00s\n",
      "epoch 10 | loss: 711.68524| val_0_unsup_loss_numpy: 1904.2930908203125|  0:00:01s\n",
      "epoch 20 | loss: 38.57199| val_0_unsup_loss_numpy: 564.6282958984375|  0:00:03s\n",
      "epoch 30 | loss: 33.08686| val_0_unsup_loss_numpy: 261.15325927734375|  0:00:04s\n",
      "epoch 40 | loss: 56.23543| val_0_unsup_loss_numpy: 73.16909790039062|  0:00:06s\n",
      "epoch 50 | loss: 17.6799 | val_0_unsup_loss_numpy: 213.44993591308594|  0:00:07s\n",
      "epoch 60 | loss: 63.28276| val_0_unsup_loss_numpy: 203.166259765625|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 61 with best_epoch = 41 and best_val_0_unsup_loss_numpy = 21.714920043945312\n",
      "epoch 0  | loss: 0.65593 | valid_auc: 0.52988 |  0:00:00s\n",
      "epoch 10 | loss: 0.46077 | valid_auc: 0.48522 |  0:00:00s\n",
      "epoch 20 | loss: 0.44638 | valid_auc: 0.6017  |  0:00:01s\n",
      "epoch 30 | loss: 0.42583 | valid_auc: 0.68155 |  0:00:02s\n",
      "epoch 40 | loss: 0.41678 | valid_auc: 0.71286 |  0:00:03s\n",
      "epoch 50 | loss: 0.41089 | valid_auc: 0.73018 |  0:00:04s\n",
      "epoch 60 | loss: 0.40975 | valid_auc: 0.73012 |  0:00:05s\n",
      "epoch 70 | loss: 0.39049 | valid_auc: 0.74623 |  0:00:06s\n",
      "epoch 80 | loss: 0.38759 | valid_auc: 0.74778 |  0:00:06s\n",
      "epoch 90 | loss: 0.37898 | valid_auc: 0.73636 |  0:00:07s\n",
      "epoch 100| loss: 0.37382 | valid_auc: 0.74636 |  0:00:08s\n",
      "epoch 110| loss: 0.3718  | valid_auc: 0.74494 |  0:00:09s\n",
      "epoch 120| loss: 0.35987 | valid_auc: 0.75244 |  0:00:10s\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 102 and best_valid_auc = 0.75667\n",
      "epoch 0  | loss: 94370.03027| val_0_unsup_loss_numpy: 17508.9921875|  0:00:00s\n",
      "epoch 10 | loss: 50.14755| val_0_unsup_loss_numpy: 386726.625|  0:00:01s\n",
      "epoch 20 | loss: 26.16672| val_0_unsup_loss_numpy: 3018.63134765625|  0:00:03s\n",
      "epoch 30 | loss: 66.98974| val_0_unsup_loss_numpy: 114.78819274902344|  0:00:04s\n",
      "epoch 40 | loss: 21.48916| val_0_unsup_loss_numpy: 5724.16064453125|  0:00:06s\n",
      "epoch 50 | loss: 14.70267| val_0_unsup_loss_numpy: 458.03485107421875|  0:00:07s\n",
      "epoch 60 | loss: 5.83688 | val_0_unsup_loss_numpy: 18.403139114379883|  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 46 and best_val_0_unsup_loss_numpy = 4.336289882659912\n",
      "epoch 0  | loss: 0.61599 | valid_auc: 0.5856  |  0:00:00s\n",
      "epoch 10 | loss: 0.44422 | valid_auc: 0.68138 |  0:00:00s\n",
      "epoch 20 | loss: 0.43285 | valid_auc: 0.68199 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_valid_auc = 0.7296\n",
      "epoch 0  | loss: 172606.04688| val_0_unsup_loss_numpy: 443988.1875|  0:00:00s\n",
      "epoch 10 | loss: 302.13637| val_0_unsup_loss_numpy: 122103.421875|  0:00:01s\n",
      "epoch 20 | loss: 50.9832 | val_0_unsup_loss_numpy: 21448.150390625|  0:00:03s\n",
      "epoch 30 | loss: 101.98731| val_0_unsup_loss_numpy: 1743.913330078125|  0:00:04s\n",
      "epoch 40 | loss: 14.66358| val_0_unsup_loss_numpy: 7841.11767578125|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_val_0_unsup_loss_numpy = 1078.7030029296875\n",
      "epoch 0  | loss: 0.63946 | valid_auc: 0.49185 |  0:00:00s\n",
      "epoch 10 | loss: 0.45231 | valid_auc: 0.6158  |  0:00:00s\n",
      "epoch 20 | loss: 0.44445 | valid_auc: 0.66228 |  0:00:01s\n",
      "epoch 30 | loss: 0.42855 | valid_auc: 0.6914  |  0:00:02s\n",
      "epoch 40 | loss: 0.41515 | valid_auc: 0.71254 |  0:00:03s\n",
      "epoch 50 | loss: 0.40289 | valid_auc: 0.72965 |  0:00:04s\n",
      "epoch 60 | loss: 0.38184 | valid_auc: 0.7246  |  0:00:05s\n",
      "epoch 70 | loss: 0.3712  | valid_auc: 0.72264 |  0:00:05s\n",
      "epoch 80 | loss: 0.37229 | valid_auc: 0.69919 |  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 85 with best_epoch = 65 and best_valid_auc = 0.73217\n",
      "epoch 0  | loss: 94648.92773| val_0_unsup_loss_numpy: 26948.091796875|  0:00:00s\n",
      "epoch 10 | loss: 66.31772| val_0_unsup_loss_numpy: 398369.0|  0:00:01s\n",
      "epoch 20 | loss: 58.88721| val_0_unsup_loss_numpy: 15123.94921875|  0:00:03s\n",
      "epoch 30 | loss: 253.05495| val_0_unsup_loss_numpy: 10266.33984375|  0:00:04s\n",
      "epoch 40 | loss: 15.11711| val_0_unsup_loss_numpy: 19079.060546875|  0:00:06s\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_unsup_loss_numpy = 202.38461303710938\n",
      "epoch 0  | loss: 0.66325 | valid_auc: 0.59274 |  0:00:00s\n",
      "epoch 10 | loss: 0.44115 | valid_auc: 0.64124 |  0:00:00s\n",
      "epoch 20 | loss: 0.43025 | valid_auc: 0.5524  |  0:00:01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 14:53:31,376]\u001b[0m Trial 29 finished with value: 0.7656666666666667 and parameters: {'n_d': 39, 'n_a': 23, 'n_step': 4, 'gamma': 1.864792365897348, 'mask_type': 'entmatx'}. Best is trial 0 with value: 0.7656666666666667.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 | loss: 0.42158 | valid_auc: 0.6189  |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 12 and best_valid_auc = 0.67009\n"
     ]
    }
   ],
   "source": [
    "sampler = optuna.samplers.TPESampler(seed=random_state)\n",
    "study = optuna.create_study(sampler=sampler, direction='maximize')\n",
    "study.optimize(objective, n_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dd085928-9bb2-45d6-9e3c-bd091e44473c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc(best)=0.7657\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_d': 47,\n",
       " 'n_a': 24,\n",
       " 'n_step': 3,\n",
       " 'gamma': 1.5513147690828912,\n",
       " 'mask_type': 'entmatx'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trial = study.best_trial\n",
    "print('acc(best)={:.4f}'.format(trial.value))\n",
    "display(trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3b9c69c7-7198-4c8f-92a0-89ce861a6bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_d': 47,\n",
       " 'n_a': 24,\n",
       " 'n_step': 3,\n",
       " 'gamma': 1.5513147690828912,\n",
       " 'mask_type': 'entmax',\n",
       " 'optimizer_fn': torch.optim.adam.Adam,\n",
       " 'optimizer_params': {'lr': 0.02, 'weight_decay': 1e-05},\n",
       " 'scheduler_params': {'mode': 'min',\n",
       "  'patience': 5,\n",
       "  'min_lr': 1e-05,\n",
       "  'factor': 0.9,\n",
       "  'scheduler_fn': torch.optim.lr_scheduler.ReduceLROnPlateau},\n",
       " 'verbose': 10,\n",
       " 'seed': 123}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params_best = trial.params\n",
    "params_best.update(params_base)\n",
    "display(params_best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
