{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9828789-bec1-4170-8a76-262379ab71b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "import lightgbm as lgb\n",
    "\n",
    "#データ読み込み\n",
    "train = pd.read_csv(\"data_EDA/train.csv\")\n",
    "test = pd.read_csv(\"data_EDA/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "564c7484-3a88-48b9-8538-8ce80818836e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "      <th>BloodPressure_0</th>\n",
       "      <th>SkinThickness_0</th>\n",
       "      <th>Insulin_0</th>\n",
       "      <th>SkinThickness_na</th>\n",
       "      <th>BloodPressure_na</th>\n",
       "      <th>Insulin_na</th>\n",
       "      <th>SkinThickness_mean</th>\n",
       "      <th>BloodPressure_mean</th>\n",
       "      <th>Insulin_mean</th>\n",
       "      <th>Pregnancies_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.00000</td>\n",
       "      <td>2000.00000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1927.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2509.303000</td>\n",
       "      <td>3.584000</td>\n",
       "      <td>114.29350</td>\n",
       "      <td>68.76650</td>\n",
       "      <td>11.204000</td>\n",
       "      <td>11.859000</td>\n",
       "      <td>35.586624</td>\n",
       "      <td>0.401755</td>\n",
       "      <td>29.075500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71.371562</td>\n",
       "      <td>26.486998</td>\n",
       "      <td>146.407407</td>\n",
       "      <td>0.577000</td>\n",
       "      <td>0.036500</td>\n",
       "      <td>0.919000</td>\n",
       "      <td>26.710875</td>\n",
       "      <td>71.372937</td>\n",
       "      <td>136.508818</td>\n",
       "      <td>5.586500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1438.260835</td>\n",
       "      <td>3.053786</td>\n",
       "      <td>21.98925</td>\n",
       "      <td>16.17482</td>\n",
       "      <td>14.056037</td>\n",
       "      <td>49.826253</td>\n",
       "      <td>6.936853</td>\n",
       "      <td>0.267051</td>\n",
       "      <td>8.571729</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.247546</td>\n",
       "      <td>7.881171</td>\n",
       "      <td>104.898919</td>\n",
       "      <td>0.494159</td>\n",
       "      <td>0.187578</td>\n",
       "      <td>0.272903</td>\n",
       "      <td>5.127628</td>\n",
       "      <td>9.077127</td>\n",
       "      <td>29.914680</td>\n",
       "      <td>2.853698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>57.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.285680</td>\n",
       "      <td>0.137377</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1284.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>64.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.581209</td>\n",
       "      <td>0.234628</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>83.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26.875000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>135.636364</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2549.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>111.00000</td>\n",
       "      <td>70.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.814634</td>\n",
       "      <td>0.271275</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26.875000</td>\n",
       "      <td>71.409223</td>\n",
       "      <td>135.636364</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3743.750000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>125.00000</td>\n",
       "      <td>78.00000</td>\n",
       "      <td>24.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.694403</td>\n",
       "      <td>0.506439</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>189.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26.875000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>135.636364</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4995.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>199.00000</td>\n",
       "      <td>110.00000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>744.000000</td>\n",
       "      <td>52.960258</td>\n",
       "      <td>2.175784</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>744.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>744.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             index  Pregnancies     Glucose  BloodPressure  SkinThickness  \\\n",
       "count  2000.000000  2000.000000  2000.00000     2000.00000    2000.000000   \n",
       "mean   2509.303000     3.584000   114.29350       68.76650      11.204000   \n",
       "std    1438.260835     3.053786    21.98925       16.17482      14.056037   \n",
       "min       1.000000     0.000000    57.00000        0.00000       0.000000   \n",
       "25%    1284.750000     1.000000   100.00000       64.00000       0.000000   \n",
       "50%    2549.500000     3.000000   111.00000       70.00000       0.000000   \n",
       "75%    3743.750000     6.000000   125.00000       78.00000      24.250000   \n",
       "max    4995.000000    13.000000   199.00000      110.00000      52.000000   \n",
       "\n",
       "           Insulin          BMI  DiabetesPedigreeFunction          Age  \\\n",
       "count  2000.000000  2000.000000               2000.000000  2000.000000   \n",
       "mean     11.859000    35.586624                  0.401755    29.075500   \n",
       "std      49.826253     6.936853                  0.267051     8.571729   \n",
       "min       0.000000     9.285680                  0.137377    21.000000   \n",
       "25%       0.000000    32.581209                  0.234628    22.000000   \n",
       "50%       0.000000    33.814634                  0.271275    26.000000   \n",
       "75%       0.000000    39.694403                  0.506439    33.000000   \n",
       "max     744.000000    52.960258                  2.175784    67.000000   \n",
       "\n",
       "       Outcome  BloodPressure_0  SkinThickness_0   Insulin_0  \\\n",
       "count      0.0      1927.000000       846.000000  162.000000   \n",
       "mean       NaN        71.371562        26.486998  146.407407   \n",
       "std        NaN         9.247546         7.881171  104.898919   \n",
       "min        NaN        38.000000         8.000000   15.000000   \n",
       "25%        NaN        64.000000        19.000000   83.250000   \n",
       "50%        NaN        70.000000        27.000000  130.000000   \n",
       "75%        NaN        78.000000        32.000000  189.500000   \n",
       "max        NaN       110.000000        52.000000  744.000000   \n",
       "\n",
       "       SkinThickness_na  BloodPressure_na   Insulin_na  SkinThickness_mean  \\\n",
       "count       2000.000000       2000.000000  2000.000000         2000.000000   \n",
       "mean           0.577000          0.036500     0.919000           26.710875   \n",
       "std            0.494159          0.187578     0.272903            5.127628   \n",
       "min            0.000000          0.000000     0.000000            8.000000   \n",
       "25%            0.000000          0.000000     1.000000           26.875000   \n",
       "50%            1.000000          0.000000     1.000000           26.875000   \n",
       "75%            1.000000          0.000000     1.000000           26.875000   \n",
       "max            1.000000          1.000000     1.000000           52.000000   \n",
       "\n",
       "       BloodPressure_mean  Insulin_mean  Pregnancies_bin  \n",
       "count         2000.000000   2000.000000      2000.000000  \n",
       "mean            71.372937    136.508818         5.586500  \n",
       "std              9.077127     29.914680         2.853698  \n",
       "min             38.000000     15.000000         3.000000  \n",
       "25%             64.000000    135.636364         3.000000  \n",
       "50%             71.409223    135.636364         6.000000  \n",
       "75%             78.000000    135.636364        10.000000  \n",
       "max            110.000000    744.000000        10.000000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4df67cb-c4bd-4f8c-8fb7-ef2b18cc8a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "      <th>BloodPressure_0</th>\n",
       "      <th>SkinThickness_0</th>\n",
       "      <th>Insulin_0</th>\n",
       "      <th>SkinThickness_na</th>\n",
       "      <th>BloodPressure_na</th>\n",
       "      <th>Insulin_na</th>\n",
       "      <th>SkinThickness_mean</th>\n",
       "      <th>BloodPressure_mean</th>\n",
       "      <th>Insulin_mean</th>\n",
       "      <th>Pregnancies_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>2887.000000</td>\n",
       "      <td>1234.000000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2492.964667</td>\n",
       "      <td>3.557000</td>\n",
       "      <td>113.733667</td>\n",
       "      <td>68.743667</td>\n",
       "      <td>11.164000</td>\n",
       "      <td>11.663333</td>\n",
       "      <td>35.408959</td>\n",
       "      <td>0.400476</td>\n",
       "      <td>28.932000</td>\n",
       "      <td>0.239000</td>\n",
       "      <td>71.434361</td>\n",
       "      <td>27.141005</td>\n",
       "      <td>136.679688</td>\n",
       "      <td>0.588667</td>\n",
       "      <td>0.037667</td>\n",
       "      <td>0.914667</td>\n",
       "      <td>26.984417</td>\n",
       "      <td>71.433414</td>\n",
       "      <td>135.725394</td>\n",
       "      <td>5.594000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1447.218078</td>\n",
       "      <td>3.032131</td>\n",
       "      <td>20.885612</td>\n",
       "      <td>16.332755</td>\n",
       "      <td>14.351159</td>\n",
       "      <td>45.064090</td>\n",
       "      <td>6.990180</td>\n",
       "      <td>0.274666</td>\n",
       "      <td>8.469078</td>\n",
       "      <td>0.426544</td>\n",
       "      <td>9.215697</td>\n",
       "      <td>8.182799</td>\n",
       "      <td>82.032125</td>\n",
       "      <td>0.492157</td>\n",
       "      <td>0.190421</td>\n",
       "      <td>0.279424</td>\n",
       "      <td>5.248441</td>\n",
       "      <td>9.040411</td>\n",
       "      <td>23.922032</td>\n",
       "      <td>2.857951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.748040</td>\n",
       "      <td>0.145844</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1218.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.301920</td>\n",
       "      <td>0.230987</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>79.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26.875000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>135.636364</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2465.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>111.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.835873</td>\n",
       "      <td>0.268674</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26.875000</td>\n",
       "      <td>71.409223</td>\n",
       "      <td>135.636364</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3750.250000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.578256</td>\n",
       "      <td>0.506778</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26.875000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>135.636364</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4999.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>579.000000</td>\n",
       "      <td>53.400629</td>\n",
       "      <td>2.302072</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>579.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>579.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             index  Pregnancies      Glucose  BloodPressure  SkinThickness  \\\n",
       "count  3000.000000  3000.000000  3000.000000    3000.000000    3000.000000   \n",
       "mean   2492.964667     3.557000   113.733667      68.743667      11.164000   \n",
       "std    1447.218078     3.032131    20.885612      16.332755      14.351159   \n",
       "min       0.000000     0.000000    57.000000       0.000000       0.000000   \n",
       "25%    1218.750000     1.000000   100.000000      64.000000       0.000000   \n",
       "50%    2465.500000     3.000000   111.000000      70.000000       0.000000   \n",
       "75%    3750.250000     6.000000   125.000000      78.000000      24.000000   \n",
       "max    4999.000000    13.000000   196.000000     110.000000      49.000000   \n",
       "\n",
       "           Insulin          BMI  DiabetesPedigreeFunction          Age  \\\n",
       "count  3000.000000  3000.000000               3000.000000  3000.000000   \n",
       "mean     11.663333    35.408959                  0.400476    28.932000   \n",
       "std      45.064090     6.990180                  0.274666     8.469078   \n",
       "min       0.000000     7.748040                  0.145844    21.000000   \n",
       "25%       0.000000    32.301920                  0.230987    22.000000   \n",
       "50%       0.000000    33.835873                  0.268674    26.000000   \n",
       "75%       0.000000    39.578256                  0.506778    33.000000   \n",
       "max     579.000000    53.400629                  2.302072    67.000000   \n",
       "\n",
       "           Outcome  BloodPressure_0  SkinThickness_0   Insulin_0  \\\n",
       "count  3000.000000      2887.000000      1234.000000  256.000000   \n",
       "mean      0.239000        71.434361        27.141005  136.679688   \n",
       "std       0.426544         9.215697         8.182799   82.032125   \n",
       "min       0.000000        46.000000         7.000000   15.000000   \n",
       "25%       0.000000        64.000000        19.000000   79.250000   \n",
       "50%       0.000000        70.000000        28.000000  126.000000   \n",
       "75%       0.000000        78.000000        33.000000  180.000000   \n",
       "max       1.000000       110.000000        49.000000  579.000000   \n",
       "\n",
       "       SkinThickness_na  BloodPressure_na   Insulin_na  SkinThickness_mean  \\\n",
       "count       3000.000000       3000.000000  3000.000000         3000.000000   \n",
       "mean           0.588667          0.037667     0.914667           26.984417   \n",
       "std            0.492157          0.190421     0.279424            5.248441   \n",
       "min            0.000000          0.000000     0.000000            7.000000   \n",
       "25%            0.000000          0.000000     1.000000           26.875000   \n",
       "50%            1.000000          0.000000     1.000000           26.875000   \n",
       "75%            1.000000          0.000000     1.000000           26.875000   \n",
       "max            1.000000          1.000000     1.000000           49.000000   \n",
       "\n",
       "       BloodPressure_mean  Insulin_mean  Pregnancies_bin  \n",
       "count         3000.000000   3000.000000      3000.000000  \n",
       "mean            71.433414    135.725394         5.594000  \n",
       "std              9.040411     23.922032         2.857951  \n",
       "min             46.000000     15.000000         3.000000  \n",
       "25%             64.000000    135.636364         3.000000  \n",
       "50%             71.409223    135.636364         6.000000  \n",
       "75%             78.000000    135.636364        10.000000  \n",
       "max            110.000000    579.000000        10.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0b40f27-4ece-46b4-a6c7-38b12b2ebd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[['DiabetesPedigreeFunction',\n",
    "                 'BMI',\n",
    "                 'Glucose',\n",
    "                 'Age',\n",
    "                 'Pregnancies',\n",
    "                 'BloodPressure_mean', \n",
    "                 'SkinThickness_mean',\n",
    "                 'Insulin_mean',\n",
    "                ]]\n",
    "id_train = train[['index']]\n",
    "y_train = train[['Outcome']]\n",
    "\n",
    "X_test = test[['DiabetesPedigreeFunction',\n",
    "                 'BMI',\n",
    "                 'Glucose',\n",
    "                 'Age',\n",
    "                 'Pregnancies',\n",
    "                 'BloodPressure_mean', \n",
    "                 'SkinThickness_mean',\n",
    "                 'Insulin_mean',\n",
    "                ]]\n",
    "id_test = test[['index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c1bfd15-d3ee-4ebd-81b1-7c17ab4d0042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 標準化\n",
    "change_cloumns = ['DiabetesPedigreeFunction',\n",
    "                 'BMI',\n",
    "                 'Glucose',\n",
    "                 'Age',\n",
    "                 'Pregnancies',\n",
    "                 'BloodPressure_mean', \n",
    "                 'SkinThickness_mean',\n",
    "                 'Insulin_mean',\n",
    "                ]\n",
    "\n",
    "for col in change_cloumns :\n",
    "    value_min = X_train[col].min()\n",
    "    value_max = X_train[col].max()\n",
    "    X_train[col] = (X_train[col] - value_min) / (value_max - value_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcd87649-e08e-4882-9b60-317611485b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in change_cloumns :\n",
    "    value_min = X_test[col].min()\n",
    "    value_max = X_test[col].max()\n",
    "    X_test[col] = (X_test[col] - value_min) / (value_max - value_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b322d24d-594b-43f5-98e9-037f78b28046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>Age</th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>BloodPressure_mean</th>\n",
       "      <th>SkinThickness_mean</th>\n",
       "      <th>Insulin_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.118091</td>\n",
       "      <td>0.605900</td>\n",
       "      <td>0.408156</td>\n",
       "      <td>0.172435</td>\n",
       "      <td>0.273615</td>\n",
       "      <td>0.397397</td>\n",
       "      <td>0.475819</td>\n",
       "      <td>0.214052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.127383</td>\n",
       "      <td>0.153117</td>\n",
       "      <td>0.150256</td>\n",
       "      <td>0.184110</td>\n",
       "      <td>0.233241</td>\n",
       "      <td>0.141256</td>\n",
       "      <td>0.124963</td>\n",
       "      <td>0.042415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.039487</td>\n",
       "      <td>0.537842</td>\n",
       "      <td>0.309353</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.473214</td>\n",
       "      <td>0.213894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.056965</td>\n",
       "      <td>0.571443</td>\n",
       "      <td>0.388489</td>\n",
       "      <td>0.108696</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.397019</td>\n",
       "      <td>0.473214</td>\n",
       "      <td>0.213894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.167392</td>\n",
       "      <td>0.697227</td>\n",
       "      <td>0.489209</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.473214</td>\n",
       "      <td>0.213894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       DiabetesPedigreeFunction          BMI      Glucose          Age  \\\n",
       "count               3000.000000  3000.000000  3000.000000  3000.000000   \n",
       "mean                   0.118091     0.605900     0.408156     0.172435   \n",
       "std                    0.127383     0.153117     0.150256     0.184110   \n",
       "min                    0.000000     0.000000     0.000000     0.000000   \n",
       "25%                    0.039487     0.537842     0.309353     0.021739   \n",
       "50%                    0.056965     0.571443     0.388489     0.108696   \n",
       "75%                    0.167392     0.697227     0.489209     0.260870   \n",
       "max                    1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "       Pregnancies  BloodPressure_mean  SkinThickness_mean  Insulin_mean  \n",
       "count  3000.000000         3000.000000         3000.000000   3000.000000  \n",
       "mean      0.273615            0.397397            0.475819      0.214052  \n",
       "std       0.233241            0.141256            0.124963      0.042415  \n",
       "min       0.000000            0.000000            0.000000      0.000000  \n",
       "25%       0.076923            0.281250            0.473214      0.213894  \n",
       "50%       0.230769            0.397019            0.473214      0.213894  \n",
       "75%       0.461538            0.500000            0.473214      0.213894  \n",
       "max       1.000000            1.000000            1.000000      1.000000  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1638f98-c1b6-48ee-a765-6369e686e7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>Age</th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>BloodPressure_mean</th>\n",
       "      <th>SkinThickness_mean</th>\n",
       "      <th>Insulin_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.129698</td>\n",
       "      <td>0.602203</td>\n",
       "      <td>0.403475</td>\n",
       "      <td>0.175554</td>\n",
       "      <td>0.275692</td>\n",
       "      <td>0.463513</td>\n",
       "      <td>0.425247</td>\n",
       "      <td>0.166679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.131009</td>\n",
       "      <td>0.158830</td>\n",
       "      <td>0.154854</td>\n",
       "      <td>0.186342</td>\n",
       "      <td>0.234907</td>\n",
       "      <td>0.126071</td>\n",
       "      <td>0.116537</td>\n",
       "      <td>0.041035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.047709</td>\n",
       "      <td>0.533389</td>\n",
       "      <td>0.302817</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.428977</td>\n",
       "      <td>0.165482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.065687</td>\n",
       "      <td>0.561630</td>\n",
       "      <td>0.380282</td>\n",
       "      <td>0.108696</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.464017</td>\n",
       "      <td>0.428977</td>\n",
       "      <td>0.165482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.181054</td>\n",
       "      <td>0.696257</td>\n",
       "      <td>0.478873</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.428977</td>\n",
       "      <td>0.165482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       DiabetesPedigreeFunction          BMI      Glucose          Age  \\\n",
       "count               2000.000000  2000.000000  2000.000000  2000.000000   \n",
       "mean                   0.129698     0.602203     0.403475     0.175554   \n",
       "std                    0.131009     0.158830     0.154854     0.186342   \n",
       "min                    0.000000     0.000000     0.000000     0.000000   \n",
       "25%                    0.047709     0.533389     0.302817     0.021739   \n",
       "50%                    0.065687     0.561630     0.380282     0.108696   \n",
       "75%                    0.181054     0.696257     0.478873     0.260870   \n",
       "max                    1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "       Pregnancies  BloodPressure_mean  SkinThickness_mean  Insulin_mean  \n",
       "count  2000.000000         2000.000000         2000.000000   2000.000000  \n",
       "mean      0.275692            0.463513            0.425247      0.166679  \n",
       "std       0.234907            0.126071            0.116537      0.041035  \n",
       "min       0.000000            0.000000            0.000000      0.000000  \n",
       "25%       0.076923            0.361111            0.428977      0.165482  \n",
       "50%       0.230769            0.464017            0.428977      0.165482  \n",
       "75%       0.461538            0.555556            0.428977      0.165482  \n",
       "max       1.000000            1.000000            1.000000      1.000000  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b84c0a7-0533-442f-94a4-cf5d5b9720f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-14 18:49:38.559344: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-14 18:49:40.187016: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-14 18:49:40.187202: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-14 18:49:40.493054: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-03-14 18:49:44.041884: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-14 18:49:44.042655: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-14 18:49:44.042686: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "random_state=123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "188c5d42-7fcd-4c2c-a2d6-e9a8896de4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    session_conf = tf.compat.v1.ConfigProto(\n",
    "        intra_op_parallelism_threads=1,\n",
    "        inter_op_parallelism_threads=1\n",
    "    )\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11843a2-84ac-4a99-8f3d-bc716ae893f7",
   "metadata": {},
   "source": [
    "## NNでの評価（CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "893af8e4-3936-4c0f-aa9e-a7cab3dc93ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, x_va, y_tr, y_va = train_test_split(X_train,\n",
    "                                          y_train,\n",
    "                                          test_size=0.2,\n",
    "                                          shuffle=True,\n",
    "                                          stratify=y_train,\n",
    "                                          random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81657fd1-039d-4c1a-a464-20b69ac9ff4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    input_num = Input(shape=(8,))\n",
    "    x_num = Dense(10, activation='relu')(input_num)\n",
    "    x_num = BatchNormalization()(x_num)\n",
    "    x_num = Dropout(0.3)(x_num)\n",
    "    x_num = Dense(10, activation='relu')(x_num)\n",
    "    x_num = BatchNormalization()(x_num)\n",
    "    x_num = Dropout(0.2)(x_num)\n",
    "    x_num = Dense(5, activation='relu')(x_num)\n",
    "    x_num = BatchNormalization()(x_num)\n",
    "    x_num = Dropout(0.1)(x_num)\n",
    "    out = Dense(1, activation='sigmoid')(x_num)\n",
    "    \n",
    "    model = Model(inputs=input_num, outputs=out,)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='Adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['binary_crossentropy'],\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44798591-6ec9-4ec6-9d9a-d8fd8c8701a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-14 18:49:46.718780: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-14 18:49:46.719887: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-14 18:49:46.720443: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (3ed327de65ae): /proc/driver/nvidia/version does not exist\n",
      "2023-03-14 18:49:46.723293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                90        \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 10)               40        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 10)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 10)               40        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 55        \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 5)                20        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 5)                 0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 361\n",
      "Trainable params: 311\n",
      "Non-trainable params: 50\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# モデルの確認\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3eb77bde-69b1-4d5c-b81b-89204a7deb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cv(input_x,\n",
    "             input_y,\n",
    "             input_id,\n",
    "             random_state=123,\n",
    "             n_splits=5\n",
    "            ):\n",
    "    \n",
    "    metrics = []\n",
    "    imp = pd.DataFrame()\n",
    "    cv = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state).\n",
    "              split(input_x, input_y))\n",
    "    \n",
    "    \n",
    "    for nfold in np.arange(n_splits):\n",
    "        print('-'*20, nfold, '-'*20)\n",
    "        idx_tr, idx_va = cv[nfold][0], cv[nfold][1]\n",
    "        x_tr, y_tr = input_x.loc[idx_tr, :], input_y.loc[idx_tr, :]\n",
    "        x_va, y_va = input_x.loc[idx_va, :], input_y.loc[idx_va, :]\n",
    "        print(x_tr.shape, y_tr.shape)\n",
    "        print(x_va.shape, y_va.shape)\n",
    "        print('y_train:{:.3f}, y_tr:{:.3f}, y_va{:.3f}'.\n",
    "              format(y_train['Outcome'].mean(), y_tr['Outcome'].mean(), y_va['Outcome'].mean(),))\n",
    "\n",
    "        model = create_model()\n",
    "        model.fit(x=x_tr,\n",
    "                  y=y_tr,\n",
    "                 validation_data=(x_va, y_va),\n",
    "                 batch_size=8,\n",
    "                 epochs=1000,\n",
    "                 callbacks=[ModelCheckpoint(filepath='model_keras.h5',\n",
    "                                            moniter='val_loss',\n",
    "                                            mode='min', \n",
    "                                            verbose=1,\n",
    "                                            save_best_only=True,\n",
    "                                            ),\n",
    "                            EarlyStopping(monitor='val_loss',\n",
    "                                          mode='min',\n",
    "                                          min_delta=0,\n",
    "                                          patience=5,\n",
    "                                          verbose=1,\n",
    "                                          restore_best_weights=True),\n",
    "                            ReduceLROnPlateau(moniter='val_loss',\n",
    "                                             mode='min',\n",
    "                                             factor=0.1,\n",
    "                                             patience=5,\n",
    "                                             verbose=1),\n",
    "                           ],\n",
    "                  verbose=1,\n",
    "                 )\n",
    "\n",
    "        y_tr_pred = model.predict(x_tr)\n",
    "        y_va_pred = model.predict(x_va)\n",
    "        metric_tr = accuracy_score(y_tr, np.where(y_tr_pred>=0.5,1,0))\n",
    "        metric_va = accuracy_score(y_va, np.where(y_va_pred>=0.5,1,0))\n",
    "        print('[accuracy] tr: {:.2f}, va: {:2f}'.\n",
    "             format(metric_tr, metric_va))\n",
    "        metrics.append([nfold, metric_tr, metric_va])\n",
    "\n",
    "\n",
    "    print('-'*20, 'result', '-'*20)\n",
    "    metrics = np.array(metrics)\n",
    "    print(metrics)\n",
    "\n",
    "    print('[cv] tr: {:.2f}+-{:.2f}, va: {:.2f}'.format(\n",
    "        metrics[:,1].mean(), metrics[:,1].std(),\n",
    "        metrics[:,2].mean(), metrics[:,2].std()\n",
    "    ))\n",
    "\n",
    "    print('Done')\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf1c7951-6b35-4f07-894d-814aa7eca215",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- 0 --------------------\n",
      "(2250, 8) (2250, 1)\n",
      "(750, 8) (750, 1)\n",
      "y_train:0.239, y_tr:0.239, y_va0.240\n",
      "Epoch 1/1000\n",
      "278/282 [============================>.] - ETA: 0s - loss: 0.7341 - binary_crossentropy: 0.7341\n",
      "Epoch 1: val_loss improved from inf to 0.55761, saving model to model_keras.h5\n",
      "282/282 [==============================] - 3s 7ms/step - loss: 0.7342 - binary_crossentropy: 0.7342 - val_loss: 0.5576 - val_binary_crossentropy: 0.5576 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "277/282 [============================>.] - ETA: 0s - loss: 0.5868 - binary_crossentropy: 0.5868\n",
      "Epoch 2: val_loss improved from 0.55761 to 0.53484, saving model to model_keras.h5\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5902 - binary_crossentropy: 0.5902 - val_loss: 0.5348 - val_binary_crossentropy: 0.5348 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "272/282 [===========================>..] - ETA: 0s - loss: 0.5763 - binary_crossentropy: 0.5763\n",
      "Epoch 3: val_loss improved from 0.53484 to 0.53119, saving model to model_keras.h5\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5770 - binary_crossentropy: 0.5770 - val_loss: 0.5312 - val_binary_crossentropy: 0.5312 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.5617 - binary_crossentropy: 0.5617\n",
      "Epoch 4: val_loss improved from 0.53119 to 0.51872, saving model to model_keras.h5\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.5617 - binary_crossentropy: 0.5617 - val_loss: 0.5187 - val_binary_crossentropy: 0.5187 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "276/282 [============================>.] - ETA: 0s - loss: 0.5559 - binary_crossentropy: 0.5559\n",
      "Epoch 5: val_loss improved from 0.51872 to 0.51717, saving model to model_keras.h5\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5560 - binary_crossentropy: 0.5560 - val_loss: 0.5172 - val_binary_crossentropy: 0.5172 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.5438 - binary_crossentropy: 0.5438\n",
      "Epoch 6: val_loss improved from 0.51717 to 0.51003, saving model to model_keras.h5\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.5447 - binary_crossentropy: 0.5447 - val_loss: 0.5100 - val_binary_crossentropy: 0.5100 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "272/282 [===========================>..] - ETA: 0s - loss: 0.5401 - binary_crossentropy: 0.5401\n",
      "Epoch 7: val_loss improved from 0.51003 to 0.50786, saving model to model_keras.h5\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5429 - binary_crossentropy: 0.5429 - val_loss: 0.5079 - val_binary_crossentropy: 0.5079 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "278/282 [============================>.] - ETA: 0s - loss: 0.5345 - binary_crossentropy: 0.5345\n",
      "Epoch 8: val_loss improved from 0.50786 to 0.49693, saving model to model_keras.h5\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5358 - binary_crossentropy: 0.5358 - val_loss: 0.4969 - val_binary_crossentropy: 0.4969 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "278/282 [============================>.] - ETA: 0s - loss: 0.5293 - binary_crossentropy: 0.5293\n",
      "Epoch 9: val_loss improved from 0.49693 to 0.49431, saving model to model_keras.h5\n",
      "282/282 [==============================] - 2s 7ms/step - loss: 0.5285 - binary_crossentropy: 0.5285 - val_loss: 0.4943 - val_binary_crossentropy: 0.4943 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "273/282 [============================>.] - ETA: 0s - loss: 0.5307 - binary_crossentropy: 0.5307\n",
      "Epoch 10: val_loss improved from 0.49431 to 0.49034, saving model to model_keras.h5\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5328 - binary_crossentropy: 0.5328 - val_loss: 0.4903 - val_binary_crossentropy: 0.4903 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "278/282 [============================>.] - ETA: 0s - loss: 0.5214 - binary_crossentropy: 0.5214\n",
      "Epoch 11: val_loss improved from 0.49034 to 0.48210, saving model to model_keras.h5\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5204 - binary_crossentropy: 0.5204 - val_loss: 0.4821 - val_binary_crossentropy: 0.4821 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "268/282 [===========================>..] - ETA: 0s - loss: 0.5243 - binary_crossentropy: 0.5243\n",
      "Epoch 12: val_loss improved from 0.48210 to 0.48185, saving model to model_keras.h5\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5238 - binary_crossentropy: 0.5238 - val_loss: 0.4819 - val_binary_crossentropy: 0.4819 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.5145 - binary_crossentropy: 0.5145\n",
      "Epoch 13: val_loss improved from 0.48185 to 0.47906, saving model to model_keras.h5\n",
      "282/282 [==============================] - 2s 7ms/step - loss: 0.5152 - binary_crossentropy: 0.5152 - val_loss: 0.4791 - val_binary_crossentropy: 0.4791 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "273/282 [============================>.] - ETA: 0s - loss: 0.5111 - binary_crossentropy: 0.5111\n",
      "Epoch 14: val_loss improved from 0.47906 to 0.47840, saving model to model_keras.h5\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5172 - binary_crossentropy: 0.5172 - val_loss: 0.4784 - val_binary_crossentropy: 0.4784 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "274/282 [============================>.] - ETA: 0s - loss: 0.5165 - binary_crossentropy: 0.5165\n",
      "Epoch 15: val_loss did not improve from 0.47840\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.5165 - binary_crossentropy: 0.5165 - val_loss: 0.4791 - val_binary_crossentropy: 0.4791 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.5069 - binary_crossentropy: 0.5069\n",
      "Epoch 16: val_loss did not improve from 0.47840\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.5062 - binary_crossentropy: 0.5062 - val_loss: 0.4793 - val_binary_crossentropy: 0.4793 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "272/282 [===========================>..] - ETA: 0s - loss: 0.5045 - binary_crossentropy: 0.5045\n",
      "Epoch 17: val_loss improved from 0.47840 to 0.47589, saving model to model_keras.h5\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5077 - binary_crossentropy: 0.5077 - val_loss: 0.4759 - val_binary_crossentropy: 0.4759 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "278/282 [============================>.] - ETA: 0s - loss: 0.5150 - binary_crossentropy: 0.5150\n",
      "Epoch 18: val_loss improved from 0.47589 to 0.47496, saving model to model_keras.h5\n",
      "282/282 [==============================] - 2s 5ms/step - loss: 0.5139 - binary_crossentropy: 0.5139 - val_loss: 0.4750 - val_binary_crossentropy: 0.4750 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.5060 - binary_crossentropy: 0.5060\n",
      "Epoch 19: val_loss improved from 0.47496 to 0.47242, saving model to model_keras.h5\n",
      "282/282 [==============================] - 2s 5ms/step - loss: 0.5060 - binary_crossentropy: 0.5060 - val_loss: 0.4724 - val_binary_crossentropy: 0.4724 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "270/282 [===========================>..] - ETA: 0s - loss: 0.5045 - binary_crossentropy: 0.5045\n",
      "Epoch 20: val_loss improved from 0.47242 to 0.47122, saving model to model_keras.h5\n",
      "282/282 [==============================] - 2s 7ms/step - loss: 0.5029 - binary_crossentropy: 0.5029 - val_loss: 0.4712 - val_binary_crossentropy: 0.4712 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.5056 - binary_crossentropy: 0.5056\n",
      "Epoch 21: val_loss improved from 0.47122 to 0.46990, saving model to model_keras.h5\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5056 - binary_crossentropy: 0.5056 - val_loss: 0.4699 - val_binary_crossentropy: 0.4699 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "269/282 [===========================>..] - ETA: 0s - loss: 0.5058 - binary_crossentropy: 0.5058\n",
      "Epoch 22: val_loss improved from 0.46990 to 0.46802, saving model to model_keras.h5\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5036 - binary_crossentropy: 0.5036 - val_loss: 0.4680 - val_binary_crossentropy: 0.4680 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "275/282 [============================>.] - ETA: 0s - loss: 0.5015 - binary_crossentropy: 0.5015\n",
      "Epoch 23: val_loss did not improve from 0.46802\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5018 - binary_crossentropy: 0.5018 - val_loss: 0.4688 - val_binary_crossentropy: 0.4688 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.5059 - binary_crossentropy: 0.5059\n",
      "Epoch 24: val_loss did not improve from 0.46802\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.5059 - binary_crossentropy: 0.5059 - val_loss: 0.4709 - val_binary_crossentropy: 0.4709 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.5087 - binary_crossentropy: 0.5087\n",
      "Epoch 25: val_loss did not improve from 0.46802\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.5087 - binary_crossentropy: 0.5087 - val_loss: 0.4700 - val_binary_crossentropy: 0.4700 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.5045 - binary_crossentropy: 0.5045\n",
      "Epoch 26: val_loss did not improve from 0.46802\n",
      "282/282 [==============================] - 2s 7ms/step - loss: 0.5043 - binary_crossentropy: 0.5043 - val_loss: 0.4704 - val_binary_crossentropy: 0.4704 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.5023 - binary_crossentropy: 0.5023\n",
      "Epoch 27: val_loss did not improve from 0.46802\n",
      "Restoring model weights from the end of the best epoch: 22.\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5018 - binary_crossentropy: 0.5018 - val_loss: 0.4690 - val_binary_crossentropy: 0.4690 - lr: 0.0010\n",
      "Epoch 27: early stopping\n",
      "71/71 [==============================] - 0s 2ms/step\n",
      "24/24 [==============================] - 0s 3ms/step\n",
      "[accuracy] tr: 0.77, va: 0.769333\n",
      "-------------------- 1 --------------------\n",
      "(2250, 8) (2250, 1)\n",
      "(750, 8) (750, 1)\n",
      "y_train:0.239, y_tr:0.239, y_va0.239\n",
      "Epoch 1/1000\n",
      "269/282 [===========================>..] - ETA: 0s - loss: 0.7732 - binary_crossentropy: 0.7732\n",
      "Epoch 1: val_loss improved from inf to 0.58683, saving model to model_keras.h5\n",
      "282/282 [==============================] - 3s 6ms/step - loss: 0.7696 - binary_crossentropy: 0.7696 - val_loss: 0.5868 - val_binary_crossentropy: 0.5868 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "272/282 [===========================>..] - ETA: 0s - loss: 0.5973 - binary_crossentropy: 0.5973\n",
      "Epoch 2: val_loss improved from 0.58683 to 0.53166, saving model to model_keras.h5\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5945 - binary_crossentropy: 0.5945 - val_loss: 0.5317 - val_binary_crossentropy: 0.5317 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "276/282 [============================>.] - ETA: 0s - loss: 0.5628 - binary_crossentropy: 0.5628\n",
      "Epoch 3: val_loss improved from 0.53166 to 0.50149, saving model to model_keras.h5\n",
      "282/282 [==============================] - 2s 8ms/step - loss: 0.5629 - binary_crossentropy: 0.5629 - val_loss: 0.5015 - val_binary_crossentropy: 0.5015 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.5376 - binary_crossentropy: 0.5376\n",
      "Epoch 4: val_loss improved from 0.50149 to 0.49997, saving model to model_keras.h5\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5376 - binary_crossentropy: 0.5376 - val_loss: 0.5000 - val_binary_crossentropy: 0.5000 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "273/282 [============================>.] - ETA: 0s - loss: 0.5418 - binary_crossentropy: 0.5418\n",
      "Epoch 5: val_loss improved from 0.49997 to 0.49309, saving model to model_keras.h5\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5411 - binary_crossentropy: 0.5411 - val_loss: 0.4931 - val_binary_crossentropy: 0.4931 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "274/282 [============================>.] - ETA: 0s - loss: 0.5156 - binary_crossentropy: 0.5156\n",
      "Epoch 6: val_loss improved from 0.49309 to 0.48785, saving model to model_keras.h5\n",
      "282/282 [==============================] - 2s 5ms/step - loss: 0.5161 - binary_crossentropy: 0.5161 - val_loss: 0.4879 - val_binary_crossentropy: 0.4879 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "273/282 [============================>.] - ETA: 0s - loss: 0.5269 - binary_crossentropy: 0.5269\n",
      "Epoch 7: val_loss improved from 0.48785 to 0.48399, saving model to model_keras.h5\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5256 - binary_crossentropy: 0.5256 - val_loss: 0.4840 - val_binary_crossentropy: 0.4840 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "275/282 [============================>.] - ETA: 0s - loss: 0.5247 - binary_crossentropy: 0.5247\n",
      "Epoch 8: val_loss did not improve from 0.48399\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.5242 - binary_crossentropy: 0.5242 - val_loss: 0.4885 - val_binary_crossentropy: 0.4885 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "274/282 [============================>.] - ETA: 0s - loss: 0.5116 - binary_crossentropy: 0.5116\n",
      "Epoch 9: val_loss did not improve from 0.48399\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5142 - binary_crossentropy: 0.5142 - val_loss: 0.4859 - val_binary_crossentropy: 0.4859 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "270/282 [===========================>..] - ETA: 0s - loss: 0.5046 - binary_crossentropy: 0.5046\n",
      "Epoch 10: val_loss did not improve from 0.48399\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.5067 - binary_crossentropy: 0.5067 - val_loss: 0.4850 - val_binary_crossentropy: 0.4850 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "274/282 [============================>.] - ETA: 0s - loss: 0.5085 - binary_crossentropy: 0.5085\n",
      "Epoch 11: val_loss did not improve from 0.48399\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.5065 - binary_crossentropy: 0.5065 - val_loss: 0.4844 - val_binary_crossentropy: 0.4844 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "275/282 [============================>.] - ETA: 0s - loss: 0.5065 - binary_crossentropy: 0.5065\n",
      "Epoch 12: val_loss improved from 0.48399 to 0.48220, saving model to model_keras.h5\n",
      "282/282 [==============================] - 2s 5ms/step - loss: 0.5071 - binary_crossentropy: 0.5071 - val_loss: 0.4822 - val_binary_crossentropy: 0.4822 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "274/282 [============================>.] - ETA: 0s - loss: 0.5047 - binary_crossentropy: 0.5047\n",
      "Epoch 13: val_loss improved from 0.48220 to 0.47975, saving model to model_keras.h5\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5049 - binary_crossentropy: 0.5049 - val_loss: 0.4797 - val_binary_crossentropy: 0.4797 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "275/282 [============================>.] - ETA: 0s - loss: 0.5092 - binary_crossentropy: 0.5092\n",
      "Epoch 14: val_loss did not improve from 0.47975\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5071 - binary_crossentropy: 0.5071 - val_loss: 0.4809 - val_binary_crossentropy: 0.4809 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "270/282 [===========================>..] - ETA: 0s - loss: 0.5057 - binary_crossentropy: 0.5057\n",
      "Epoch 15: val_loss improved from 0.47975 to 0.47831, saving model to model_keras.h5\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5066 - binary_crossentropy: 0.5066 - val_loss: 0.4783 - val_binary_crossentropy: 0.4783 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "279/282 [============================>.] - ETA: 0s - loss: 0.5069 - binary_crossentropy: 0.5069\n",
      "Epoch 16: val_loss did not improve from 0.47831\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.5059 - binary_crossentropy: 0.5059 - val_loss: 0.4806 - val_binary_crossentropy: 0.4806 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.4921 - binary_crossentropy: 0.4921\n",
      "Epoch 17: val_loss improved from 0.47831 to 0.47582, saving model to model_keras.h5\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.4933 - binary_crossentropy: 0.4933 - val_loss: 0.4758 - val_binary_crossentropy: 0.4758 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "271/282 [===========================>..] - ETA: 0s - loss: 0.4985 - binary_crossentropy: 0.4985\n",
      "Epoch 18: val_loss did not improve from 0.47582\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.4989 - binary_crossentropy: 0.4989 - val_loss: 0.4769 - val_binary_crossentropy: 0.4769 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "277/282 [============================>.] - ETA: 0s - loss: 0.4993 - binary_crossentropy: 0.4993\n",
      "Epoch 19: val_loss did not improve from 0.47582\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.4995 - binary_crossentropy: 0.4995 - val_loss: 0.4780 - val_binary_crossentropy: 0.4780 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "273/282 [============================>.] - ETA: 0s - loss: 0.4979 - binary_crossentropy: 0.4979\n",
      "Epoch 20: val_loss did not improve from 0.47582\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.4980 - binary_crossentropy: 0.4980 - val_loss: 0.4781 - val_binary_crossentropy: 0.4781 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "274/282 [============================>.] - ETA: 0s - loss: 0.4953 - binary_crossentropy: 0.4953\n",
      "Epoch 21: val_loss did not improve from 0.47582\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.4937 - binary_crossentropy: 0.4937 - val_loss: 0.4787 - val_binary_crossentropy: 0.4787 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "273/282 [============================>.] - ETA: 0s - loss: 0.4968 - binary_crossentropy: 0.4968\n",
      "Epoch 22: val_loss did not improve from 0.47582\n",
      "Restoring model weights from the end of the best epoch: 17.\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.4982 - binary_crossentropy: 0.4982 - val_loss: 0.4798 - val_binary_crossentropy: 0.4798 - lr: 0.0010\n",
      "Epoch 22: early stopping\n",
      "71/71 [==============================] - 0s 2ms/step\n",
      "24/24 [==============================] - 0s 3ms/step\n",
      "[accuracy] tr: 0.76, va: 0.781333\n",
      "-------------------- 2 --------------------\n",
      "(2250, 8) (2250, 1)\n",
      "(750, 8) (750, 1)\n",
      "y_train:0.239, y_tr:0.239, y_va0.239\n",
      "Epoch 1/1000\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.6646 - binary_crossentropy: 0.6646\n",
      "Epoch 1: val_loss improved from inf to 0.60927, saving model to model_keras.h5\n",
      "282/282 [==============================] - 3s 6ms/step - loss: 0.6645 - binary_crossentropy: 0.6645 - val_loss: 0.6093 - val_binary_crossentropy: 0.6093 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "272/282 [===========================>..] - ETA: 0s - loss: 0.5914 - binary_crossentropy: 0.5914\n",
      "Epoch 2: val_loss improved from 0.60927 to 0.56739, saving model to model_keras.h5\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5883 - binary_crossentropy: 0.5883 - val_loss: 0.5674 - val_binary_crossentropy: 0.5674 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "269/282 [===========================>..] - ETA: 0s - loss: 0.5650 - binary_crossentropy: 0.5650\n",
      "Epoch 3: val_loss improved from 0.56739 to 0.55737, saving model to model_keras.h5\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5672 - binary_crossentropy: 0.5672 - val_loss: 0.5574 - val_binary_crossentropy: 0.5574 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "279/282 [============================>.] - ETA: 0s - loss: 0.5631 - binary_crossentropy: 0.5631\n",
      "Epoch 4: val_loss improved from 0.55737 to 0.55192, saving model to model_keras.h5\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5627 - binary_crossentropy: 0.5627 - val_loss: 0.5519 - val_binary_crossentropy: 0.5519 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "277/282 [============================>.] - ETA: 0s - loss: 0.5507 - binary_crossentropy: 0.5507\n",
      "Epoch 5: val_loss improved from 0.55192 to 0.54376, saving model to model_keras.h5\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.5520 - binary_crossentropy: 0.5520 - val_loss: 0.5438 - val_binary_crossentropy: 0.5438 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "272/282 [===========================>..] - ETA: 0s - loss: 0.5518 - binary_crossentropy: 0.5518\n",
      "Epoch 6: val_loss improved from 0.54376 to 0.53405, saving model to model_keras.h5\n",
      "282/282 [==============================] - 2s 5ms/step - loss: 0.5494 - binary_crossentropy: 0.5494 - val_loss: 0.5340 - val_binary_crossentropy: 0.5340 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "277/282 [============================>.] - ETA: 0s - loss: 0.5412 - binary_crossentropy: 0.5412\n",
      "Epoch 7: val_loss improved from 0.53405 to 0.52260, saving model to model_keras.h5\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.5419 - binary_crossentropy: 0.5419 - val_loss: 0.5226 - val_binary_crossentropy: 0.5226 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "276/282 [============================>.] - ETA: 0s - loss: 0.5324 - binary_crossentropy: 0.5324\n",
      "Epoch 8: val_loss improved from 0.52260 to 0.51177, saving model to model_keras.h5\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.5320 - binary_crossentropy: 0.5320 - val_loss: 0.5118 - val_binary_crossentropy: 0.5118 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "270/282 [===========================>..] - ETA: 0s - loss: 0.5348 - binary_crossentropy: 0.5348\n",
      "Epoch 9: val_loss improved from 0.51177 to 0.50527, saving model to model_keras.h5\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5334 - binary_crossentropy: 0.5334 - val_loss: 0.5053 - val_binary_crossentropy: 0.5053 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "276/282 [============================>.] - ETA: 0s - loss: 0.5267 - binary_crossentropy: 0.5267\n",
      "Epoch 10: val_loss improved from 0.50527 to 0.49662, saving model to model_keras.h5\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5245 - binary_crossentropy: 0.5245 - val_loss: 0.4966 - val_binary_crossentropy: 0.4966 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "276/282 [============================>.] - ETA: 0s - loss: 0.5174 - binary_crossentropy: 0.5174\n",
      "Epoch 11: val_loss improved from 0.49662 to 0.49118, saving model to model_keras.h5\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5196 - binary_crossentropy: 0.5196 - val_loss: 0.4912 - val_binary_crossentropy: 0.4912 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "277/282 [============================>.] - ETA: 0s - loss: 0.5102 - binary_crossentropy: 0.5102\n",
      "Epoch 12: val_loss improved from 0.49118 to 0.48685, saving model to model_keras.h5\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5105 - binary_crossentropy: 0.5105 - val_loss: 0.4869 - val_binary_crossentropy: 0.4869 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "279/282 [============================>.] - ETA: 0s - loss: 0.5142 - binary_crossentropy: 0.5142\n",
      "Epoch 13: val_loss improved from 0.48685 to 0.48258, saving model to model_keras.h5\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5151 - binary_crossentropy: 0.5151 - val_loss: 0.4826 - val_binary_crossentropy: 0.4826 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.5165 - binary_crossentropy: 0.5165\n",
      "Epoch 14: val_loss did not improve from 0.48258\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5164 - binary_crossentropy: 0.5164 - val_loss: 0.4929 - val_binary_crossentropy: 0.4929 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "275/282 [============================>.] - ETA: 0s - loss: 0.5130 - binary_crossentropy: 0.5130\n",
      "Epoch 15: val_loss did not improve from 0.48258\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5144 - binary_crossentropy: 0.5144 - val_loss: 0.4855 - val_binary_crossentropy: 0.4855 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "274/282 [============================>.] - ETA: 0s - loss: 0.5107 - binary_crossentropy: 0.5107\n",
      "Epoch 16: val_loss did not improve from 0.48258\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.5120 - binary_crossentropy: 0.5120 - val_loss: 0.4849 - val_binary_crossentropy: 0.4849 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.5096 - binary_crossentropy: 0.5096\n",
      "Epoch 17: val_loss did not improve from 0.48258\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5097 - binary_crossentropy: 0.5097 - val_loss: 0.4860 - val_binary_crossentropy: 0.4860 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "270/282 [===========================>..] - ETA: 0s - loss: 0.5059 - binary_crossentropy: 0.5059\n",
      "Epoch 18: val_loss did not improve from 0.48258\n",
      "Restoring model weights from the end of the best epoch: 13.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.5118 - binary_crossentropy: 0.5118 - val_loss: 0.4840 - val_binary_crossentropy: 0.4840 - lr: 0.0010\n",
      "Epoch 18: early stopping\n",
      "71/71 [==============================] - 0s 2ms/step\n",
      "24/24 [==============================] - 0s 2ms/step\n",
      "[accuracy] tr: 0.77, va: 0.762667\n",
      "-------------------- 3 --------------------\n",
      "(2250, 8) (2250, 1)\n",
      "(750, 8) (750, 1)\n",
      "y_train:0.239, y_tr:0.239, y_va0.239\n",
      "Epoch 1/1000\n",
      "278/282 [============================>.] - ETA: 0s - loss: 0.6594 - binary_crossentropy: 0.6594\n",
      "Epoch 1: val_loss improved from inf to 0.59609, saving model to model_keras.h5\n",
      "282/282 [==============================] - 3s 6ms/step - loss: 0.6587 - binary_crossentropy: 0.6587 - val_loss: 0.5961 - val_binary_crossentropy: 0.5961 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "270/282 [===========================>..] - ETA: 0s - loss: 0.5777 - binary_crossentropy: 0.5777\n",
      "Epoch 2: val_loss improved from 0.59609 to 0.55063, saving model to model_keras.h5\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5769 - binary_crossentropy: 0.5769 - val_loss: 0.5506 - val_binary_crossentropy: 0.5506 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "271/282 [===========================>..] - ETA: 0s - loss: 0.5273 - binary_crossentropy: 0.5273\n",
      "Epoch 3: val_loss improved from 0.55063 to 0.51631, saving model to model_keras.h5\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5285 - binary_crossentropy: 0.5285 - val_loss: 0.5163 - val_binary_crossentropy: 0.5163 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "277/282 [============================>.] - ETA: 0s - loss: 0.5174 - binary_crossentropy: 0.5174\n",
      "Epoch 4: val_loss improved from 0.51631 to 0.51038, saving model to model_keras.h5\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5174 - binary_crossentropy: 0.5174 - val_loss: 0.5104 - val_binary_crossentropy: 0.5104 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "270/282 [===========================>..] - ETA: 0s - loss: 0.5105 - binary_crossentropy: 0.5105\n",
      "Epoch 5: val_loss improved from 0.51038 to 0.50284, saving model to model_keras.h5\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5131 - binary_crossentropy: 0.5131 - val_loss: 0.5028 - val_binary_crossentropy: 0.5028 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.5108 - binary_crossentropy: 0.5108\n",
      "Epoch 6: val_loss did not improve from 0.50284\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5107 - binary_crossentropy: 0.5107 - val_loss: 0.5034 - val_binary_crossentropy: 0.5034 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.5150 - binary_crossentropy: 0.5150\n",
      "Epoch 7: val_loss improved from 0.50284 to 0.50175, saving model to model_keras.h5\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5143 - binary_crossentropy: 0.5143 - val_loss: 0.5018 - val_binary_crossentropy: 0.5018 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "273/282 [============================>.] - ETA: 0s - loss: 0.4941 - binary_crossentropy: 0.4941\n",
      "Epoch 8: val_loss improved from 0.50175 to 0.49770, saving model to model_keras.h5\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.4964 - binary_crossentropy: 0.4964 - val_loss: 0.4977 - val_binary_crossentropy: 0.4977 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "275/282 [============================>.] - ETA: 0s - loss: 0.4960 - binary_crossentropy: 0.4960\n",
      "Epoch 9: val_loss did not improve from 0.49770\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.4973 - binary_crossentropy: 0.4973 - val_loss: 0.5044 - val_binary_crossentropy: 0.5044 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "279/282 [============================>.] - ETA: 0s - loss: 0.5055 - binary_crossentropy: 0.5055\n",
      "Epoch 10: val_loss did not improve from 0.49770\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5056 - binary_crossentropy: 0.5056 - val_loss: 0.5020 - val_binary_crossentropy: 0.5020 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "280/282 [============================>.] - ETA: 0s - loss: 0.4966 - binary_crossentropy: 0.4966\n",
      "Epoch 11: val_loss did not improve from 0.49770\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.4967 - binary_crossentropy: 0.4967 - val_loss: 0.4977 - val_binary_crossentropy: 0.4977 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "271/282 [===========================>..] - ETA: 0s - loss: 0.4918 - binary_crossentropy: 0.4918\n",
      "Epoch 12: val_loss did not improve from 0.49770\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.4941 - binary_crossentropy: 0.4941 - val_loss: 0.5005 - val_binary_crossentropy: 0.5005 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "270/282 [===========================>..] - ETA: 0s - loss: 0.4959 - binary_crossentropy: 0.4959\n",
      "Epoch 13: val_loss improved from 0.49770 to 0.49687, saving model to model_keras.h5\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.4939 - binary_crossentropy: 0.4939 - val_loss: 0.4969 - val_binary_crossentropy: 0.4969 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "272/282 [===========================>..] - ETA: 0s - loss: 0.4894 - binary_crossentropy: 0.4894\n",
      "Epoch 14: val_loss did not improve from 0.49687\n",
      "282/282 [==============================] - 2s 5ms/step - loss: 0.4905 - binary_crossentropy: 0.4905 - val_loss: 0.4990 - val_binary_crossentropy: 0.4990 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "276/282 [============================>.] - ETA: 0s - loss: 0.4928 - binary_crossentropy: 0.4928\n",
      "Epoch 15: val_loss improved from 0.49687 to 0.49582, saving model to model_keras.h5\n",
      "282/282 [==============================] - 2s 5ms/step - loss: 0.4914 - binary_crossentropy: 0.4914 - val_loss: 0.4958 - val_binary_crossentropy: 0.4958 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.4998 - binary_crossentropy: 0.4998\n",
      "Epoch 16: val_loss did not improve from 0.49582\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.4998 - binary_crossentropy: 0.4998 - val_loss: 0.4963 - val_binary_crossentropy: 0.4963 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "270/282 [===========================>..] - ETA: 0s - loss: 0.4905 - binary_crossentropy: 0.4905\n",
      "Epoch 17: val_loss did not improve from 0.49582\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.4918 - binary_crossentropy: 0.4918 - val_loss: 0.4996 - val_binary_crossentropy: 0.4996 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.4961 - binary_crossentropy: 0.4961\n",
      "Epoch 18: val_loss did not improve from 0.49582\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.4961 - binary_crossentropy: 0.4961 - val_loss: 0.4961 - val_binary_crossentropy: 0.4961 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "278/282 [============================>.] - ETA: 0s - loss: 0.4953 - binary_crossentropy: 0.4953\n",
      "Epoch 19: val_loss did not improve from 0.49582\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.4934 - binary_crossentropy: 0.4934 - val_loss: 0.4989 - val_binary_crossentropy: 0.4989 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "272/282 [===========================>..] - ETA: 0s - loss: 0.4916 - binary_crossentropy: 0.4916\n",
      "Epoch 20: val_loss improved from 0.49582 to 0.49508, saving model to model_keras.h5\n",
      "282/282 [==============================] - 2s 5ms/step - loss: 0.4922 - binary_crossentropy: 0.4922 - val_loss: 0.4951 - val_binary_crossentropy: 0.4951 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.4934 - binary_crossentropy: 0.4934\n",
      "Epoch 21: val_loss did not improve from 0.49508\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.4934 - binary_crossentropy: 0.4934 - val_loss: 0.4972 - val_binary_crossentropy: 0.4972 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "278/282 [============================>.] - ETA: 0s - loss: 0.4934 - binary_crossentropy: 0.4934\n",
      "Epoch 22: val_loss did not improve from 0.49508\n",
      "282/282 [==============================] - 2s 5ms/step - loss: 0.4948 - binary_crossentropy: 0.4948 - val_loss: 0.4953 - val_binary_crossentropy: 0.4953 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "274/282 [============================>.] - ETA: 0s - loss: 0.4890 - binary_crossentropy: 0.4890\n",
      "Epoch 23: val_loss did not improve from 0.49508\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.4912 - binary_crossentropy: 0.4912 - val_loss: 0.4993 - val_binary_crossentropy: 0.4993 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "272/282 [===========================>..] - ETA: 0s - loss: 0.4968 - binary_crossentropy: 0.4968\n",
      "Epoch 24: val_loss did not improve from 0.49508\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.4971 - binary_crossentropy: 0.4971 - val_loss: 0.4965 - val_binary_crossentropy: 0.4965 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "272/282 [===========================>..] - ETA: 0s - loss: 0.4952 - binary_crossentropy: 0.4952\n",
      "Epoch 25: val_loss did not improve from 0.49508\n",
      "Restoring model weights from the end of the best epoch: 20.\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.4941 - binary_crossentropy: 0.4941 - val_loss: 0.4957 - val_binary_crossentropy: 0.4957 - lr: 0.0010\n",
      "Epoch 25: early stopping\n",
      "71/71 [==============================] - 0s 2ms/step\n",
      "24/24 [==============================] - 0s 3ms/step\n",
      "[accuracy] tr: 0.76, va: 0.761333\n",
      "-------------------- result --------------------\n",
      "[[0.         0.772      0.76933333]\n",
      " [1.         0.76444444 0.78133333]\n",
      " [2.         0.76533333 0.76266667]\n",
      " [3.         0.76088889 0.76133333]]\n",
      "[cv] tr: 0.77+-0.00, va: 0.77\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "metrics = train_cv(X_train, y_train, id_train, random_state=random_state, n_splits=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaffb7f-6fa3-431f-bd0a-26028e65a3f4",
   "metadata": {},
   "source": [
    "## 推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0781cb8f-1000-4470-8dd2-009723c92965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論用に全てのデータで学習\n",
    "x_tr, x_va, y_tr, y_va = train_test_split(X_train,\n",
    "                                          y_train,\n",
    "                                          test_size=0.2,\n",
    "                                          shuffle=True,\n",
    "                                          stratify=y_train,\n",
    "                                          random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c6f5328-321e-4d4c-b9e3-54e304d25b0e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_3467/3039491272.py:12: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n",
      "Epoch 1/10000\n",
      "288/300 [===========================>..] - ETA: 0s - loss: 0.6569 - binary_crossentropy: 0.6569\n",
      "Epoch 1: val_loss improved from inf to 0.59942, saving model to model_keras.h5\n",
      "300/300 [==============================] - 3s 6ms/step - loss: 0.6537 - binary_crossentropy: 0.6537 - val_loss: 0.5994 - val_binary_crossentropy: 0.5994 - lr: 0.0010\n",
      "Epoch 2/10000\n",
      "295/300 [============================>.] - ETA: 0s - loss: 0.5781 - binary_crossentropy: 0.5781\n",
      "Epoch 2: val_loss improved from 0.59942 to 0.52681, saving model to model_keras.h5\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5779 - binary_crossentropy: 0.5779 - val_loss: 0.5268 - val_binary_crossentropy: 0.5268 - lr: 0.0010\n",
      "Epoch 3/10000\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.5490 - binary_crossentropy: 0.5490\n",
      "Epoch 3: val_loss improved from 0.52681 to 0.50241, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 8ms/step - loss: 0.5490 - binary_crossentropy: 0.5490 - val_loss: 0.5024 - val_binary_crossentropy: 0.5024 - lr: 0.0010\n",
      "Epoch 4/10000\n",
      "293/300 [============================>.] - ETA: 0s - loss: 0.5297 - binary_crossentropy: 0.5297\n",
      "Epoch 4: val_loss improved from 0.50241 to 0.49747, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 8ms/step - loss: 0.5288 - binary_crossentropy: 0.5288 - val_loss: 0.4975 - val_binary_crossentropy: 0.4975 - lr: 0.0010\n",
      "Epoch 5/10000\n",
      "295/300 [============================>.] - ETA: 0s - loss: 0.5335 - binary_crossentropy: 0.5335\n",
      "Epoch 5: val_loss improved from 0.49747 to 0.48150, saving model to model_keras.h5\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.5329 - binary_crossentropy: 0.5329 - val_loss: 0.4815 - val_binary_crossentropy: 0.4815 - lr: 0.0010\n",
      "Epoch 6/10000\n",
      "291/300 [============================>.] - ETA: 0s - loss: 0.5245 - binary_crossentropy: 0.5245\n",
      "Epoch 6: val_loss improved from 0.48150 to 0.48025, saving model to model_keras.h5\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.5249 - binary_crossentropy: 0.5249 - val_loss: 0.4802 - val_binary_crossentropy: 0.4802 - lr: 0.0010\n",
      "Epoch 7/10000\n",
      "297/300 [============================>.] - ETA: 0s - loss: 0.5202 - binary_crossentropy: 0.5202\n",
      "Epoch 7: val_loss improved from 0.48025 to 0.47821, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.5196 - binary_crossentropy: 0.5196 - val_loss: 0.4782 - val_binary_crossentropy: 0.4782 - lr: 0.0010\n",
      "Epoch 8/10000\n",
      "291/300 [============================>.] - ETA: 0s - loss: 0.5192 - binary_crossentropy: 0.5192\n",
      "Epoch 8: val_loss improved from 0.47821 to 0.47663, saving model to model_keras.h5\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5220 - binary_crossentropy: 0.5220 - val_loss: 0.4766 - val_binary_crossentropy: 0.4766 - lr: 0.0010\n",
      "Epoch 9/10000\n",
      "294/300 [============================>.] - ETA: 0s - loss: 0.5175 - binary_crossentropy: 0.5175\n",
      "Epoch 9: val_loss did not improve from 0.47663\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.5185 - binary_crossentropy: 0.5185 - val_loss: 0.4788 - val_binary_crossentropy: 0.4788 - lr: 0.0010\n",
      "Epoch 10/10000\n",
      "294/300 [============================>.] - ETA: 0s - loss: 0.5044 - binary_crossentropy: 0.5044\n",
      "Epoch 10: val_loss improved from 0.47663 to 0.47203, saving model to model_keras.h5\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5047 - binary_crossentropy: 0.5047 - val_loss: 0.4720 - val_binary_crossentropy: 0.4720 - lr: 0.0010\n",
      "Epoch 11/10000\n",
      "297/300 [============================>.] - ETA: 0s - loss: 0.5170 - binary_crossentropy: 0.5170\n",
      "Epoch 11: val_loss did not improve from 0.47203\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.5180 - binary_crossentropy: 0.5180 - val_loss: 0.4771 - val_binary_crossentropy: 0.4771 - lr: 0.0010\n",
      "Epoch 12/10000\n",
      "290/300 [============================>.] - ETA: 0s - loss: 0.5167 - binary_crossentropy: 0.5167\n",
      "Epoch 12: val_loss did not improve from 0.47203\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.5162 - binary_crossentropy: 0.5162 - val_loss: 0.4768 - val_binary_crossentropy: 0.4768 - lr: 0.0010\n",
      "Epoch 13/10000\n",
      "297/300 [============================>.] - ETA: 0s - loss: 0.5173 - binary_crossentropy: 0.5173\n",
      "Epoch 13: val_loss did not improve from 0.47203\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5173 - binary_crossentropy: 0.5173 - val_loss: 0.4740 - val_binary_crossentropy: 0.4740 - lr: 0.0010\n",
      "Epoch 14/10000\n",
      "286/300 [===========================>..] - ETA: 0s - loss: 0.5103 - binary_crossentropy: 0.5103\n",
      "Epoch 14: val_loss did not improve from 0.47203\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.5130 - binary_crossentropy: 0.5130 - val_loss: 0.4735 - val_binary_crossentropy: 0.4735 - lr: 0.0010\n",
      "Epoch 15/10000\n",
      "299/300 [============================>.] - ETA: 0s - loss: 0.5153 - binary_crossentropy: 0.5153\n",
      "Epoch 15: val_loss did not improve from 0.47203\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.5155 - binary_crossentropy: 0.5155 - val_loss: 0.4721 - val_binary_crossentropy: 0.4721 - lr: 0.0010\n",
      "Epoch 15: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f62dff034c0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_everything(seed=random_state)\n",
    "model = create_model()\n",
    "model.fit(x=x_tr,\n",
    "          y=y_tr,\n",
    "         validation_data=(x_va, y_va),\n",
    "         batch_size=8,\n",
    "         epochs=10000,\n",
    "         callbacks=[ModelCheckpoint(filepath='model_keras.h5',\n",
    "                                    moniter='val_loss',\n",
    "                                    mode='min', \n",
    "                                    verbose=1,\n",
    "                                    save_best_only=True,\n",
    "                                    ),\n",
    "                    EarlyStopping(monitor='val_loss',\n",
    "                                  mode='min',\n",
    "                                  min_delta=0,\n",
    "                                  patience=5,\n",
    "                                  verbose=1,\n",
    "                                  restore_best_weights=True),\n",
    "                    ReduceLROnPlateau(moniter='val_loss',\n",
    "                                     mode='min',\n",
    "                                     factor=0.1,\n",
    "                                     patience=5,\n",
    "                                     verbose=1),\n",
    "                   ],\n",
    "          verbose=1,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59db41f5-f433-4373-bf72-57fb48f2a5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 0s 2ms/step\n",
      "accuracy: 0.7517\n"
     ]
    }
   ],
   "source": [
    "# 全データのモデルの結果の確認\n",
    "y_va_pred = model.predict(x_va, batch_size=8, verbose=1)\n",
    "print('accuracy: {:.4f}'.format(accuracy_score(y_va, np.where(y_va_pred>=0.5,1,0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25b5152f-4db6-4adf-95f5-e6ce25a30cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 1s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>398</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3833</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4836</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4572</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>636</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2545</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1161</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2230</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>148</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2530</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4070</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1261</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4682</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>906</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3170</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>483</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2825</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1778</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2466</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  Outcome\n",
       "0     398        0\n",
       "1    3833        0\n",
       "2    4836        0\n",
       "3    4572        0\n",
       "4     636        0\n",
       "5    2545        0\n",
       "6    1161        0\n",
       "7    2230        0\n",
       "8     148        1\n",
       "9    2530        0\n",
       "10   4070        1\n",
       "11   1261        0\n",
       "12   4682        0\n",
       "13    333        0\n",
       "14    906        0\n",
       "15   3170        0\n",
       "16    483        0\n",
       "17   2825        0\n",
       "18   1778        0\n",
       "19   2466        0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_test_pred_proba = model.predict(X_test, batch_size=8, verbose=1)\n",
    "y_test_pred = np.where(y_test_pred_proba>=0.5,1,0)\n",
    "y_test_pred = np.squeeze(y_test_pred)\n",
    "df_sub = pd.DataFrame({'index':id_test['index'], 'Outcome':y_test_pred})\n",
    "display(df_sub.head(20))\n",
    "df_sub.to_csv('submission_nn.csv', index=None, header=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0c848410-f964-4be3-bcd6-2af32407dacb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2656804 , 0.1783321 , 0.16428877, ..., 0.5772522 , 0.18998696,\n",
       "       0.4275994 ], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred_proba =np.squeeze(y_test_pred_proba)\n",
    "y_test_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f399aa0-5484-45fe-a99e-ec7f8c58ea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    " \n",
    "with open('nn_proba.pickle', mode='wb') as fo:\n",
    "    pickle.dump(y_test_pred_proba, fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b427085-30ec-4db2-9b2b-c1822213b9cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
